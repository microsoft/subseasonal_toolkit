{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SubX++\n",
    "\n",
    "Learned correction for SubX ensemble forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from subseasonal_toolkit.utils.notebook_util import isnotebook\n",
    "if isnotebook():\n",
    "    # Autoreload packages that are modified\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    #%cd \"~/forecast_rodeo_ii\"\n",
    "    #%pwd\n",
    "else:\n",
    "    from argparse import ArgumentParser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist, euclidean\n",
    "from datetime import datetime, timedelta\n",
    "from filelock import FileLock\n",
    "from subseasonal_data.utils import get_measurement_variable\n",
    "from subseasonal_toolkit.utils.general_util import printf, tic, toc\n",
    "from subseasonal_toolkit.utils.experiments_util import (get_first_year, get_start_delta,\n",
    "                                                        get_forecast_delta)\n",
    "from subseasonal_toolkit.utils.models_util import (get_submodel_name, start_logger, log_params, get_forecast_filename,\n",
    "                                                   save_forecasts, get_selected_submodel_name)\n",
    "from subseasonal_toolkit.utils.eval_util import get_target_dates, mean_rmse_to_score, save_metric\n",
    "from sklearn.linear_model import *\n",
    "\n",
    "from subseasonal_data import data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Specify model parameters\n",
    "#\n",
    "model_name = \"subxpp\"\n",
    "if not isnotebook():\n",
    "    # If notebook run as a script, parse command-line arguments\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"pos_vars\",nargs=\"*\")  # gt_id and horizon                                                                                  \n",
    "    parser.add_argument('--target_dates', '-t', default=\"std_test\")\n",
    "    # Fit intercept parameter if and only if this flag is specified\n",
    "    parser.add_argument('--forecast', '-f', default=\"cfsv2\",\n",
    "                        help=\"include the forecasts of this dynamical model as features\")\n",
    "    parser.add_argument('--fit_intercept', '-i', default=\"False\",\n",
    "                        choices=['True', 'False'],\n",
    "                        help=\"Fit intercept parameter if \\\"True\\\"; do not if \\\"False\\\"\")\n",
    "    parser.add_argument('--train_years', '-y', default=\"all\",\n",
    "                       help=\"Number of years to use in training (\\\"all\\\" or integer)\")\n",
    "    parser.add_argument('--margin_in_days', '-m', default=\"None\",\n",
    "                       help=\"number of month-day combinations on either side of the target combination \"\n",
    "                            \"to include when training; set to 0 include only target month-day combo; \"\n",
    "                            \"set to None to include entire year\")\n",
    "    parser.add_argument('--first_day', '-fd', default=1,\n",
    "                        help=\"first available daily subx forecast (1 or greater) to average\")\n",
    "    parser.add_argument('--last_day', '-ld', default=1,\n",
    "                        help=\"last available daily subx forecast (first_day or greater) to average\")\n",
    "    parser.add_argument('--loss', '-l', default=\"mse\", \n",
    "                        help=\"loss function: mse, rmse, skill, or ssm\")\n",
    "    parser.add_argument('--first_lead', '-fl', default=0, \n",
    "                        help=\"first subx lead to average into forecast (0-29)\")\n",
    "    parser.add_argument('--last_lead', '-ll', default=29, \n",
    "                        help=\"last subx lead to average into forecast (0-29)\")\n",
    "    parser.add_argument('--mei', default=False, action='store_true', help=\"Whether to condition on MEI\")\n",
    "    parser.add_argument('--mjo', default=False, action='store_true', help=\"Whether to condition on MJO\")\n",
    "    args, opt = parser.parse_known_args()\n",
    "    \n",
    "    # Assign variables                                                                                                                                     \n",
    "    gt_id = args.pos_vars[0] # \"contest_precip\" or \"contest_tmp2m\"                                                                            \n",
    "    horizon = args.pos_vars[1] # \"12w\", \"34w\", or \"56w\"                                                                                        \n",
    "    target_dates = args.target_dates\n",
    "    forecast = args.forecast\n",
    "    fit_intercept = args.fit_intercept\n",
    "    mei = args.mei\n",
    "    mjo = args.mjo\n",
    "    if fit_intercept == \"False\":\n",
    "        fit_intercept = False\n",
    "    elif fit_intercept == \"True\":\n",
    "        fit_intercept = True\n",
    "    else:\n",
    "        raise ValueError(f\"unrecognized value {fit_intercept} for fit_intercept\")\n",
    "    train_years = args.train_years\n",
    "    if train_years != \"all\":\n",
    "        train_years = int(train_years)\n",
    "    if args.margin_in_days == \"None\":\n",
    "        margin_in_days = None\n",
    "    else:\n",
    "        margin_in_days = int(args.margin_in_days)\n",
    "    first_day = int(args.first_day)\n",
    "    last_day = int(args.last_day)\n",
    "    loss = args.loss\n",
    "    first_lead = int(args.first_lead)\n",
    "    last_lead = int(args.last_lead)\n",
    "else:\n",
    "    # Otherwise, specify arguments interactively \n",
    "    gt_id = \"us_precip_1.5x1.5\"\n",
    "    horizon = \"34w\"\n",
    "    target_dates = \"20200101\"#\"std_paper_forecast\"\n",
    "    forecast = \"subx_mean\"\n",
    "    fit_intercept = True\n",
    "    loss = \"mse\"\n",
    "    train_years = 12\n",
    "    margin_in_days = 28\n",
    "    mei = False\n",
    "    mjo = False\n",
    "    if \"tmp2m\" in gt_id and (horizon == \"12w\"):\n",
    "        first_day = 1\n",
    "        last_day = 1\n",
    "        first_lead = 0\n",
    "        last_lead = 0\n",
    "    elif \"precip\" in gt_id and (horizon == \"12w\"):\n",
    "        first_day = 1\n",
    "        last_day = 1\n",
    "        first_lead = 0\n",
    "        last_lead = 0\n",
    "    elif \"tmp2m\" in gt_id and (horizon == \"34w\"):\n",
    "        first_day = 1\n",
    "        last_day = 7\n",
    "        first_lead = 15\n",
    "        last_lead = 22\n",
    "    elif \"precip\" in gt_id and (horizon == \"34w\"):\n",
    "        first_day = 1\n",
    "        last_day = 35\n",
    "        first_lead = 0\n",
    "        last_lead = 29\n",
    "    elif \"tmp2m\" in gt_id and (horizon == \"56w\"):\n",
    "        first_day = 1\n",
    "        last_day = 14 \n",
    "        first_lead = 29\n",
    "        last_lead = 29\n",
    "    elif \"precip\" in gt_id and (horizon == \"56w\"):\n",
    "        first_day = 1\n",
    "        last_day = 21\n",
    "        first_lead = 0\n",
    "        last_lead = 29\n",
    "        \n",
    "#\n",
    "# Choose regression parameters\n",
    "#\n",
    "# Record standard settings of these parameters\n",
    "x_cols = ['zeros']\n",
    "if gt_id.endswith(\"1.5x1.5\"):\n",
    "    prefix = f\"iri_{forecast}\"\n",
    "else:\n",
    "    prefix = f\"subx_{forecast}\" \n",
    "if \"tmp2m\" in gt_id:\n",
    "    base_col = prefix+'_tmp2m'\n",
    "elif \"precip\" in gt_id:\n",
    "    base_col = prefix+'_precip'\n",
    "group_by_cols = ['lat', 'lon']    \n",
    "\n",
    "#\n",
    "# Process model parameters\n",
    "#\n",
    "\n",
    "# Get list of target date objects\n",
    "target_date_objs = pd.Series(get_target_dates(date_str=target_dates, horizon=horizon))\n",
    "\n",
    "# Identify measurement variable name\n",
    "measurement_variable = get_measurement_variable(gt_id) # 'tmp2m' or 'precip'\n",
    "\n",
    "# Column names for gt_col, clim_col and anom_col \n",
    "gt_col = measurement_variable\n",
    "clim_col = measurement_variable+\"_clim\"\n",
    "anom_col = get_measurement_variable(gt_id)+\"_anom\" # 'tmp2m_anom' or 'precip_anom'\n",
    "\n",
    "# For a given target date, the last observable training date is target date - gt_delta\n",
    "# as gt_delta is the gap between the start of the target date and the start of the\n",
    "# last ground truth period that's fully observable at the time of forecast issuance\n",
    "gt_delta = timedelta(days=get_start_delta(horizon, gt_id))\n",
    "\n",
    "LAST_SAVE_YEAR = get_first_year(prefix) # Don't save forecasts for years earlier than LAST_SAVE_YEAR\n",
    "\n",
    "# Record model and submodel names\n",
    "submodel_name = get_submodel_name(\n",
    "    model_name, forecast=forecast, fit_intercept=fit_intercept,\n",
    "    train_years=train_years, margin_in_days=margin_in_days,\n",
    "    first_day=first_day, last_day=last_day, loss=loss, \n",
    "    first_lead=first_lead, last_lead=last_lead, mei=mei, mjo=mjo)\n",
    "\n",
    "if not isnotebook():\n",
    "    # Save output to log file\n",
    "    logger = start_logger(model=model_name,submodel=submodel_name,gt_id=gt_id,\n",
    "                          horizon=horizon,target_dates=target_dates)\n",
    "    # Store parameter values in log                                                                                                                        \n",
    "    params_names = ['gt_id', 'horizon', 'target_dates', 'forecast',\n",
    "                    'fit_intercept', 'train_years', 'margin_in_days',\n",
    "                    'first_day', 'last_day', 'loss', \n",
    "                    'first_lead', 'last_lead',\n",
    "                    'base_col', 'x_cols', 'group_by_cols'\n",
    "                   ]\n",
    "    params_values = [eval(param) for param in params_names]\n",
    "    log_params(params_names, params_values)\n",
    "    \n",
    "def geometric_median(X, eps=1e-5):\n",
    "    \"\"\"Computes the geometric median of the columns of X, up to a tolerance epsilon.\n",
    "    The geometric median is the vector that minimizes the mean Euclidean norm to\n",
    "    each column of X.\n",
    "    \"\"\"\n",
    "    y = np.mean(X, 0)\n",
    "\n",
    "    while True:\n",
    "        D = cdist(X, [y])\n",
    "        nonzeros = (D != 0)[:, 0]\n",
    "\n",
    "        Dinv = 1 / D[nonzeros]\n",
    "        Dinvs = np.sum(Dinv)\n",
    "        W = Dinv / Dinvs\n",
    "        T = np.sum(W * X[nonzeros], 0)\n",
    "\n",
    "        num_zeros = len(X) - np.sum(nonzeros)\n",
    "        if num_zeros == 0:\n",
    "            y1 = T\n",
    "        elif num_zeros == len(X):\n",
    "            return y\n",
    "        else:\n",
    "            R = (T - y) * Dinvs\n",
    "            r = np.linalg.norm(R)\n",
    "            rinv = 0 if r == 0 else num_zeros/r\n",
    "            y1 = max(0, 1-rinv)*T + min(1, rinv)*y\n",
    "\n",
    "        if euclidean(y, y1) < eps:\n",
    "            return y1\n",
    "\n",
    "        y = y1\n",
    "\n",
    "def ssm(X, alpha=1):\n",
    "    \"\"\"Computes stabilized sample mean (Orenstein, 2019) of each column of X\n",
    "    \n",
    "    Args:\n",
    "        alpha: if infinity, recovers the mean; if 0 approximates median\n",
    "    \"\"\"\n",
    "    # Compute first, second, and third uncentered moments\n",
    "    mu = np.mean(X,0)\n",
    "    mu2 = np.mean(np.square(X),0)\n",
    "    mu3 = np.mean(np.power(X,3),0)\n",
    "    # Return mean - (third central moment)/(3*(2+numrows(X))*variance)\n",
    "    return mu - (mu3 - 3*mu*mu2+2*np.power(mu,3)).div(3*(2+alpha*X.shape[0])*(mu2 - np.square(mu)))\n",
    "    \n",
    "# Select estimator based on loss\n",
    "if loss == \"rmse\":\n",
    "    estimator = geometric_median \n",
    "elif loss == \"ssm\":\n",
    "    estimator = ssm\n",
    "else: \n",
    "    estimator = np.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process data\n",
    "printf(\"Loading subx data and averaging leads\")\n",
    "# Choose data shift based on horizon and first day to be averaged\n",
    "base_shift = get_forecast_delta(horizon) + first_day - 1\n",
    "tic()\n",
    "mask = None\n",
    "if gt_id.startswith(\"us_\"):\n",
    "    suffix = \"-us\"  \n",
    "else:\n",
    "    suffix = \"\"\n",
    "if gt_id.endswith(\"1.5x1.5\"):\n",
    "    suffix += \"1_5\"\n",
    "else:\n",
    "    mask = data_loaders.get_us_mask()\n",
    "\n",
    "if forecast == \"subx_mean\":\n",
    "    forecast_id = prefix+\"-\"+measurement_variable+\"_\"+horizon+suffix\n",
    "else:\n",
    "    forecast_id = prefix+\"-\"+measurement_variable+suffix\n",
    "\n",
    "tic(); data = data_loaders.get_forecast(forecast_id=forecast_id, mask_df=mask, shift=base_shift); toc()\n",
    "\n",
    "cols = [prefix+\"_\"+gt_id.split(\"_\")[1]+\"-{}.5d_shift{}\".format(col,base_shift) \n",
    "        for col in range(first_lead, last_lead+1)]\n",
    "data[base_col] = data[cols].mean(axis=1)\n",
    "toc()\n",
    "\n",
    "printf('Pivoting dataframe to have one row per start_date')\n",
    "tic()\n",
    "data = data[['lat','lon','start_date',base_col]].set_index(['lat','lon','start_date']).unstack(['lat','lon'])\n",
    "toc()\n",
    "\n",
    "printf(f\"Computing rolling mean over days {first_day}-{last_day}\")\n",
    "days = last_day - first_day + 1\n",
    "tic()\n",
    "data = data.rolling(f\"{days}d\").mean().dropna(how='any')\n",
    "toc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth\n",
    "tic()\n",
    "# gt = get_ground_truth(gt_id).loc[:,['lat','lon','start_date',gt_col]]\n",
    "gt = data_loaders.get_ground_truth(gt_id).loc[:,['lat','lon','start_date',gt_col]]\n",
    "toc()\n",
    "printf('Pivoting ground truth to have one row per start_date')\n",
    "tic()\n",
    "gt = gt.loc[gt.start_date.isin(data.index),['lat','lon','start_date',gt_col]].set_index(['lat','lon','start_date']).unstack(['lat','lon'])\n",
    "toc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printf(\"Merging ground truth\")\n",
    "tic()\n",
    "data = data.join(gt, how=\"left\") \n",
    "# del gt\n",
    "toc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printf('Extracting target variable (ground-truth - base prediction) and dropping NAs')\n",
    "tic()\n",
    "target = (data[gt_col] - data[base_col]).dropna(how='any')\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditioning\n",
    "if mei or mjo:\n",
    "    conditioning_data = data_loaders.load_combined_data('date_data', gt_id, horizon)\n",
    "    conditioning_columns = get_conditioning_cols(gt_id, horizon, mei=mei, mjo=mjo)\n",
    "    # Combined data start dates and gt start dates don't fully overlap\n",
    "    conditioned_targets = pd.DataFrame(gt.index).merge(conditioning_data[[\"start_date\"] + conditioning_columns], on=\"start_date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for each target date\n",
    "printf('Creating dataframes to store performance and date-based covariates')\n",
    "tic()\n",
    "rmses = pd.Series(index=target_date_objs, dtype=np.float64)\n",
    "X = pd.DataFrame(index=target.index, columns = [\"delta\", \"dividend\", \"remainder\"], \n",
    "                 dtype=np.float64)\n",
    "toc()\n",
    "printf('Initializing target date predictions to base column')\n",
    "tic()\n",
    "# Only form predictions for target dates in data matrix\n",
    "valid_targets = data.index.intersection(target_date_objs)\n",
    "preds = data.loc[valid_targets, base_col]\n",
    "preds.index.name = \"start_date\"\n",
    "# Order valid targets by day of week\n",
    "valid_targets = valid_targets[valid_targets.weekday.argsort(kind='stable')]\n",
    "toc()\n",
    "days_per_year = 365.242199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_date_obj in valid_targets:\n",
    "    # Skip if forecast already produced for this target\n",
    "    target_date_str = datetime.strftime(target_date_obj, '%Y%m%d')\n",
    "    forecast_file = get_forecast_filename(\n",
    "        model=model_name, submodel=submodel_name, \n",
    "        gt_id=gt_id, horizon=horizon, \n",
    "        target_date_str=target_date_str)\n",
    "    if os.path.isfile(forecast_file):\n",
    "        printf(f\"prior forecast exists for target={target_date_obj}\")\n",
    "        pred = pd.read_hdf(forecast_file).set_index(['lat','lon']).pred - preds.loc[target_date_obj,:]\n",
    "    else:\n",
    "        tic()\n",
    "        printf(f\"Preparing covariates for {target_date_str}\")\n",
    "        # Compute days from target date\n",
    "        X['delta'] = (target_date_obj - target.index).days\n",
    "        # Extract the dividend and remainder when delta is divided by the number of days per year\n",
    "        # The dividend is analogous to the year\n",
    "        # (Negative values will ultimately be excluded)\n",
    "        X['dividend'] = np.floor(X.delta / days_per_year)\n",
    "        # The remainder is analogous to the day of the year\n",
    "        X['remainder'] = np.floor(X.delta % days_per_year)\n",
    "        # Find the last observable training date for this target\n",
    "        last_train_date = target_date_obj - gt_delta\n",
    "        # Restrict data based on training date, dividend, and remainder\n",
    "        if mei or mjo:\n",
    "            target_conditioning_val = conditioning_data[conditioning_data.start_date == target_date_obj][conditioning_columns].values[0]\n",
    "            indic = cond_indices(conditioned_targets, conditioning_columns, target_conditioning_val)\n",
    "            indic &= (X.index <= last_train_date)\n",
    "        else:\n",
    "            indic = (X.index <= last_train_date)\n",
    "        if margin_in_days is not None:\n",
    "            indic &= ((X.remainder <= margin_in_days) | (X.remainder >= 365-margin_in_days))\n",
    "        if train_years != \"all\":\n",
    "            indic = indic & (X.dividend < train_years)\n",
    "        toc()\n",
    "        printf(f'Fitting {model_name} model with loss {loss} for {target_date_obj}')\n",
    "        tic()\n",
    "        if fit_intercept and not indic.any():\n",
    "            printf(f'-Warning: no training data for {target_date_str}; using base prediction')\n",
    "            # Do not adjust base prediction\n",
    "            pred = 0\n",
    "        elif fit_intercept:\n",
    "            # Add learned prediction to base prediction\n",
    "            pred = estimator(target.loc[indic,:])\n",
    "            preds.loc[target_date_obj,:] += pred\n",
    "        else:\n",
    "            # Do not adjust base prediction\n",
    "            pred = 0\n",
    "        # Save prediction to file in standard format\n",
    "        if target_date_obj.year >= LAST_SAVE_YEAR:\n",
    "            save_forecasts(\n",
    "                preds.loc[[target_date_obj],:].unstack().rename(\"pred\").reset_index(),\n",
    "                model=model_name, submodel=submodel_name, \n",
    "                gt_id=gt_id, horizon=horizon, \n",
    "                target_date_str=target_date_str)\n",
    "        toc()\n",
    "    # Evaluate and store error if we have ground truth data\n",
    "    tic()\n",
    "    if target_date_obj in target.index:\n",
    "        rmse = np.sqrt(np.square(pred - target.loc[target_date_obj,:]).mean())\n",
    "        rmses.loc[target_date_obj] = rmse\n",
    "        print(\"-rmse: {}, score: {}\".format(rmse, mean_rmse_to_score(rmse)))\n",
    "        mean_rmse = rmses.mean()\n",
    "        print(\"-mean rmse: {}, running score: {}\".format(mean_rmse, mean_rmse_to_score(mean_rmse)))\n",
    "    toc()\n",
    "printf(\"Save rmses in standard format\")\n",
    "rmses = rmses.sort_index().reset_index()\n",
    "rmses.columns = ['start_date','rmse']\n",
    "save_metric(rmses, model=model_name, submodel=submodel_name, gt_id=gt_id, horizon=horizon, target_dates=target_dates, metric=\"rmse\")\n",
    "save_metric(rmses, model=f'{forecast}pp', submodel=submodel_name, gt_id=gt_id, horizon=horizon, target_dates=target_dates, metric=\"rmse\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
