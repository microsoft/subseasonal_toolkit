{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLLR: local linear regression with multitask feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure notebook is being run from base repository directory\n",
    "import os, sys\n",
    "# Disable numpy multithreading\n",
    "os.environ.update(\n",
    "    OMP_NUM_THREADS = '1',\n",
    "    OPENBLAS_NUM_THREADS = '1',\n",
    "    NUMEXPR_NUM_THREADS = '1',\n",
    "    MKL_NUM_THREADS = '1',\n",
    ")\n",
    "from subseasonal_toolkit.utils.notebook_util import isnotebook\n",
    "if isnotebook():\n",
    "    # Autoreload packages that are modified\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "else:\n",
    "    from argparse import ArgumentParser\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "from ttictoc import tic, toc\n",
    "from subseasonal_data.utils import get_measurement_variable\n",
    "from subseasonal_toolkit.utils.general_util import printf, num_available_cpus, make_directories\n",
    "from subseasonal_toolkit.utils.experiments_util import get_first_year, get_start_delta, clim_merge, month_day_subset\n",
    "from subseasonal_toolkit.utils.fit_and_predict import apply_parallel\n",
    "from subseasonal_toolkit.utils.skill import skill_report, skill_report_summary_stats_multicol\n",
    "from subseasonal_toolkit.models.multillr.stepwise_util import *\n",
    "from subseasonal_toolkit.utils.models_util import (get_submodel_name, start_logger, log_params, get_forecast_filename,\n",
    "                                                   save_forecasts)\n",
    "from subseasonal_toolkit.utils.eval_util import get_target_dates\n",
    "from subseasonal_data import data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Specify model parameters\n",
    "#\n",
    "model_name = \"multillr\"\n",
    "\n",
    "if not isnotebook():\n",
    "    # If notebook run as a script, parse command-line arguments\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"pos_vars\",nargs=\"*\")  # gt_id and horizon                                                                                  \n",
    "    parser.add_argument('--target_dates', '-t', default=\"std_test\")\n",
    "    # margin_in_days: regression will train on all dates within margin_in_days \n",
    "    #  of the target day and month in each training year\n",
    "    # Set to 0 to include only target month-day combo\n",
    "    # Set to \"None\" to include entire year\n",
    "    parser.add_argument('--margin_in_days', '-m', default=56)\n",
    "    # criterion: string criterion to use for backward stepwise (use \"mean\" to reproduce results in paper).\n",
    "    #  Choose from: mean, mean_over_sd, similar_mean, similar_mean_over_sd, similar_quantile_0.5,\n",
    "    #  similar_quantile_0.25, similar_quantile_0.1 or create your own in the function\n",
    "    #  skill_report_summary_stats in skill.py\n",
    "    parser.add_argument('--criterion', default=\"mean\") \n",
    "    parser.add_argument('--metric', default=\"rmse\",\n",
    "                        help=\"metric for assessing improvement in \"\n",
    "                             \"\\{\\'cos\\', \\'rmse\\', \\'mse\\'\\}\")\n",
    "    parser.add_argument('--num_cores', default=num_available_cpus(),\n",
    "                       help=\"number of cores to use in execution\")\n",
    "    parser.add_argument('--date_order_seed', default=\"None\",\n",
    "                       help=\"None or integer determining random order in which \"\n",
    "                            \"target dates are processed; if None, target dates \"\n",
    "                            \"are sorted by day of the week\")\n",
    "    args, opt = parser.parse_known_args()\n",
    "    \n",
    "    # Assign variables\n",
    "    gt_id = args.pos_vars[0] # \"contest_precip\" or \"contest_tmp2m\"                                                                            \n",
    "    horizon = args.pos_vars[1] # \"34w\" or \"56w\"                                                                                        \n",
    "    target_dates = args.target_dates\n",
    "    if args.margin_in_days == \"None\":\n",
    "        margin_in_days = None\n",
    "    else:\n",
    "        margin_in_days = int(args.margin_in_days)\n",
    "    criterion = args.criterion\n",
    "    metric = args.metric\n",
    "    num_cores = args.num_cores\n",
    "    if args.date_order_seed == \"None\":\n",
    "        date_order_seed = None\n",
    "    else:\n",
    "        date_order_seed = int(args.date_order_seed)\n",
    "else:\n",
    "    # Otherwise, specify arguments interactively \n",
    "    gt_id = \"contest_tmp2m\" \n",
    "    horizon = \"56w\" \n",
    "    target_dates = \"20210223\"\n",
    "    margin_in_days = 56\n",
    "    criterion = 'mean' \n",
    "    metric = 'rmse'\n",
    "    num_cores = num_available_cpus()\n",
    "    date_order_seed = None\n",
    "\n",
    "#\n",
    "# Process model parameters\n",
    "#\n",
    "\n",
    "# Ensure metric is valid\n",
    "valid_metrics = ['rmse','mse','cos']\n",
    "if metric not in valid_metrics:\n",
    "    raise ValueError(f\"Unknown metric {metric}. Please choose from {valid_metrics}.\")\n",
    "\n",
    "# Get list of target date objects\n",
    "target_date_objs = pd.Series(get_target_dates(date_str=target_dates,horizon=horizon))\n",
    "if date_order_seed is None:\n",
    "    # Sort target_date_objs by day of week\n",
    "    target_date_objs = target_date_objs[target_date_objs.dt.weekday.argsort(kind='stable')]\n",
    "else:\n",
    "    # Sort target_date_objs in random order\n",
    "    target_date_objs = target_date_objs.sample(frac=1, random_state=date_order_seed)\n",
    "\n",
    "# Store delta between target date and forecast issuance date\n",
    "# forecast_delta = timedelta(days=get_forecast_delta(horizon))\n",
    "forecast_delta =  timedelta(days=get_start_delta(horizon, gt_id))\n",
    "\n",
    "\n",
    "# Identify measurement variable name\n",
    "measurement_variable = get_measurement_variable(gt_id) # 'tmp2m' or 'precip'\n",
    "\n",
    "# column names for gt_col, clim_col and anom_col\n",
    "gt_col = measurement_variable\n",
    "clim_col = measurement_variable+\"_clim\"\n",
    "anom_col = get_measurement_variable(gt_id)+\"_anom\" # 'tmp2m_anom' or 'precip_anom'\n",
    "\n",
    "# anom_inv_std_col: column name of inverse standard deviation of anomalies for each start_date\n",
    "anom_inv_std_col = anom_col+\"_inv_std\"\n",
    "\n",
    "#\n",
    "# Default stepwise parameter values\n",
    "#\n",
    "# Define candidate predictors\n",
    "initial_candidate_x_cols = default_stepwise_candidate_predictors(gt_id, horizon)\n",
    "\n",
    "### TODO: REMOVE THIS BLOCK OF CODE\n",
    "if False:\n",
    "    initial_candidate_x_cols = [col for col in initial_candidate_x_cols if not col.startswith(\"phase_\")]\n",
    "\n",
    "# Copy the list of candidates for later modification\n",
    "### TODO: unnecessary to have both initial_candidate and candidate; get rid of one\n",
    "candidate_x_cols = initial_candidate_x_cols[:]\n",
    "# Tolerance for convergence: if performance hurt by more than tolerance, terminate.\n",
    "tolerance = 0.001 if metric == 'rmse' else 0.01\n",
    "# Whether to use margin days (days around the target date) in assessing\n",
    "use_margin = False\n",
    "# Whether metric should be maximized or minimized\n",
    "maximize_metric = metric in {'cos','cos_margin'}\n",
    "\n",
    "\n",
    "#\n",
    "# Default regression parameter values\n",
    "#\n",
    "# anom_scale_col: multiply anom_col by this amount prior to prediction\n",
    "# (e.g., 'ones' or anom_inv_std_col)\n",
    "anom_scale_col = 'ones'\n",
    "# pred_anom_scale_col: multiply predicted anomalies by this amount\n",
    "# (e.g., 'ones' or anom_inv_std_col)\n",
    "pred_anom_scale_col = 'ones'\n",
    "# choose first year to use in training set\n",
    "first_train_year = 1948 if gt_id == 'contest_precip' else 1979\n",
    "# columns to group by when fitting regressions (a separate regression\n",
    "# is fit for each group); use ['ones'] to fit a single regression to all points\n",
    "group_by_cols = ['lat', 'lon']\n",
    "# base_col: column which should be subtracted from gt_col prior to prediction\n",
    "# (e.g., this might be clim_col or a baseline predictor like NMME);\n",
    "# if None, subtract nothing\n",
    "base_col = None\n",
    "# Name the collection of supporting columns\n",
    "# Discard None if it exists\n",
    "supporting_cols = set([base_col,clim_col,anom_col]).difference([None])\n",
    "\n",
    "#\n",
    "# Record submodel names\n",
    "#\n",
    "### TODO: decide whether name_to_params json file should sit somewhere else\n",
    "submodel_name = get_submodel_name(\n",
    "    model_name, margin_in_days=margin_in_days, criterion=criterion,\n",
    "    metric=metric, x_cols = initial_candidate_x_cols)\n",
    "\n",
    "if not isnotebook():\n",
    "    # Save output to log file\n",
    "    logger = start_logger(model=model_name,submodel=submodel_name,gt_id=gt_id,\n",
    "                          horizon=horizon,target_dates=target_dates)\n",
    "    # Store parameter values in log                                                                                                                        \n",
    "    params_names = ['gt_id', 'horizon', 'target_dates',\n",
    "                    'margin_in_days', 'criterion',\n",
    "                    'metric', 'initial_candidate_x_cols'\n",
    "                   ]\n",
    "    params_values = [eval(param) for param in params_names]\n",
    "    log_params(params_names, params_values)\n",
    "\n",
    "#\n",
    "# Create directory for storing model outputs, like\n",
    "# convergence indicators, prediction paths, and summary statistics\n",
    "#\n",
    "task = f\"{gt_id}_{horizon}\"\n",
    "outdir = os.path.join('models', model_name, 'storage', submodel_name, task)\n",
    "make_directories(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load and process dataset\n",
    "#\n",
    "### TODO: can we disable numpy multithreading after this step so feather can load data in parallel?\n",
    "# Load relevant data columns, excluding clim_col\n",
    "relevant_cols = supporting_cols.union(['start_date','lat','lon']+group_by_cols+\n",
    "    candidate_x_cols+[anom_scale_col,pred_anom_scale_col]) - set([clim_col])\n",
    "if gt_id.endswith(\"precip\"):\n",
    "    data = data_loaders.load_combined_data('all_data', gt_id, horizon, columns=relevant_cols - set(['tmp2m_clim']))\n",
    "else:\n",
    "    data = data_loaders.load_combined_data('all_data', gt_id, horizon, columns=relevant_cols)\n",
    "\n",
    "last_target_date = target_date_objs.max()\n",
    "printf(f\"Restricting data to years >= {first_train_year}\"\n",
    "       f\" and dates <= {last_target_date}\")\n",
    "tic()\n",
    "data = data[(data.start_date <= last_target_date) &\n",
    "            (data.start_date >= f\"{first_train_year}-01-01\")]\n",
    "toc()\n",
    "\n",
    "printf(\"Merging in climatology\")\n",
    "tic()\n",
    "clim = data_loaders.get_climatology(gt_id).rename(columns={gt_col:clim_col})\n",
    "data = clim_merge(data, clim)\n",
    "toc()\n",
    "\n",
    "printf(f\"Adding sample weight and target columns\")\n",
    "tic()\n",
    "### TODO: could define sample_weight_col = 'ones' in typical case\n",
    "# To minimize the mean-squared error between predictions of the form\n",
    "# (f(x_cols) + base_col - clim_col) * pred_anom_scale_col\n",
    "# and a target of the form anom_col * anom_scale_col, we will\n",
    "# estimate f using weighted least squares with datapoint weights\n",
    "# pred_anom_scale_col^2 and effective target variable\n",
    "# anom_col * anom_scale_col / pred_anom_scale_col + clim_col - base_col\n",
    "data['sample_weight'] = data[pred_anom_scale_col]**2\n",
    "# Ensure that we do not divide by zero when dividing by pred_anom_scale_col\n",
    "data['target'] = (data[clim_col] - (0 if base_col is None else data[base_col]) +\n",
    "                  data[anom_col] * data[anom_scale_col] /\n",
    "                    (data[pred_anom_scale_col]+(data[pred_anom_scale_col]==0)))\n",
    "# Add sample_weight and target to supporting cols\n",
    "supporting_cols.update(['sample_weight','target'])\n",
    "toc()\n",
    "\n",
    "### TODO: Include tmp2m_clim in precip lat_lon_date_data\n",
    "if gt_id.endswith(\"precip\") and \"tmp2m_clim\" in candidate_x_cols:\n",
    "    print(\"Adding tmp2m_clim to dataframe\")\n",
    "    tic()\n",
    "    tmp_clim = data_loaders.get_climatology(gt_id.replace(\"precip\",\"tmp2m\")).rename(columns={'tmp2m':'tmp2m_clim'})\n",
    "    data = data.merge(tmp_clim[['tmp2m_clim']], left_on=['lat','lon',data.start_date.dt.month,data.start_date.dt.day], \n",
    "                      right_on=[tmp_clim.lat, tmp_clim.lon, tmp_clim.start_date.dt.month, tmp_clim.start_date.dt.day],\n",
    "                      how=\"left\").drop(['key_2', 'key_3'], axis=1)\n",
    "    toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    ###TODO: REMOVE ME\n",
    "    cfsv2 = load_measurement(\"data/dataframes/subx-cfsv2-tmp2m-all_leads-4_periods_avg.h5\", shift=30)\n",
    "    cfsv2.set_index(['lat','lon','start_date'],inplace=True)\n",
    "    data = pd.merge(data, cfsv2[[\"subx_cfsv2_tmp2m-0.5d_shift30\",\"subx_cfsv2_tmp2m-28.5d_shift30\"]],\n",
    "                   left_on=['lat','lon','start_date'], right_index=True,\n",
    "                   how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_rolling_linear_regression(X, y, sample_weight, t, threshold_date, \n",
    "                                       ridge=0.0, test_X=None, test_t=None):\n",
    "    \"\"\"Fits backward rolling weighted ridge regression without an intercept.  \n",
    "    For the equivalent threshold date in each year, \n",
    "    forms 'hindcast' predictions based on leaving out one year\n",
    "    of training data at a time.  Fits regression with each column of X withheld \n",
    "    and returns hindcast predictions for each model fit in a DataFrame with \n",
    "    columns corresponding to the withheld column name.\n",
    "    \n",
    "    Args:\n",
    "       X: feature matrix\n",
    "       y: target vector\n",
    "       sample_weight: weight assigned to each datapoint\n",
    "       t: DateTimeIndex corresponding to rows of X\n",
    "       threshold_date: Cutoff used to determine holdout batch boundaries (must not be Feb. 29);\n",
    "          each batch runs from threshold_date in one year (exclusive) to threshold_date\n",
    "          in subsequent year (inclusive)\n",
    "       ridge (optional): regularization parameter for ridge regression objective\n",
    "          [sum_i (y_i - <w, x_i>)^2] + ridge ||w||_2^2\n",
    "       test_X: test feature matrix; if None, use X as the test feature matrix\n",
    "       test_t: DateTimeIndex corresponding to rows of test_X; if test_X is None, use t\n",
    "    \"\"\"\n",
    "    # Process test set arguments\n",
    "    if test_X is None:\n",
    "        test_X = X\n",
    "        test_t = t\n",
    "    \n",
    "    # Extract size of feature matrix\n",
    "    (n, p) = X.shape\n",
    "    \n",
    "    # Multiply y and X by the sqrt of the sample weights\n",
    "    if all(sample_weight == 1):\n",
    "        wtd_y = y\n",
    "        wtd_X = X\n",
    "    else:\n",
    "        sqrt_sample_weight = np.sqrt(sample_weight)\n",
    "        wtd_y = y * sqrt_sample_weight\n",
    "        wtd_X = X.multiply(sqrt_sample_weight, axis=0)\n",
    "    \n",
    "    # Maintain training sufficient statistics for linear regression\n",
    "    Xty = np.dot(wtd_X.T, wtd_y)\n",
    "    XtX = np.zeros((p,p))\n",
    "    # Set diagonal of XtX to ridge regularization parameter\n",
    "    if ridge != 0:\n",
    "        np.fill_diagonal(XtX, ridge)\n",
    "    # Add data component of XtX\n",
    "    XtX += np.dot(wtd_X.T, wtd_X)\n",
    "\n",
    "    # Store predictions associated with each model in dataframe\n",
    "    preds = pd.DataFrame(index=test_X.index, columns=test_X.columns, \n",
    "                         dtype=np.float64)\n",
    "    \n",
    "    # Find range of years in dataset\n",
    "    first_year = min(t.min().year, test_t.min().year)\n",
    "    last_year = max(t.max().year, test_t.max().year)\n",
    "\n",
    "    #\n",
    "    # Produce hindcast predictions\n",
    "    #            \n",
    "    # Form hindcast predictions for all blocks\n",
    "    #printf(\"Form hindcast predictions for each block with each column removed\")\n",
    "    #tic()\n",
    "    for year in range(first_year,last_year+2):\n",
    "        # Identify unweighted test data for prediction\n",
    "        upper_threshold_date = threshold_date.replace(year = year)\n",
    "        lower_threshold_date = threshold_date.replace(year = year-1)\n",
    "        test_date_block = ((test_t <= upper_threshold_date) & \n",
    "                           (test_t > lower_threshold_date))\n",
    "        test_X_slice = test_X[test_date_block]\n",
    "        if test_X_slice.empty:\n",
    "            # No predictions to make for this block\n",
    "            continue\n",
    "        # Find block of training dates for this year's threshold\n",
    "        date_block = ((t <= upper_threshold_date) & \n",
    "                  (t > lower_threshold_date))\n",
    "        # Form training data with current date block removed\n",
    "        X_slice = wtd_X[date_block]\n",
    "        y_slice = wtd_y[date_block]\n",
    "        train_XtX = XtX-np.dot(X_slice.T,X_slice)\n",
    "        train_Xty = Xty-np.dot(X_slice.T,y_slice)\n",
    "        for col_ind, col in enumerate(X.columns):\n",
    "            # Fit coefficients using all columns exept for col\n",
    "            train_col_inds = list(range(col_ind)) + list(range(col_ind+1, p))\n",
    "            # Find minimum norm solution (to be robust to poor conditioning)\n",
    "            # Use numpy broadcast indexing to obtain submatrix of train_XtX\n",
    "            coef = np.linalg.lstsq(train_XtX[np.ix_(train_col_inds,train_col_inds)], \n",
    "                                   train_Xty[train_col_inds], rcond=None)[0]\n",
    "#             try:\n",
    "#                 # Solve linear system when first argument is full rank\n",
    "#                 # Use numpy broadcast indexing to obtain submatrix of train_XtX\n",
    "#                 coef = np.linalg.solve(train_XtX[np.ix_(train_col_inds,train_col_inds)], \n",
    "#                                        train_Xty[train_col_inds])\n",
    "#             except np.linalg.LinAlgError:\n",
    "#                 #printf(f\"Not full rank: {year}, {col}\")\n",
    "#                 # Otherwise, find minimum norm solution\n",
    "#                 coef = np.linalg.lstsq(train_XtX[np.ix_(train_col_inds,train_col_inds)], \n",
    "#                                        train_Xty[train_col_inds], rcond=None)[0]\n",
    "            \n",
    "            # Store predictions on this block of dates for this candidate using unweighted data\n",
    "            preds.loc[test_date_block,col] = np.dot(test_X_slice.iloc[:,train_col_inds], coef)\n",
    "    #toc()\n",
    "    # Return predictions\n",
    "    return preds\n",
    "\n",
    "def backward_rolling_linear_regression_wrapper(\n",
    "    df, x_cols=None, base_col=None, clim_col=None, anom_col=None, \n",
    "    last_train_date=None, ridge=0.0, target_date=None):\n",
    "    \"\"\"Wrapper for backward_rolling_linear_regression that selects appropriate \n",
    "    training and test sets from df, associates sample weights with each training datapoint, \n",
    "    carries out rolling linear regression with each column removed, and for each test point\n",
    "    returns hindcast anomalies per candidate column with ground truth anomalies and climatology\n",
    "    \n",
    "    Args:\n",
    "        df: Dataframe with columns 'start_date', 'lat', 'lon', \n",
    "           clim_col, anom_col, x_cols, 'target', 'sample_weight'\n",
    "        x_cols: Names of columns used as input features\n",
    "        base_col: Name of column subtracted from target prior to prediction (or None\n",
    "           if no column should be subtracted)\n",
    "        clim_col: Name of climatology column in df\n",
    "        anom_col: Name of ground truth anomaly column in df\n",
    "        last_train_date: last date on which to train and cutoff used to determine \n",
    "           holdout batch boundaries (must not be Feb. 29); each batch runs from \n",
    "           last_train_date in one year (exclusive) to last_train_date\n",
    "           in subsequent year (inclusive)\n",
    "        ridge (optional): regularization parameter for ridge regression objective\n",
    "           [sum_i (y_i - <w, x_i>)^2] + ridge ||w||_2^2\n",
    "        target_date (optional): if not None, returns predictions only on points with the\n",
    "           same month-day combination as target_date; otherwise, returns predictions\n",
    "           on all points\n",
    "\n",
    "    Returns predictions representing f(x_cols) + base_col - clim_col       \n",
    "    \"\"\"\n",
    "    # Restrict to datapoints with valid features\n",
    "    df = df.dropna(subset=x_cols)\n",
    "    # Select training set by dropping training points after last_train_date\n",
    "    # and points with invalid target or sample weights\n",
    "    train_df = df.xs(\n",
    "        slice(None, last_train_date), level='start_date', drop_level=False).dropna(\n",
    "        subset=['target','sample_weight'])\n",
    "    \n",
    "    if target_date is None:\n",
    "        # Test on all points if no target date given\n",
    "        test_df = df\n",
    "    else:\n",
    "        # Test on points with same month-day combo as target\n",
    "        dates = df.index.get_level_values(level='start_date')\n",
    "        test_df = df[(dates.month == target_date.month) & (dates.day == target_date.day)]\n",
    "\n",
    "    # Collect predictions and add base column minus climatology to predictions,\n",
    "    if base_col is None:\n",
    "        base_minus_clim = - test_df[clim_col].values\n",
    "    else:\n",
    "        base_minus_clim = test_df[base_col].values - test_df[clim_col].values\n",
    "    preds = backward_rolling_linear_regression(\n",
    "        train_df[x_cols],\n",
    "        train_df['target'], \n",
    "        train_df['sample_weight'],\n",
    "        train_df.index.get_level_values(level='start_date'),\n",
    "        last_train_date, \n",
    "        ridge = ridge,\n",
    "        test_X = test_df[x_cols],\n",
    "        test_t = test_df.index.get_level_values(level='start_date')).add(\n",
    "        base_minus_clim, axis = 'index')\n",
    "    # Return dataframe with predicted anomalies, ground truth anomalies, and climatology\n",
    "    preds = preds.assign(truth=test_df[anom_col].values,\n",
    "                         clim=test_df[clim_col].values)\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: deal with leap days\n",
    "### TODO: deal with fact that different predictors defined over different dates?\n",
    "#\n",
    "# Form predictions for each target date\n",
    "#\n",
    "for target_date_obj in target_date_objs: \n",
    "    # If any features are missing for target date, skip\n",
    "    if data.loc[data.start_date == target_date_obj, candidate_x_cols].isnull().values.any():\n",
    "        printf(f\"warning: some features unavailable for target={target_date_obj}; skipping\")\n",
    "        printf(\"{}\".format(data.loc[data.start_date == target_date_obj, candidate_x_cols].isnull().any()))\n",
    "        continue\n",
    "        \n",
    "    target_date_str = datetime.strftime(target_date_obj, '%Y%m%d')\n",
    "    printf(f'\\ntarget={target_date_str}')\n",
    "    \n",
    "    # Check if algorithm has already converged for this target\n",
    "    converged_outfile = os.path.join(outdir, 'converged-'+target_date_str)\n",
    "    if os.path.exists(converged_outfile):\n",
    "        # If algorithm has converged previously, skip\n",
    "        printf('{} already exists; skipping.'.format(converged_outfile))\n",
    "        continue\n",
    "\n",
    "    #\n",
    "    # Subset data for training\n",
    "    #\n",
    "    \n",
    "    # Find the last observable training date for this target\n",
    "    last_train_date = target_date_obj - forecast_delta\n",
    "    if (last_train_date.day == 29) and (last_train_date.month == 2):\n",
    "        # Treat Feb. 29 as Feb. 28\n",
    "        last_train_date = last_train_date.replace(day = 28)\n",
    "\n",
    "    printf(\"Restricting data to margin around target date\")\n",
    "    tic()\n",
    "    if margin_in_days is not None:\n",
    "        ### TODO: is there a faster way to do this?  maybe by indexing by start_date, lat, lon\n",
    "        ### If we load dayofyear as a feature, could just measure proximity to target date's climpp?\n",
    "        sub_data = month_day_subset(data, target_date_obj, margin_in_days)\n",
    "    else:\n",
    "        sub_data = data\n",
    "    toc()\n",
    "    printf(\"Dropping rows beyond target date and setting index\")\n",
    "    tic()\n",
    "    sub_data = sub_data.loc[\n",
    "        sub_data.start_date <= target_date_obj,:].set_index(\n",
    "        ['lat','lon','start_date'])\n",
    "    toc()\n",
    "\n",
    "    #\n",
    "    # Store target-date predictions and summary stats of each model on stepwise path\n",
    "    #\n",
    "    preds_outfile = os.path.join(outdir, 'preds-'+target_date_str+'.h5')\n",
    "    stats_outfile = os.path.join(outdir, 'stats-'+target_date_str+'.pkl')\n",
    "    if os.path.exists(preds_outfile) and os.path.exists(stats_outfile):\n",
    "        # If preds and stats already exist on disk, load them and start from the latest model\n",
    "        printf('Loading existing predictions and stats')\n",
    "        tic()\n",
    "        path_preds = pd.read_hdf(preds_outfile, key=\"data\")\n",
    "        path_stats = pickle.load( open( stats_outfile, \"rb\" ) )\n",
    "        # The set of predictors in the latest model is specified by the last column name in\n",
    "        # path_preds\n",
    "        model_str = path_preds.columns[-1]\n",
    "        current_x_col_set = eval(model_str)\n",
    "        x_cols_current = list(current_x_col_set)\n",
    "        printf(x_cols_current)\n",
    "        # Enumerate non-model columns in path_preds\n",
    "        non_model_cols = ['lat','lon','start_date','truth','clim']\n",
    "        # Find the last feature removed from x cols\n",
    "        best_x_col = (\n",
    "            set(candidate_x_cols).symmetric_difference(current_x_col_set).pop() \n",
    "            if (len(path_preds.columns)-len(non_model_cols)) == 1 \n",
    "            else current_x_col_set.symmetric_difference(\n",
    "                eval(path_preds.columns[-2])).pop())\n",
    "        printf(best_x_col)\n",
    "        # Reconstruct the current best criterion\n",
    "        best_criterion_current = path_stats[model_str][criterion][best_x_col]\n",
    "        printf(best_criterion_current)\n",
    "        # Last completed model round\n",
    "        model_round = len(path_preds.columns) - 5\n",
    "        toc()\n",
    "    else:\n",
    "        path_preds = data.loc[data.start_date == target_date_obj,\n",
    "                              ['lat','lon','start_date',anom_col,clim_col]].copy()\n",
    "        path_preds = path_preds.rename(index=str, columns={anom_col: \"truth\", clim_col: \"clim\"})\n",
    "        path_stats = {}\n",
    "        x_cols_current = candidate_x_cols[:]\n",
    "        ### TODO: eventually replace with the score of training the full model\n",
    "        best_criterion_current = -np.inf if maximize_metric else np.inf\n",
    "        model_round = 0\n",
    "        ### TODO: eventually get rid of set call and sort list of values?\n",
    "        model_str = str(set(x_cols_current))\n",
    "    \n",
    "    #\n",
    "    # Fit model\n",
    "    #\n",
    "    tic()\n",
    "    printf(\"\\nFitting backward stepwise regression\")\n",
    "    converged = False\n",
    "    while not converged:\n",
    "        model_round += 1\n",
    "        printf(f\"\\nBackward stepwise regression; round {model_round}\")\n",
    "        tt = time.time()\n",
    "        gc.collect()\n",
    "        criteria = {}\n",
    "        ### TODO: appropriately handle the empty set of predictors case (just predict 0)\n",
    "        if not x_cols_current:\n",
    "            converged = True\n",
    "            break\n",
    "\n",
    "        relevant_cols = supporting_cols.union(x_cols_current)\n",
    "        printf(\"Fitting model with core predictors {}\".format(x_cols_current))\n",
    "        tic()\n",
    "        preds = apply_parallel(sub_data.loc[:,relevant_cols].groupby(group_by_cols),\n",
    "            backward_rolling_linear_regression_wrapper, num_cores,\n",
    "#         preds = sub_data.loc[:,relevant_cols].groupby(group_by_cols).apply(\n",
    "#             backward_rolling_linear_regression_wrapper, \n",
    "            x_cols=x_cols_current,\n",
    "            base_col=base_col,\n",
    "            clim_col=clim_col,\n",
    "            anom_col=anom_col,\n",
    "            last_train_date=last_train_date,\n",
    "            target_date=target_date_obj)\n",
    "        toc()\n",
    "        # Ensure raw precipitation predictions are never less than zero\n",
    "        if gt_id.endswith(\"precip\"):\n",
    "            printf(\"Ensure predicted precipitation >= 0\")\n",
    "            tic()\n",
    "            preds[x_cols_current] = np.maximum(\n",
    "                preds[x_cols_current].add(preds.clim, axis=0),0).subtract(\n",
    "                preds.clim, axis=0)\n",
    "            toc()\n",
    "        preds = preds.reset_index()\n",
    "        # Assess prediction quality\n",
    "        printf(\"Getting skills\")\n",
    "        tic()\n",
    "        skills = skill_report(preds, target_date_obj,\n",
    "                              pred_cols=x_cols_current,\n",
    "                              gt_anom_col='truth',\n",
    "                              clim_col='clim',\n",
    "                              include_trunc0 = False,\n",
    "                              include_cos = metric == 'cos',\n",
    "                              include_cos_margin = metric == 'cos_margin',\n",
    "                              include_mse_by_latlon = False,\n",
    "                              verbose = False)\n",
    "        # Remove the target year from the skills dataframe so it isn't used in evaluation\n",
    "        skills[metric] = skills[metric][skills[metric].index != target_date_obj]\n",
    "        toc()\n",
    "        summary_stats = skill_report_summary_stats_multicol(\n",
    "            skills, metric = metric, use_margin = metric=='cos_margin')\n",
    "        # Pick best column based on summary stats, and\n",
    "        # compute difference from current best performance\n",
    "        criteria = summary_stats[criterion]\n",
    "        if maximize_metric:\n",
    "            # Larger criterion values are better\n",
    "            best_x_col = criteria.idxmax()\n",
    "            improvement = criteria[best_x_col] - best_criterion_current\n",
    "        else: \n",
    "            # Smaller criterion values are better\n",
    "            best_x_col = criteria.idxmin()\n",
    "            improvement = best_criterion_current - criteria[best_x_col]\n",
    "        printf(\"-old criterion = {}, new criterion = {}, best_x_col = {}\".format(\n",
    "            best_criterion_current, criteria[best_x_col], best_x_col))\n",
    "        printf(\"-improvement = {}, tolerance = -{}\".format(improvement, tolerance))\n",
    "\n",
    "        if improvement <= -tolerance:\n",
    "            # Removing predictor is too costly; we're done\n",
    "            converged = True\n",
    "            printf(\"Model has converged\")\n",
    "            printf(\"--round {} elapsed time: {}s\".format(model_round, time.time() - tt))\n",
    "            break\n",
    "\n",
    "        # Otherwise, performance hit is within tolerance:\n",
    "        # Remove from model\n",
    "        x_cols_current.remove(best_x_col)\n",
    "        best_criterion_current = criteria[best_x_col]\n",
    "        printf(\"Removing {} from model, current criterion is {}\".format(best_x_col, best_criterion_current))\n",
    "        # Store the predictions of the selected model\n",
    "        printf(\"Storing the predictions of the selected model\")\n",
    "        tic()\n",
    "        path_preds = pd.merge(path_preds,\n",
    "                              preds.loc[preds.start_date == target_date_obj,\n",
    "                                        ['lat','lon',best_x_col]],\n",
    "                              on=[\"lat\",\"lon\"], how=\"left\");\n",
    "        # Rename added column to reflect the set of predictors in the model\n",
    "        model_str = str(set(x_cols_current))\n",
    "        path_preds = path_preds.rename(\n",
    "            index=str, columns = { best_x_col : model_str })\n",
    "        # Store summary stats of the selected model\n",
    "        path_stats[model_str] = summary_stats\n",
    "        toc()\n",
    "        #\n",
    "        # Save predictions and summary stats to disk after each round\n",
    "        printf(f\"Saving predictions and summary stats to disk after round {model_round}\")\n",
    "        # Write path predictions to file\n",
    "        tic()\n",
    "        path_preds.to_hdf(preds_outfile, key=\"data\", mode=\"w\", format=\"table\")\n",
    "        # Write path stats to file\n",
    "        f = open(stats_outfile,\"wb\")\n",
    "        pickle.dump(path_stats,f)\n",
    "        f.close()\n",
    "        toc()\n",
    "        printf(\"Model hasn't converged\")\n",
    "        printf(\"--round {} elapsed time: {}s\".format(model_round, time.time() - tt))\n",
    "    toc()\n",
    "    # Save prediction of selected model to file in standard format\n",
    "    # after converting predicted anomalies into raw predictions\n",
    "    path_preds[model_str] = path_preds[model_str].add(path_preds.clim)\n",
    "    save_forecasts(\n",
    "        path_preds[['lat','lon','start_date',model_str]].rename(\n",
    "            columns={model_str:'pred'}),\n",
    "        model=model_name, submodel=submodel_name, \n",
    "        gt_id=gt_id, horizon=horizon, \n",
    "        target_date_str=target_date_str)\n",
    "    # Mark convergence by creating converged file\n",
    "    printf('\\nSaving ' + converged_outfile)\n",
    "    open(converged_outfile, 'w').close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
