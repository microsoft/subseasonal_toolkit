{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABC paper: Neural network baseline (Fan et al., 2021)\n",
    "\n",
    "Learned correction for SubX ensemble forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from subseasonal_toolkit.utils.notebook_util import isnotebook\n",
    "if isnotebook():\n",
    "    # Autoreload packages that are modified\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    #%cd \"~/forecast_rodeo_ii\"\n",
    "    #%pwd\n",
    "else:\n",
    "    from argparse import ArgumentParser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist, euclidean\n",
    "from datetime import datetime, timedelta\n",
    "from filelock import FileLock\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from IPython.display import Markdown, display\n",
    "from subseasonal_data.utils import get_measurement_variable\n",
    "from subseasonal_toolkit.utils.general_util import printf, tic, toc\n",
    "from subseasonal_toolkit.utils.experiments_util import (get_first_year, get_start_delta,\n",
    "                                                        get_forecast_delta, pandas2hdf)\n",
    "from subseasonal_toolkit.utils.models_util import (get_submodel_name, start_logger, log_params, get_forecast_filename,\n",
    "                                                   save_forecasts, get_selected_submodel_name)\n",
    "from subseasonal_toolkit.utils.eval_util import get_target_dates, mean_rmse_to_score, save_metric\n",
    "from sklearn.linear_model import *\n",
    "\n",
    "from subseasonal_data import data_loaders\n",
    "\n",
    "# Make NumPy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Specify model parameters\n",
    "#\n",
    "model_name = \"nn-a\"\n",
    "if not isnotebook():\n",
    "    # If notebook run as a script, parse command-line arguments\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"pos_vars\",nargs=\"*\")  # gt_id and horizon                                                                                  \n",
    "    parser.add_argument('--target_dates', '-t', default=\"std_test\")\n",
    "    # Fit intercept parameter if and only if this flag is specified\n",
    "    parser.add_argument('--num_epochs', '-e', default=10, \n",
    "                        help=\"number of training epochs\")\n",
    "    args, opt = parser.parse_known_args()\n",
    "    \n",
    "    # Assign variables                                                                                                                                     \n",
    "    gt_id = args.pos_vars[0] # \"contest_precip\" or \"contest_tmp2m\"                                                                            \n",
    "    horizon = args.pos_vars[1] # \"12w\", \"34w\", or \"56w\"                                                                                        \n",
    "    target_dates = args.target_dates\n",
    "    num_epochs = int(args.num_epochs)\n",
    "else:\n",
    "    # Otherwise, specify arguments interactively \n",
    "    gt_id = \"us_precip_1.5x1.5\"\n",
    "    horizon = \"12w\"\n",
    "    target_dates = \"20200101\"#\"std_paper_forecast\"\n",
    "    num_epochs = 10\n",
    "    \n",
    "#\n",
    "# Process model parameters\n",
    "#\n",
    "\n",
    "# Get list of target date objects\n",
    "target_date_objs = pd.Series(get_target_dates(date_str=target_dates, horizon=horizon))\n",
    "\n",
    "# Identify measurement variable name\n",
    "measurement_variable = get_measurement_variable(gt_id) # 'tmp2m' or 'precip'\n",
    "\n",
    "# Column names for gt_col, clim_col and anom_col \n",
    "gt_col = measurement_variable\n",
    "clim_col = measurement_variable+\"_clim\"\n",
    "anom_col = get_measurement_variable(gt_id)+\"_anom\" # 'tmp2m_anom' or 'precip_anom'\n",
    "\n",
    "# For a given target date, the last observable training date is target date - gt_delta\n",
    "# as gt_delta is the gap between the start of the target date and the start of the\n",
    "# last ground truth period that's fully observable at the time of forecast issuance\n",
    "gt_delta = timedelta(days=get_start_delta(horizon, gt_id))\n",
    "\n",
    "base_shift = get_forecast_delta(horizon) \n",
    "\n",
    "# Record model and submodel names\n",
    "submodel_name = get_submodel_name(model=model_name, horizon=horizon, num_epochs=num_epochs)\n",
    "\n",
    "if not isnotebook():\n",
    "    # Save output to log file\n",
    "    logger = start_logger(model=model_name,submodel=submodel_name,gt_id=gt_id,\n",
    "                          horizon=horizon,target_dates=target_dates)\n",
    "    # Store parameter values in log                                                                                                                        \n",
    "    params_names = ['gt_id', 'horizon', 'target_dates', 'num_epochs'\n",
    "                   ]\n",
    "    params_values = [eval(param) for param in params_names]\n",
    "    log_params(params_names, params_values)\n",
    "    \n",
    "def get_input_precip(horizon = \"34w\"):\n",
    "    m = \"deb_cfsv2\"\n",
    "    # the daily bias corrected Week 3-4 lead time forecast for total precipitation \n",
    "    # CFSv2 bias-corrected ensemble means for total precip ==> use the data in models/deb_cfsv2/submodel_forecasts/\n",
    "    gt_id, horizon = \"us_precip_1.5x1.5\", horizon\n",
    "    base_shift = get_forecast_delta(horizon) \n",
    "    measurement_variable = f\"precip_shift{base_shift}\"\n",
    "    task = f\"{gt_id}_{horizon}\"\n",
    "    sn = get_selected_submodel_name(model=m, gt_id=gt_id, horizon=horizon)\n",
    "    # Input data:\n",
    "    data_file = os.path.join(\"data\", \"dataframes\", f\"nn-a_{task}.h5\")\n",
    "    if os.path.isfile(data_file):\n",
    "        printf(f\"Loading {data_file}...\"); tic()\n",
    "        data_precip = pd.read_hdf(data_file)\n",
    "        toc()\n",
    "    else:\n",
    "        printf(f\"Creating {data_file}...\"); tic()\n",
    "        forecasts_dir = os.path.join(\"models\",m,\"submodel_forecasts\",sn,task)\n",
    "        filenames = sorted([f for f in os.listdir(forecasts_dir) if f.endswith(\".h5\")])\n",
    "        for i, f in enumerate(filenames):\n",
    "            printf(f)\n",
    "            data_f = pd.read_hdf(os.path.join(forecasts_dir, f))\n",
    "            data_precip = data_f if i==0 else data_precip.append(data_f)\n",
    "        data_precip = data_precip.rename(columns={\"pred\": measurement_variable})\n",
    "        toc()\n",
    "        pandas2hdf(data_precip, data_file)\n",
    "    return data_precip\n",
    "\n",
    "def get_inp_tmp2m_anom(horizon=\"34w\"):\n",
    "    m = \"deb_cfsv2\"\n",
    "    # CFSv2 bias-corrected ensemble means for anomaly T2m ==> subtract ground truth tmp2m climatology from \n",
    "    # the data in models/deb_cfsv2/submodel_forecasts/deb_cfsv2-years1999-2010_leads15-15/\n",
    "    gt_id, horizon = \"us_tmp2m_1.5x1.5\", horizon\n",
    "    base_shift = get_forecast_delta(horizon) \n",
    "    measurement_variable = f\"tmp2m_shift{base_shift}\"\n",
    "    task = f\"{gt_id}_{horizon}\"\n",
    "    sn = get_selected_submodel_name(model=m, gt_id=gt_id, horizon=horizon)# Input data:\n",
    "    forecasts_dir = os.path.join(\"models\",m,\"submodel_forecasts\",sn,task)\n",
    "    data_file = os.path.join(\"data\", \"dataframes\", f\"nn-a_{task}.h5\")\n",
    "    if os.path.isfile(data_file):\n",
    "        printf(f\"\\nLoading {data_file}\"); tic()\n",
    "        printf(f\"Based on forecasts in\\n{forecasts_dir}\")\n",
    "        data = pd.read_hdf(data_file)\n",
    "        toc()\n",
    "    else:\n",
    "        printf(f\"\\nCreating {data_file}\"); tic()\n",
    "        printf(f\"Using forecasts in\\n{forecasts_dir}\")\n",
    "        filenames = sorted([f for f in os.listdir(forecasts_dir) if f.endswith(\".h5\")])\n",
    "        # data_template = data_loaders.get_forecast(forecast_id = \"iri_cfsv2-precip-us1_5\")\n",
    "        # filenames\n",
    "        for i, f in enumerate(filenames):\n",
    "            printf(f)\n",
    "            data_f = pd.read_hdf(os.path.join(forecasts_dir, f))\n",
    "            # Transform to wide format\n",
    "        #     data_f_wide = data_f.set_index(['lat','lon','start_date']).unstack(['lat','lon'])\n",
    "            data = data_f if i==0 else data.append(data_f)\n",
    "        data = data.rename(columns={\"pred\": f\"tmp2m_shift{base_shift}\"})\n",
    "        pandas2hdf(data, data_file)\n",
    "        toc()\n",
    "    # printf(data)\n",
    "\n",
    "    # Calculate anomalies\n",
    "    data_file = f\"baseline_nn_input_tmp2m_anom_{horizon}.h5\"\n",
    "    data_file = os.path.join(\"data\", \"dataframes\", f\"nn-a_{task}_anom.h5\")\n",
    "    if os.path.isfile(data_file):\n",
    "        printf(f\"Loading {data_file}\"); tic()\n",
    "        printf(f\"Based on forecasts in\\n{forecasts_dir}\")\n",
    "        data_tmp2m_anom = pd.read_hdf(data_file)\n",
    "        toc()\n",
    "    else:\n",
    "        base_shift = get_forecast_delta(horizon) \n",
    "        gt = data_loaders.get_ground_truth(gt_id = \"us_tmp2m_1.5x1.5\", shift=base_shift)\n",
    "        measurement_variable = f\"tmp2m_shift{base_shift}\"\n",
    "        # Compute climatology based on post 2011 data\n",
    "        print(\"\\nLoading anomalies\"); tic()\n",
    "        sub_gt = gt[gt.start_date < \"2011-01-01\"]\n",
    "        clim = sub_gt.groupby(['lat','lon', sub_gt.start_date.dt.month, \n",
    "                               sub_gt.start_date.dt.day]).mean()\n",
    "        toc() \n",
    "        clim\n",
    "        print(\"\\nComputing anomalies\"); tic()\n",
    "        data_anom = pd.merge(data, clim, left_on = ['lat','lon',\n",
    "                                           data.start_date.dt.month,\n",
    "                                           data.start_date.dt.day], \n",
    "                      right_index = True, how='left', suffixes=('','_clim'))\n",
    "        data_anom[measurement_variable] -= data_anom[measurement_variable+'_clim']\n",
    "        data_anom.drop(columns=measurement_variable+'_clim',inplace=True)\n",
    "        data_anom.rename(columns={measurement_variable: measurement_variable+'_anom'},\n",
    "                  inplace=True)\n",
    "        data_tmp2m_anom = data_anom.drop(columns=[f\"tmp2m_sqd_shift{base_shift}\", f\"tmp2m_std_shift{base_shift}\"])\n",
    "        pandas2hdf(data_tmp2m_anom, data_file)\n",
    "        toc()\n",
    "    return data_tmp2m_anom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create the training and testing datasets\n",
    "#### Input model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get input data\n",
    "data_precip = get_input_precip(horizon = horizon)\n",
    "data_tmp2m_anom = get_inp_tmp2m_anom(horizon=horizon)\n",
    "\n",
    "# Create input dataframe\n",
    "data_input = pd.merge(data_precip, data_tmp2m_anom, on=['lat', 'lon', 'start_date'], how=\"left\")\n",
    "# add ground truth climatologies for tmp2m\n",
    "gt_var = 'tmp2m'\n",
    "clim = data_loaders.get_climatology(gt_id = f\"us_{gt_var}_1.5x1.5\") \n",
    "clim.rename(columns={gt_var: f'{gt_var}_clim'}, inplace=True)\n",
    "clim = clim.groupby(['lat','lon', clim.start_date.dt.month, \n",
    "                       clim.start_date.dt.day]).mean()\n",
    "data_input = pd.merge(data_input, clim, left_on = ['lat','lon',\n",
    "                                   data_input.start_date.dt.month,\n",
    "                                   data_input.start_date.dt.day], \n",
    "              right_index = True, how='left', suffixes=('','_clim'))\n",
    "# add ground truth climatologies for precip\n",
    "gt_var = 'precip'\n",
    "clim = data_loaders.get_climatology(gt_id = f\"us_{gt_var}_1.5x1.5\") \n",
    "clim.rename(columns={gt_var: f'{gt_var}_clim'}, inplace=True)\n",
    "clim = clim.groupby(['lat','lon', clim.start_date.dt.month, \n",
    "                       clim.start_date.dt.day]).mean()\n",
    "data_input = pd.merge(data_input, clim, left_on = ['lat','lon',\n",
    "                                   data_input.start_date.dt.month,\n",
    "                                   data_input.start_date.dt.day], \n",
    "              right_index = True, how='left', suffixes=('','_clim'))\n",
    "# data_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output data:\n",
    "# Accumulated precip over weeks 3-4\n",
    "gt_var, horizon = \"precip\", horizon\n",
    "base_shift = get_forecast_delta(horizon) \n",
    "gt_id, measurement_variable = f\"us_{gt_var}_1.5x1.5\", f\"{gt_var}_shift{base_shift}\"\n",
    "task = f\"{gt_id}_{horizon}\"\n",
    "data_out_precip = data_loaders.get_ground_truth(gt_id = gt_id, shift=base_shift).rename(columns={measurement_variable: f'{measurement_variable}_out'})\n",
    "\n",
    "# Mean tmp2m anomalies over weeks 3-4\n",
    "gt_var, horizon = \"tmp2m\", horizon\n",
    "base_shift = get_forecast_delta(horizon) \n",
    "gt_id, measurement_variable = f\"us_{gt_var}_1.5x1.5\", f\"{gt_var}_shift{base_shift}\"\n",
    "task = f\"{gt_id}_{horizon}\"\n",
    "data_out_tmp2m = data_loaders.get_ground_truth_anomalies(gt_id = gt_id, shift=base_shift)[['lat','lon','start_date',f'{measurement_variable}_anom']].rename(columns={f'{measurement_variable}_anom': f'{measurement_variable}_anom_out'})\n",
    "\n",
    "# Merged output data\n",
    "data_output = pd.merge(data_out_precip, data_out_tmp2m, on=['lat', 'lon', 'start_date'], how=\"left\")\n",
    "# data_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.merge(data_input, data_output, on=['lat', 'lon', 'start_date'], how=\"left\")\n",
    "\n",
    "# Transform to wide format\n",
    "raw_dataset_wide = dataset.set_index(['lat','lon','start_date']).unstack(['lat','lon'])\n",
    "raw_dataset_wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the data\n",
    "\n",
    "Clean if the dataset contains a few unknown values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ratio = .92\n",
    "# dataset_train = raw_dataset_wide.iloc[:int(train_ratio*len(raw_dataset_wide))]\n",
    "# dataset_test = raw_dataset_wide.drop(dataset_train.index)\n",
    "dataset_test = raw_dataset_wide.loc[target_date_objs[0]:]\n",
    "dataset_train = raw_dataset_wide.drop(dataset_test.index)\n",
    "\n",
    "# For a given target date, the last observable training date is target date - gt_delta\n",
    "# as gt_delta is the gap between the start of the target date and the start of the\n",
    "# last ground truth period that's fully observable at the time of forecast issuance\n",
    "gt_delta = timedelta(days=get_start_delta(horizon, gt_id))\n",
    "# Find the last observable training date \n",
    "last_train_date = dataset_test.index[0] - gt_delta\n",
    "# Update training dataset accordingly\n",
    "dataset_train = dataset_train.loc[:last_train_date]\n",
    "display(dataset_train)\n",
    "display(dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the data\n",
    "\n",
    "Review the joint distribution of a few pairs of columns from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(train_data_all[['precip_shift15','tmp2m_shift15_anom','tmp2m_clim','precip_clim']], diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check the overall statistics. Note how each feature covers a very different range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split features from labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [f'precip_shift{base_shift}', f'tmp2m_shift{base_shift}_anom', 'tmp2m_clim', 'precip_clim']\n",
    "labels = [f'precip_shift{base_shift}_out', f'tmp2m_shift{base_shift}_anom_out']\n",
    "column_features = [c for c in dataset_train.columns if c[0] in features]\n",
    "column_labels = [c for c in dataset_train.columns if c[0] in labels]\n",
    "\n",
    "train_features = dataset_train.copy()[column_features]\n",
    "train_labels = dataset_train.copy()[column_labels]\n",
    "\n",
    "test_features = dataset_test.copy()[column_features]\n",
    "test_labels = dataset_test.copy()[column_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "In the table of statistics it's easy to see how different the ranges of each feature are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.describe().transpose()[['mean', 'std']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(train_features))\n",
    "print(normalizer.mean.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the layer is called, it returns the input data, with each feature independently normalized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = np.array(train_features[:1])\n",
    "\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "  print('First example:', first)\n",
    "  print()\n",
    "  print('Normalized:', normalizer(first).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a NumPy array made of the train features. Then, instantiate the `tf.keras.layers.Normalization` and fit its state to the train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_arr = np.array(train_features)\n",
    "baseline_nn_normalizer = layers.Normalization(input_shape=[1504,], axis=None)\n",
    "baseline_nn_normalizer.adapt(train_features_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printf(f\"train_features.shape: {train_features.shape}\")\n",
    "printf(f\"train_labels.shape: {train_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create the baseline NN-A (Fan et al., 2021)\n",
    "#### Reference: \n",
    "Fan, Y., Krasnopolsky, V., van den Dool, H., Wu, C. Y., & Gottschalck, J. (2021). Using Artificial Neural Networks to Improve CFS Week 3-4 Precipitation and 2-Meter Air Temperature Forecasts. Weather and Forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_and_compile_model(norm):\n",
    "  model = keras.Sequential([\n",
    "      norm,\n",
    "#       layers.Dense(1504, activation='relu'),\n",
    "      layers.Dense(200, input_dim=1504, activation='relu'),\n",
    "      layers.Dense(752)\n",
    "  ])\n",
    "\n",
    "  model.compile(loss=\"mean_squared_error\",\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_nn_model = build_and_compile_model(baseline_nn_normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model with Keras Model.fit:\n",
    "weights_dir = os.path.join (\"models\", model_name, \"submodel_weights\", submodel_name)\n",
    "\n",
    "if os.path.isdir(weights_dir):\n",
    "    printf(f\"Loading {weights_dir}\")\n",
    "    train_history = False\n",
    "    baseline_nn_model = tf.keras.models.load_model(weights_dir)\n",
    "else:\n",
    "    printf(f\"Training and saving to {weights_dir}\")\n",
    "    train_history = True\n",
    "    history = baseline_nn_model.fit(\n",
    "        train_features,\n",
    "        train_labels,\n",
    "        validation_split=0.06,\n",
    "        verbose=1, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "#   plt.ylim([0, 10])\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error')\n",
    "  plt.legend()\n",
    "  plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_history:\n",
    "    plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = baseline_nn_model.evaluate(test_features, test_labels, verbose=0)\n",
    "test_results = {}\n",
    "test_results['dnn_model'] = baseline_nn_model.evaluate(test_features, test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_results, index=['Mean absolute error [MPG]']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions\n",
    "\n",
    "Make predictions on the test set using Keras `Model.predict` and review the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = baseline_nn_model.predict(test_features)#.flatten()\n",
    "\n",
    "df_test_predictions = pd.DataFrame(test_predictions, columns = test_labels.columns, index = test_labels.index)\n",
    "df_test_predictions_precip = df_test_predictions.xs(f'precip_shift{base_shift}_out', level=0, axis=1)\n",
    "df_test_predictions_tmp2m = df_test_predictions.xs(f'tmp2m_shift{base_shift}_anom_out', level=0, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover temperature predictions from temperature anomalies predictions\n",
    "gt = data_loaders.get_ground_truth(gt_id = \"us_tmp2m_1.5x1.5\", shift=base_shift)\n",
    "measurement_variable = f\"tmp2m_shift{base_shift}\"\n",
    "# Compute climatology based on post 2011 data\n",
    "print(\"\\nLoading anomalies\"); tic()\n",
    "sub_gt = gt[gt.start_date < \"2011-01-01\"]\n",
    "clim = sub_gt.groupby(['lat','lon', sub_gt.start_date.dt.month, \n",
    "                       sub_gt.start_date.dt.day]).mean()\n",
    "toc() \n",
    "\n",
    "# # Store rmses\n",
    "# rmses_tmp2m = pd.Series(index=target_date_objs, dtype='float64')\n",
    "# rmses_precip = pd.Series(index=target_date_objs, dtype='float64')\n",
    "\n",
    "# target_date_objs\n",
    "for target_date_obj in target_date_objs:\n",
    "    target_date_str = datetime.strftime(target_date_obj, '%Y%m%d')\n",
    "    if target_date_obj not in df_test_predictions.index:\n",
    "        printf(f\"Warning: missing prediction for {target_date_obj}\")\n",
    "    else:\n",
    "        pred_precip = pd.DataFrame(df_test_predictions_precip.loc[target_date_obj].to_frame().reset_index().values, \n",
    "                                    columns=['lat','lon','pred'])\n",
    "        pred_precip.insert(loc=2, column='start_date', value=target_date_obj)\n",
    "        pred_tmp2m = pd.DataFrame(df_test_predictions_tmp2m.loc[target_date_obj].to_frame().reset_index().values, \n",
    "                                    columns=['lat','lon','pred'])\n",
    "        pred_tmp2m.insert(loc=2, column='start_date', value=target_date_obj)\n",
    "\n",
    "        print(\"\\nComputing temperatures from anomalies\"); tic()\n",
    "        pred_tmp2m = pd.merge(pred_tmp2m, clim, left_on = ['lat','lon',\n",
    "                                           pred_tmp2m.start_date.dt.month,\n",
    "                                           pred_tmp2m.start_date.dt.day], \n",
    "                      right_index = True, how='left')\n",
    "        pred_tmp2m['pred'] += pred_tmp2m[measurement_variable]\n",
    "        pred_tmp2m.drop(columns=clim.columns,inplace=True)\n",
    "        toc()\n",
    "\n",
    "        # Save prediction to file in standard format\n",
    "        save_forecasts(\n",
    "            pred_tmp2m,\n",
    "            model=model_name, submodel=submodel_name, \n",
    "            gt_id=\"us_tmp2m_1.5x1.5\", horizon=horizon, \n",
    "            target_date_str=target_date_str)\n",
    "        save_forecasts(\n",
    "            pred_precip,#.loc[[target_date_obj],:].unstack().rename(\"pred\").reset_index(),\n",
    "            model=model_name, submodel=submodel_name, \n",
    "            gt_id=\"us_precip_1.5x1.5\", horizon=horizon, \n",
    "            target_date_str=target_date_str)\n",
    "\n",
    "\n",
    "#         # Evaluate and store error if we have ground truth data\n",
    "#         tic()\n",
    "#         rmse = np.sqrt(np.square(pred - gt[gt.start_date ==target_date_obj]).mean())\n",
    "#         rmses.loc[target_date_obj] = rmse\n",
    "#         print(\"-rmse: {}, score: {}\".format(rmse, mean_rmse_to_score(rmse)))\n",
    "#         mean_rmse = rmses.mean()\n",
    "#         print(\"-mean rmse: {}, running score: {}\".format(mean_rmse, mean_rmse_to_score(mean_rmse)))\n",
    "#         toc()\n",
    "\n",
    "# printf(\"Save rmses in standard format\")\n",
    "# rmses = rmses.sort_index().reset_index()\n",
    "# rmses.columns = ['start_date','rmse']\n",
    "# save_metric(rmses, model=model_name, submodel=submodel_name, gt_id=gt_id, horizon=horizon, target_dates=target_dates, metric=\"rmse\")\n",
    "# save_metric(rmses, model=f'{forecast}pp', submodel=submodel_name, gt_id=gt_id, horizon=horizon, target_dates=target_dates, metric=\"rmse\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the model for later use with `Model.save`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = os.path.join (\"models\", model_name, \"submodel_weights\", submodel_name)\n",
    "baseline_nn_model.save(weights_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you reload the model, it gives identical output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    reloaded = tf.keras.models.load_model(weights_dir)\n",
    "\n",
    "    test_results['reloaded'] = reloaded.evaluate(\n",
    "        test_features, test_labels, verbose=0)\n",
    "\n",
    "    pd.DataFrame(test_results, index=['Mean squared error']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
