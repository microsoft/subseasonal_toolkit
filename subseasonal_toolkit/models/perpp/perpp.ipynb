{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistence++\n",
    "\n",
    "Locally linear combination of dynamical model forecasts, lagged measurements, and climatology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from subseasonal_toolkit.utils.notebook_util import isnotebook\n",
    "if isnotebook():\n",
    "    # Autoreload packages that are modified\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    #%cd \"~/forecast_rodeo_ii\"\n",
    "else:\n",
    "    from argparse import ArgumentParser\n",
    "\n",
    "# Imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "import sys\n",
    "import json\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "from functools import partial\n",
    "from multiprocessing import cpu_count\n",
    "from subseasonal_data.utils import get_measurement_variable, df_merge, shift_df\n",
    "from subseasonal_toolkit.utils.general_util import printf, tic, toc\n",
    "from subseasonal_toolkit.utils.experiments_util import (get_first_year, \n",
    "                                                        get_start_delta, clim_merge)\n",
    "from subseasonal_toolkit.utils.eval_util import get_target_dates, mean_rmse_to_score, save_metric\n",
    "from subseasonal_toolkit.utils.fit_and_predict import apply_parallel\n",
    "from subseasonal_toolkit.utils.models_util import (get_submodel_name, start_logger, log_params, \n",
    "                                                   get_forecast_filename, save_forecasts)\n",
    "from subseasonal_toolkit.models.perpp.perpp_util import fit_and_predict, years_ago\n",
    "from subseasonal_data import data_loaders\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Specify model parameters\n",
    "#\n",
    "model_name = \"perpp\"\n",
    "if not isnotebook():\n",
    "    # If notebook run as a script, parse command-line arguments\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"pos_vars\",nargs=\"*\")  # gt_id and horizon                                                                                  \n",
    "    parser.add_argument('--target_dates', '-t', default=\"std_contest\")\n",
    "    parser.add_argument('--train_years', '-y', default=\"all\",\n",
    "                        help=\"Number of years to use in training (\\\"all\\\" or integer)\")\n",
    "    parser.add_argument('--margin_in_days', '-m', default=\"None\",\n",
    "                        help=\"number of month-day combinations on either side of the target combination \"\n",
    "                            \"to include when training; set to 0 include only target month-day combo; \"\n",
    "                            \"set to None to include entire year\")\n",
    "    parser.add_argument('--forecast', '-f', default=\"cfsv2\", \n",
    "                        help=\"include the forecasts of this dynamical model as features\")\n",
    "    args, opt = parser.parse_known_args()\n",
    "    \n",
    "    # Assign variables                                                                                                                                     \n",
    "    gt_id = args.pos_vars[0] # \"contest_precip\" or \"contest_tmp2m\"                                                                            \n",
    "    horizon = args.pos_vars[1] # \"34w\" or \"56w\"                                                                                        \n",
    "    target_dates = args.target_dates\n",
    "    train_years = args.train_years\n",
    "    if train_years != \"all\":\n",
    "        train_years = int(train_years)\n",
    "    if args.margin_in_days == \"None\":\n",
    "        margin_in_days = None\n",
    "    else:\n",
    "        margin_in_days = int(args.margin_in_days)\n",
    "    forecast = args.forecast\n",
    "    \n",
    "else:\n",
    "    # Otherwise, specify arguments interactively \n",
    "    gt_id = \"us_tmp2m_1.5x1.5\"\n",
    "    horizon = \"34w\"\n",
    "    target_dates = \"std_paper_forecast\"\n",
    "    train_years = \"all\"\n",
    "    margin_in_days = None\n",
    "    forecast = \"cfsv2\"\n",
    "\n",
    "#\n",
    "# Process model parameters\n",
    "#\n",
    "\n",
    "# Get list of target date objects\n",
    "target_date_objs = pd.Series(get_target_dates(date_str=target_dates,horizon=horizon))\n",
    "\n",
    "# Sort target_date_objs by day of week\n",
    "target_date_objs = target_date_objs[target_date_objs.dt.weekday.argsort(kind='stable')]\n",
    "\n",
    "# Identify measurement variable name\n",
    "measurement_variable = get_measurement_variable(gt_id) # 'tmp2m' or 'precip'\n",
    "\n",
    "# Column names for gt_col, clim_col and anom_col \n",
    "gt_col = measurement_variable\n",
    "clim_col = measurement_variable+\"_clim\"\n",
    "anom_col = get_measurement_variable(gt_id)+\"_anom\" # 'tmp2m_anom' or 'precip_anom'\n",
    "\n",
    "# For a given target date, the last observable training date is target date - gt_delta\n",
    "# as gt_delta is the gap between the start of the target date and the start of the\n",
    "# last ground truth period that's fully observable at the time of forecast issuance\n",
    "gt_delta = timedelta(days=get_start_delta(horizon, gt_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Choose regression parameters\n",
    "#\n",
    "# Record standard settings of these parameters\n",
    "base_col = \"zeros\"    \n",
    "if (gt_id.endswith(\"tmp2m\")) and (horizon == \"12w\"):\n",
    "    forecast_col = f'subx_{forecast}_tmp2m'\n",
    "    x_cols = [\n",
    "    'tmp2m_shift15',\n",
    "    'tmp2m_shift30',\n",
    "    forecast_col,\n",
    "    clim_col\n",
    "    ] \n",
    "elif (gt_id.endswith(\"precip\")) and (horizon == \"12w\"):\n",
    "    forecast_col = f'subx_{forecast}_precip'\n",
    "    x_cols = [\n",
    "    'precip_shift15',\n",
    "    'precip_shift30',\n",
    "    forecast_col,\n",
    "    clim_col\n",
    "    ] \n",
    "elif (gt_id.endswith(\"tmp2m\")) and (horizon == \"34w\"):\n",
    "    forecast_col = f'subx_{forecast}_tmp2m'\n",
    "    x_cols = [\n",
    "    'tmp2m_shift29',\n",
    "    'tmp2m_shift58',\n",
    "    forecast_col,\n",
    "    clim_col\n",
    "    ] \n",
    "elif (gt_id.endswith(\"precip\")) and (horizon == \"34w\"):\n",
    "    forecast_col = f'subx_{forecast}_precip'\n",
    "    x_cols = [\n",
    "    'precip_shift29',\n",
    "    'precip_shift58',\n",
    "    forecast_col,\n",
    "    clim_col\n",
    "    ] \n",
    "elif (gt_id.endswith(\"tmp2m\")) and (horizon == \"56w\"):\n",
    "    forecast_col = f'subx_{forecast}_tmp2m'\n",
    "    x_cols = [\n",
    "    'tmp2m_shift43',\n",
    "    'tmp2m_shift86',\n",
    "    forecast_col,\n",
    "    clim_col\n",
    "    ] \n",
    "elif (gt_id.endswith(\"precip\")) and (horizon == \"56w\"):\n",
    "    forecast_col = f'subx_{forecast}_precip'\n",
    "    x_cols = [\n",
    "    'precip_shift43',\n",
    "    'precip_shift86',\n",
    "    forecast_col,\n",
    "    clim_col\n",
    "    ]\n",
    "elif (gt_id.endswith(\"tmp2m_1.5x1.5\")) and (horizon == \"12w\"):\n",
    "    forecast_col = f'iri_{forecast}_tmp2m'\n",
    "    x_cols = [\n",
    "    'tmp2m_shift15',\n",
    "    'tmp2m_shift30',\n",
    "    forecast_col,\n",
    "    clim_col\n",
    "    ] \n",
    "elif (gt_id.endswith(\"precip_1.5x1.5\")) and (horizon == \"12w\"):\n",
    "    forecast_col = f'iri_{forecast}_precip'\n",
    "    x_cols = [\n",
    "    'precip_shift15',\n",
    "    'precip_shift30',\n",
    "    forecast_col,\n",
    "    clim_col\n",
    "    ] \n",
    "elif (gt_id.endswith(\"tmp2m_1.5x1.5\")) and (horizon == \"34w\"):\n",
    "    forecast_col = f'iri_{forecast}_tmp2m'\n",
    "    x_cols = [\n",
    "    'tmp2m_shift29',\n",
    "    'tmp2m_shift58',\n",
    "    forecast_col,\n",
    "    clim_col\n",
    "    ] \n",
    "elif (gt_id.endswith(\"precip_1.5x1.5\")) and (horizon == \"34w\"):\n",
    "    forecast_col = f'iri_{forecast}_precip'\n",
    "    x_cols = [\n",
    "    'precip_shift29',\n",
    "    'precip_shift58',\n",
    "    forecast_col,\n",
    "    clim_col\n",
    "    ] \n",
    "elif (gt_id.endswith(\"tmp2m_1.5x1.5\")) and (horizon == \"56w\"):\n",
    "    forecast_col = f'iri_{forecast}_tmp2m'\n",
    "    x_cols = [\n",
    "    'tmp2m_shift43',\n",
    "    'tmp2m_shift86',\n",
    "    forecast_col,\n",
    "    clim_col\n",
    "    ] \n",
    "elif (gt_id.endswith(\"precip_1.5x1.5\")) and (horizon == \"56w\"):\n",
    "    forecast_col = f'iri_{forecast}_precip'\n",
    "    x_cols = [\n",
    "    'precip_shift43',\n",
    "    'precip_shift86',\n",
    "    forecast_col,\n",
    "    clim_col\n",
    "    ]\n",
    "group_by_cols = ['lat', 'lon']\n",
    "\n",
    "# Record submodel names for perpp model\n",
    "submodel_name = get_submodel_name(\n",
    "    model_name, train_years=train_years, margin_in_days=margin_in_days,\n",
    "    forecast=forecast)\n",
    "\n",
    "printf(f\"Submodel name {submodel_name}\")\n",
    "\n",
    "if not isnotebook():\n",
    "    # Save output to log file\n",
    "    logger = start_logger(model=model_name,submodel=submodel_name,gt_id=gt_id,\n",
    "                          horizon=horizon,target_dates=target_dates)\n",
    "    # Store parameter values in log                                                                                                                        \n",
    "    params_names = ['gt_id', 'horizon', 'target_dates',\n",
    "                    'train_years', 'margin_in_days',\n",
    "                    'base_col', 'x_cols', 'group_by_cols', 'forecast'\n",
    "                   ]\n",
    "    params_values = [eval(param) for param in params_names]\n",
    "    log_params(params_names, params_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load ground truth data\n",
    "#\n",
    "printf(\"Loading ground truth data\")\n",
    "tic()\n",
    "gt = data_loaders.get_ground_truth(gt_id)[['lat','lon','start_date',gt_col]]\n",
    "toc()\n",
    "\n",
    "#\n",
    "# Added shifted ground truth features\n",
    "#\n",
    "printf(\"Adding shifted ground truth features\")\n",
    "lld_data = gt\n",
    "shifts = [int(re.search(r'\\d+$', col).group()) for col in x_cols if col.startswith(gt_col+\"_shift\")]\n",
    "tic()\n",
    "for shift in shifts:\n",
    "    gt_shift = shift_df(gt, shift)\n",
    "    lld_data = df_merge(lld_data, gt_shift, how=\"right\")\n",
    "toc()\n",
    "\n",
    "#\n",
    "# Drop rows with empty pred_cols\n",
    "#\n",
    "pred_cols = x_cols+[base_col]\n",
    "exclude_cols = set([clim_col, forecast_col, 'zeros']) \n",
    "lld_data = lld_data.dropna(subset=set(pred_cols) - exclude_cols)\n",
    "\n",
    "# Add climatology\n",
    "if clim_col in pred_cols:\n",
    "    printf(\"Merging in climatology\")\n",
    "    tic()\n",
    "    lld_data = clim_merge(lld_data, data_loaders.get_climatology(gt_id))\n",
    "    toc()\n",
    "\n",
    "# Add zeros\n",
    "if 'zeros' in pred_cols:\n",
    "    lld_data['zeros'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Add ensemble forecast as feature\n",
    "#\n",
    "printf(f\"Forming {forecast} ensemble forecast...\")\n",
    "shift = 15 if horizon == \"34w\" else 29 if horizon == \"56w\" else 1\n",
    "first_lead = 14 if horizon == \"34w\" else 28 if horizon == \"56w\" else 0\n",
    "suffix = \"-us\" if gt_id.startswith(\"us_\") else \"\"\n",
    "\n",
    "if gt_id.endswith(\"1.5x1.5\"):\n",
    "    prefix = f\"iri_{forecast}\"\n",
    "    suffix += \"1_5\"\n",
    "else:\n",
    "    prefix = f\"subx_{forecast}\"\n",
    "\n",
    "if forecast == \"subx_mean\":\n",
    "    forecast_id = prefix+\"-\"+gt_id.split(\"_\")[1]+\"_\"+horizon+suffix\n",
    "else:\n",
    "    forecast_id = prefix+\"-\"+gt_id.split(\"_\")[1]+suffix\n",
    "\n",
    "tic(); data = data_loaders.get_forecast(forecast_id=forecast_id, shift=shift); toc()\n",
    "\n",
    "# Select last lead to include in ensemble\n",
    "if horizon == \"12w\":\n",
    "    last_lead = first_lead + 1\n",
    "else:\n",
    "    # Find the largest available lead\n",
    "    max_lead = 0\n",
    "    for col in data.columns:\n",
    "        try:\n",
    "            max_lead = max(max_lead, \n",
    "                           int(re.search(f'{prefix}_{gt_col}-(.*).5d_shift{shift}', \n",
    "                                         col).group(1)))\n",
    "        except AttributeError:\n",
    "            continue\n",
    "    last_lead = min(max_lead, 29)\n",
    "printf(f\"Aggregating lead {first_lead} with shift {shift}\")\n",
    "tic()\n",
    "feature = data[['lat','lon','start_date',f'{prefix}_{gt_col}-{first_lead}.5d_shift{shift}']].set_index(\n",
    "    ['lat','lon','start_date']).squeeze().unstack(['lat','lon']).copy()\n",
    "initial_feature_index = feature.index\n",
    "# Count the number of non-nan leads values contributing to each feature entry\n",
    "num_leads = pd.DataFrame(data=1., index=feature.index, columns=feature.columns, dtype=float)\n",
    "toc()\n",
    "for lead in range(first_lead+1,last_lead+1):\n",
    "    printf(f\"Aggregating lead {lead} with shift {shift+lead-first_lead}\")\n",
    "    tic()\n",
    "    new_feat = data[['lat','lon','start_date',f'{prefix}_{gt_col}-{lead}.5d_shift{shift}']].set_index(\n",
    "        ['lat','lon','start_date']).squeeze().unstack(['lat','lon']).shift(lead-first_lead,freq=\"D\")\n",
    "    # Add contribution of new feature if it exists and 0 otherwise\n",
    "    feature = feature.add(new_feat, fill_value=0.)\n",
    "    # Count contribution of new feature if it exists\n",
    "    num_leads = num_leads.add(new_feat.notna().astype(float),  fill_value=0.)\n",
    "    toc()\n",
    "del data\n",
    "# Normalize feature sums by the number of contributing leads\n",
    "feature /= num_leads\n",
    "del num_leads\n",
    "# Restrict to initial feature index dates\n",
    "feature = feature.loc[initial_feature_index]\n",
    "\n",
    "# Drop dates with no forecasts and reshape\n",
    "feature = feature.dropna().unstack().rename(forecast_col)\n",
    "# Merge feature forecast with lld_data\n",
    "printf(f\"Merging {forecast_col} with lld_data\")\n",
    "tic()\n",
    "lld_data = pd.merge(lld_data, feature, left_on=['lat','lon','start_date'], \n",
    "                    right_index=True)\n",
    "toc()\n",
    "del feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify regression model\n",
    "fit_intercept = True\n",
    "model = linear_model.LinearRegression(fit_intercept=fit_intercept)\n",
    "\n",
    "# Form predictions for each grid point (in parallel) using train / test split\n",
    "# and the selected model\n",
    "prediction_func = partial(fit_and_predict, model=model)\n",
    "num_cores = cpu_count()\n",
    "\n",
    "# Store rmses\n",
    "rmses = pd.Series(index=target_date_objs, dtype='float64')\n",
    "\n",
    "# Restrict data to relevant columns and rows for which predictions can be made\n",
    "relevant_cols = set(\n",
    "    ['start_date','lat','lon',gt_col,base_col]+x_cols).intersection(lld_data.columns)\n",
    "lld_data = lld_data[relevant_cols].dropna(subset=x_cols+[base_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for target_date_obj in target_date_objs:\n",
    "    if not any(lld_data.start_date.isin([target_date_obj])):\n",
    "        printf(f\"warning: some features unavailable for target={target_date_obj}; skipping\")\n",
    "        continue    \n",
    "\n",
    "    target_date_str = datetime.strftime(target_date_obj, '%Y%m%d')\n",
    "\n",
    "    # Skip if forecast already produced for this target\n",
    "    forecast_file = get_forecast_filename(\n",
    "        model=model_name, submodel=submodel_name, \n",
    "        gt_id=gt_id, horizon=horizon, \n",
    "        target_date_str=target_date_str)\n",
    "\n",
    "    if True and os.path.isfile(forecast_file):\n",
    "        printf(f\"prior forecast exists for target={target_date_obj}; loading\")\n",
    "        tic()\n",
    "        preds = pd.read_hdf(forecast_file)\n",
    "\n",
    "        # Add ground truth for later evaluation\n",
    "        preds = pd.merge(preds, lld_data.loc[lld_data.start_date==target_date_obj,['lat','lon',gt_col]], \n",
    "                         on=['lat','lon'])\n",
    "\n",
    "        preds.rename(columns={gt_col:'truth'}, inplace=True)\n",
    "        toc()\n",
    "    else:\n",
    "        printf(f'target={target_date_str}')\n",
    "\n",
    "        # Subset data based on margin\n",
    "        if margin_in_days is not None:\n",
    "            tic()\n",
    "            sub_data = month_day_subset(lld_data, target_date_obj, margin_in_days)\n",
    "            toc()\n",
    "        else:\n",
    "            sub_data = lld_data\n",
    "\n",
    "        # Find the last observable training date for this target\n",
    "        last_train_date = target_date_obj - gt_delta \n",
    "\n",
    "        # Only train on train_years worth of data\n",
    "        if train_years != \"all\":\n",
    "            tic()\n",
    "            sub_data = sub_data.loc[sub_data.start_date >= years_ago(last_train_date, train_years)]\n",
    "            toc()\n",
    "\n",
    "        tic()\n",
    "        preds = apply_parallel(\n",
    "            sub_data.groupby(group_by_cols),\n",
    "            prediction_func, \n",
    "            num_cores=num_cores,\n",
    "            gt_col=gt_col,\n",
    "            x_cols=x_cols, \n",
    "            base_col=base_col, \n",
    "            last_train_date=last_train_date,\n",
    "            test_dates=[target_date_obj])  \n",
    "\n",
    "        # Ensure raw precipitation predictions are never less than zero\n",
    "        if gt_id.endswith(\"precip\"):\n",
    "            tic()\n",
    "            preds['pred'] = np.maximum(preds['pred'],0)\n",
    "            toc()\n",
    "\n",
    "        preds = preds.reset_index()\n",
    "\n",
    "        if True:\n",
    "            # Save prediction to file in standard format\n",
    "            save_forecasts(preds.drop(columns=['truth']),\n",
    "                model=model_name, submodel=submodel_name, \n",
    "                gt_id=gt_id, horizon=horizon, \n",
    "                target_date_str=target_date_str)\n",
    "        toc()\n",
    "\n",
    "    # Evaluate and store error\n",
    "    rmse = np.sqrt(np.square(preds.pred - preds.truth).mean())\n",
    "    rmses.loc[target_date_obj] = rmse\n",
    "    printf(\"-rmse: {}, score: {}\".format(rmse, mean_rmse_to_score(rmse)))\n",
    "    mean_rmse = rmses.mean()\n",
    "    printf(\"-mean rmse: {}, running score: {}\".format(mean_rmse, mean_rmse_to_score(mean_rmse)))\n",
    "\n",
    "if True:\n",
    "    # Save rmses in standard format\n",
    "    rmses = rmses.reset_index()\n",
    "    rmses.columns = ['start_date','rmse']\n",
    "    save_metric(rmses, model=model_name, submodel=submodel_name, gt_id=gt_id, horizon=horizon, target_dates=target_dates, metric=\"rmse\")\n",
    "    save_metric(rmses, model=f'perpp_{forecast}', submodel=submodel_name, gt_id=gt_id, horizon=horizon, target_dates=target_dates, metric=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
