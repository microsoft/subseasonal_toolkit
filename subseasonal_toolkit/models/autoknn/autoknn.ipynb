{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# AutoKNN: Locally linear regression with fixed and nearest neighbor lags\n",
    "# See https://arxiv.org/abs/1809.07394 for more details\n",
    "#\n",
    "import os, sys\n",
    "from subseasonal_toolkit.utils.notebook_util import isnotebook\n",
    "\n",
    "if isnotebook():\n",
    "    # Autoreload packages that are modified\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "else:\n",
    "    from argparse import ArgumentParser\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from multiprocessing import cpu_count\n",
    "from functools import partial\n",
    "from ttictoc import tic, toc\n",
    "from subseasonal_data.utils import get_measurement_variable\n",
    "from subseasonal_toolkit.utils.experiments_util import get_first_year, get_start_delta, clim_merge, month_day_subset\n",
    "from subseasonal_toolkit.utils.fit_and_predict import apply_parallel\n",
    "from subseasonal_toolkit.utils.skill import *\n",
    "from subseasonal_toolkit.utils.eval_util import get_target_dates, mean_rmse_to_score, save_metric\n",
    "from subseasonal_toolkit.utils.models_util import (get_submodel_name, start_logger, log_params, get_forecast_filename,\n",
    "                                                   save_forecasts)\n",
    "from subseasonal_toolkit.models.autoknn.autoknn_util import *\n",
    "from subseasonal_toolkit.models.perpp.perpp_util import fit_and_predict\n",
    "from subseasonal_data import data_loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Specify model parameters\n",
    "#\n",
    "\n",
    "if not isnotebook():\n",
    "    # If notebook run as a script, parse command-line arguments\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"pos_vars\",nargs=\"*\",\n",
    "                        help=\"specify gt_id (e.g., \\\"contest_tmp2m\\\") \"\n",
    "                             \"and horizon (e.g., \\\"34w\\\")\")\n",
    "    parser.add_argument('--history', default=60,\n",
    "                       help=\"Number of past days that should contribute to measure of similarity\")\n",
    "    parser.add_argument('--lag', '-l', default=365,\n",
    "                       help=\"Number of days between target date and first date considered as neighbor\")\n",
    "    parser.add_argument('--num_neighbors', '-n', default=20,\n",
    "                       help=\"Number of date neighbors to consider\")\n",
    "    parser.add_argument('--target_dates', '-t', default='std_contest')\n",
    "    parser.add_argument('--metric', default='cos',\n",
    "                       help=\"Similarity metric ('rmse' or 'cos')\")\n",
    "    parser.add_argument('--margin_in_days', '-m', default=\"None\",\n",
    "                       help=\"number of month-day combinations on either side of the target combination \"\n",
    "                            \"to include when training; set to 0 include only target month-day combo; \"\n",
    "                            \"set to None to include entire year\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Assign variables\n",
    "    gt_id = args.pos_vars[0]                                                                         \n",
    "    horizon = args.pos_vars[1]\n",
    "    history = int(args.history)\n",
    "    lag = int(args.lag)\n",
    "    num_neighbors = int(args.num_neighbors)\n",
    "    target_dates = args.target_dates\n",
    "    metric = args.metric\n",
    "    if args.margin_in_days == \"None\":\n",
    "        margin_in_days = None\n",
    "    else:\n",
    "        margin_in_days = int(args.margin_in_days)\n",
    "else:\n",
    "    # Otherwise, specify arguments interactively \n",
    "    gt_id = \"contest_tmp2m\"\n",
    "    horizon = \"34w\"\n",
    "    history = 60\n",
    "    lag = 365\n",
    "    num_neighbors = 20 if gt_id.endswith(\"tmp2m\") else 1\n",
    "    target_dates = 'std_contest'\n",
    "    metric = 'rmse'\n",
    "    margin_in_days = None if gt_id.endswith(\"tmp2m\") else 56\n",
    "    \n",
    "#\n",
    "# Process arguments\n",
    "#\n",
    "\n",
    "# Create list of target dates corresponding to submission dates in YYYYMMDD format\n",
    "target_date_objs = pd.Series(get_target_dates(date_str=target_dates, horizon=horizon))\n",
    "# Sort target_date_objs by day of week\n",
    "target_date_objs = target_date_objs[target_date_objs.dt.weekday.argsort(kind='stable')]\n",
    "\n",
    "# Choose the name of this model\n",
    "model_name = \"autoknn\"\n",
    "\n",
    "# Name of cache directory for storing non-submission-date specific\n",
    "# intermediate files\n",
    "cache_dir = os.path.join('models', model_name, 'cache')\n",
    "# if cache_dir doesn't exist, create it\n",
    "if not os.path.isdir(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "submodel_name = get_submodel_name(model_name, \n",
    "    history = history, lag = lag, num_neighbors = num_neighbors, \n",
    "    margin_in_days = margin_in_days, metric = metric)\n",
    "printf(f\"Submodel name {submodel_name}\")\n",
    "\n",
    "if not isnotebook():\n",
    "    logger = start_logger(model=model_name,submodel=submodel_name,gt_id=gt_id,\n",
    "                          horizon=horizon,target_dates=target_dates)\n",
    "    # Store parameter values in log\n",
    "    params_names = ['gt_id', 'horizon', 'target_dates',\n",
    "                    'metric', 'history', 'lag',\n",
    "                    'num_neighbors', 'margin_in_days']\n",
    "    \n",
    "    params_values = [eval(param) for param in params_names]\n",
    "    log_params(params_names, params_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute KNN\n",
    "\n",
    "Computes similarities between each pair of dates based on how skillfully the history of one date predicts the history of the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the predictions of the top neighbors for each target date have been saved\n",
    "preds_file = get_knn_preds_file_name(gt_id, horizon, history, \n",
    "                                     lag, num_neighbors, metric, model_name)\n",
    "regen_preds = True\n",
    "regen_viable = True\n",
    "if not regen_preds and os.path.isfile(preds_file):\n",
    "    printf(\"KNN Predictions already saved in \"+preds_file)\n",
    "else:\n",
    "    # Load the similarities between each target date and each candidate date\n",
    "    viable_similarities_file = get_viable_similarities_file_name(\n",
    "        gt_id, horizon, history, lag, metric, model_name)\n",
    "    if not regen_viable and os.path.isfile(viable_similarities_file):\n",
    "        printf(\"Reading viable similarities from \"+viable_similarities_file)\n",
    "        tic();viable_similarities = pd.read_hdf(viable_similarities_file);toc()\n",
    "    else:\n",
    "        # Compute similarities\n",
    "        gt_sim = compute_gt_similarity(gt_id, metric=metric, hindcast_year=None)\n",
    "        viable_similarities = compute_viable_similarities(gt_sim, gt_id, horizon, \n",
    "                                                          history, lag, metric, model_name)\n",
    "    # Compute and save KNN predictions\n",
    "    get_knn_preds(viable_similarities, gt_id, horizon, \n",
    "                  history, lag, num_neighbors, metric, model_name)\n",
    "    del(viable_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Choose regression parameters\n",
    "#\n",
    "measurement_variable = get_measurement_variable(gt_id) # 'tmp2m' or 'precip'\n",
    "\n",
    "# column names for gt_col, clim_col and anom_col\n",
    "gt_col = measurement_variable\n",
    "clim_col = measurement_variable+\"_clim\"\n",
    "anom_col = measurement_variable+\"_anom\" # 'tmp2m_anom' or 'precip_anom'\n",
    "# anom_inv_std_col: column name of inverse standard deviation of anomalies for each start_date\n",
    "anom_inv_std_col = anom_col+\"_inv_std\"\n",
    "# Name of knn columns\n",
    "knn_cols = [\"knn\"+str(ii) for ii in range(1,num_neighbors+1)]\n",
    "\n",
    "# Construct fixed lag anomaly variable names\n",
    "lags = (['43', '86'] if horizon == '56w' else ['29', '58']) + ['365']\n",
    "if metric == 'cos':\n",
    "    # Column to subtract away from target prior to prediction\n",
    "    base_col = clim_col\n",
    "    lag_cols = [measurement_variable+'_shift'+lag+'_anom' for lag in lags]\n",
    "    # Columns to group by when fitting regressions (a separate regression\n",
    "    # is fit for each group); use ['ones'] to fit a single regression to all points\n",
    "    group_by_cols = ['lat', 'lon']\n",
    "    # anom_scale_col: multiply anom_col by this amount prior to prediction\n",
    "    # (e.g., 'ones' or anom_inv_std_col)\n",
    "    anom_scale_col = anom_inv_std_col\n",
    "    # pred_anom_scale_col: multiply predicted anomalies by this amount\n",
    "    # (e.g., 'ones' or anom_inv_std_col)\n",
    "    pred_anom_scale_col = anom_scale_col\n",
    "    # Regressors\n",
    "    x_cols = knn_cols + lag_cols + ['ones']\n",
    "else: # RMSE metric\n",
    "    base_col = 'zeros'\n",
    "    lag_cols = [measurement_variable+'_shift'+lag for lag in lags]\n",
    "    group_by_cols = ['lat', 'lon']\n",
    "    anom_scale_col = 'ones'\n",
    "    pred_anom_scale_col = anom_scale_col\n",
    "    # Regressors\n",
    "    x_cols = knn_cols + lag_cols + ['ones',clim_col]\n",
    "\n",
    "# Columns to load from lat_lon_date dataframe\n",
    "load_cols = lag_cols+[gt_col,anom_inv_std_col,'start_date','lat','lon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load data\n",
    "#\n",
    "\n",
    "# Load fixed lags and target variables\n",
    "printf(\"\\nLoading fixed lags and target data\")\n",
    "data = data_loaders.load_combined_data(\"lat_lon_date_data\", gt_id, horizon, columns=load_cols)\n",
    "\n",
    "printf(f\"\\nDropping rows with missing {lag_cols} values\")\n",
    "tic()\n",
    "data.dropna(subset = lag_cols, inplace = True)\n",
    "toc()\n",
    "\n",
    "if clim_col in x_cols+[base_col]:\n",
    "    printf(\"Merging in climatology\")\n",
    "    tic()\n",
    "    data = clim_merge(data, data_loaders.get_climatology(gt_id))\n",
    "    toc()\n",
    "\n",
    "# Load AutoKNN data\n",
    "printf(f\"\\nLoading autoknn data from {preds_file}\")\n",
    "tic()\n",
    "knn_data = pd.read_hdf(preds_file)\n",
    "if metric == \"cos\":\n",
    "    knn_data[knn_cols] /= knn_data.groupby([\"start_date\"])[knn_cols].transform('std')\n",
    "toc()\n",
    "\n",
    "# Restrict data to relevant columns\n",
    "printf(\"\\nMerging datasets\")\n",
    "tic()\n",
    "data = pd.merge(data, knn_data, on=[\"start_date\",\"lat\",\"lon\"], how=\"left\")\n",
    "del(knn_data)\n",
    "toc()\n",
    "\n",
    "# Add supplementary columns\n",
    "printf(\"\\nAdding supplementary columns\")\n",
    "tic()\n",
    "data['ones'] = 1.0\n",
    "data['zeros'] = 0.0\n",
    "# Square root of weight assigned to each training datapoint\n",
    "data['sqrt_sample_weight'] = data[pred_anom_scale_col]\n",
    "# If undefined, assign value of 1\n",
    "data.loc[data['sqrt_sample_weight'].isna(),'sqrt_sample_weight'] = 1\n",
    "toc()\n",
    "\n",
    "# Drop rows with missing values for any relevant column\n",
    "printf(\"\\nDropping irrelevant columns and rows with missing values\")\n",
    "relevant_cols = set(x_cols+[base_col,'sqrt_sample_weight',gt_col,\n",
    "                    'start_date','lat','lon']+group_by_cols)\n",
    "tic()\n",
    "data = data[relevant_cols].dropna(subset = x_cols+[base_col])\n",
    "toc()\n",
    "\n",
    "# Scale regressors and target by square root sample weight\n",
    "printf(\"\\nScaling regressors and target by square root sample weight\")\n",
    "tic()\n",
    "data.loc[:,x_cols+[base_col,gt_col]] = data.loc[:,x_cols+[base_col,gt_col]].mul(\n",
    "       data['sqrt_sample_weight'], axis = 'index')\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify regression model\n",
    "# Exclude intercept as explicit 'ones' column is included\n",
    "fit_intercept = False\n",
    "model = linear_model.LinearRegression(fit_intercept=fit_intercept)\n",
    "\n",
    "# Form predictions for each grid point (in parallel) using train / test split\n",
    "# and the selected model\n",
    "prediction_func = partial(fit_and_predict, model=model)\n",
    "num_cores = cpu_count()\n",
    "\n",
    "# Store rmses\n",
    "rmses = pd.Series(index=target_date_objs, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of days between target date and last viable training date\n",
    "start_delta = timedelta(get_start_delta(horizon, gt_id))\n",
    "for target_date_obj in target_date_objs:\n",
    "    if not any(data.start_date.isin([target_date_obj])):\n",
    "        printf(f\"warning: some features unavailable for target={target_date_obj}; skipping\")\n",
    "        continue    \n",
    "        \n",
    "    target_date_str = datetime.strftime(target_date_obj, '%Y%m%d')\n",
    "\n",
    "    # Skip if forecast already produced for this target\n",
    "    forecast_file = get_forecast_filename(\n",
    "        model=model_name, submodel=submodel_name, \n",
    "        gt_id=gt_id, horizon=horizon, \n",
    "        target_date_str=target_date_str)\n",
    "    \n",
    "    if os.path.isfile(forecast_file):\n",
    "        printf(f\"prior forecast exists for target={target_date_obj}; loading\")\n",
    "        tic()\n",
    "        preds = pd.read_hdf(forecast_file)\n",
    "        \n",
    "        # Add ground truth for later evaluation\n",
    "        preds = pd.merge(preds, data.loc[data.start_date==target_date_obj,['lat','lon',gt_col]], \n",
    "                         on=['lat','lon'])\n",
    "        \n",
    "        preds.rename(columns={gt_col:'truth'}, inplace=True)\n",
    "        toc()\n",
    "    else:\n",
    "        printf(f'target={target_date_str}')\n",
    "        \n",
    "        # Subset data based on margin\n",
    "        if margin_in_days is not None:\n",
    "            printf(f\"subsetting based on margin {margin_in_days}\")\n",
    "            tic()\n",
    "            sub_data = month_day_subset(data, target_date_obj, margin_in_days)\n",
    "            toc()\n",
    "        else:\n",
    "            sub_data = data\n",
    "            \n",
    "        # Find the last observable training date for this target\n",
    "        last_train_date = target_date_obj - start_delta\n",
    "            \n",
    "        tic()\n",
    "        preds = apply_parallel(\n",
    "            sub_data.groupby(group_by_cols),\n",
    "            prediction_func, \n",
    "            num_cores=num_cores,\n",
    "            gt_col=gt_col,\n",
    "            x_cols=x_cols, \n",
    "            base_col=base_col, \n",
    "            last_train_date=last_train_date,\n",
    "            test_dates=[target_date_obj],\n",
    "            return_cols=['sqrt_sample_weight'])\n",
    "        \n",
    "        # Undo test point sample weighting\n",
    "        preds = preds.loc[:,['pred','truth']].div(\n",
    "            preds['sqrt_sample_weight'], axis='index')\n",
    "        \n",
    "        # Ensure raw precipitation predictions are never less than zero\n",
    "        if gt_id.endswith(\"precip\"):\n",
    "            tic()\n",
    "            preds['pred'] = np.maximum(preds['pred'],0)\n",
    "            toc()\n",
    "            \n",
    "        preds = preds.reset_index()\n",
    "        \n",
    "        # Save prediction to file in standard format\n",
    "        save_forecasts(preds.drop(columns=['truth']),\n",
    "            model=model_name, submodel=submodel_name, \n",
    "            gt_id=gt_id, horizon=horizon, \n",
    "            target_date_str=target_date_str)\n",
    "        toc()\n",
    "        \n",
    "    # Evaluate and store error\n",
    "    rmse = np.sqrt(np.square(preds.pred - preds.truth).mean())\n",
    "    rmses.loc[target_date_obj] = rmse\n",
    "    print(\"-rmse: {}, score: {}\".format(rmse, mean_rmse_to_score(rmse)))\n",
    "    mean_rmse = rmses.mean()\n",
    "    print(\"-mean rmse: {}, running score: {}\".format(mean_rmse, mean_rmse_to_score(mean_rmse)))\n",
    "\n",
    "# Save rmses in standard format\n",
    "rmses = rmses.reset_index()\n",
    "rmses.columns = ['start_date','rmse']\n",
    "save_metric(rmses, model=model_name, submodel=submodel_name, gt_id=gt_id, horizon=horizon, target_dates=target_dates, metric=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
