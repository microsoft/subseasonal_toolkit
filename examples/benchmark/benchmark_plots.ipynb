{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Generates figures and LaTeX tables for \n",
    "\n",
    "SubseasonalClimateUSA: A Dataset for Subseasonal Forecasting and Benchmarking\n",
    "\n",
    "Soukayna Mouatadid, Paulo Orenstein, Genevieve Flaspohler, Miruna Oprescu, \n",
    "Judah Cohen, Franklyn Wang, Sean Knight, Maria Geogdzhayeva, Sam Levang, \n",
    "Ernest Fraenkel, and Lester Mackey. \n",
    "\"\"\"\n",
    "# Ensure notebook is being run from base repository directory\n",
    "import os, sys\n",
    "try:\n",
    "    os.chdir(\"/home/{}/forecast_rodeo_ii/\".format(os.environ[\"USER\"]))\n",
    "except Exception as err:\n",
    "    print(f\"Warning: unable to change directory; {repr(err)}\")\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline    \n",
    "    \n",
    "import itertools\n",
    "import importlib\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import copy\n",
    "import pdb\n",
    "import calendar \n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib   \n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "from subseasonal_toolkit.utils.experiments_util import pandas2hdf\n",
    "from subseasonal_toolkit.utils.general_util import printf, make_directories\n",
    "from subseasonal_toolkit.utils.eval_util import get_target_dates, score_to_mean_rmse, contest_quarter_start_dates, contest_quarter\n",
    "from subseasonal_toolkit.utils.models_util import get_selected_submodel_name\n",
    "from viz_util import *\n",
    "\n",
    "# set figure and font sizes for seaborn plots\n",
    "sns.set(rc={'figure.figsize':(8,6)}, font_scale=1)\n",
    "\n",
    "#\n",
    "# Directory for saving output\n",
    "#\n",
    "out_dir = \"/home/{}/forecast_rodeo_ii/subseasonal_toolkit/viz/benchmark\".format(os.environ[\"USER\"])\n",
    "make_directories(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Full set of regions, times, and tasks to evaluate\n",
    "#\n",
    "metrics = [\"rmse\", \"skill\", \"score\"]\n",
    "\n",
    "contest_gt_ids = [\"contest_tmp2m\", \"contest_precip\"]\n",
    "us_gt_ids = [\"us_tmp2m\", \"us_precip\"]\n",
    "east_gt_ids = [\"east_tmp2m\", \"east_precip\"]\n",
    "us_1_5_gt_ids = [\"us_tmp2m_1.5x1.5\", \"us_precip_1.5x1.5\"]\n",
    "\n",
    "# All ground truth ids\n",
    "gt_ids = contest_gt_ids + us_gt_ids \n",
    "\n",
    "horizons = [\"34w\", \"56w\"]\n",
    "target_eval_dates = [\"std_paper\", \"std_contest\"]\n",
    "\n",
    "# The full set of models we to evaluate in some\n",
    "# experiment \n",
    "all_models = [\n",
    "    # Raw Baselines\n",
    "    'raw_cfsv2', \n",
    "    # Baselines\n",
    "    \"climatology\",   \n",
    "    'deb_cfsv2',\n",
    "    'persistence',\n",
    "    # ECMWF\n",
    "    'ecmwf'\n",
    "    # ABC \n",
    "    'tuned_climpp',\n",
    "    'tuned_cfsv2pp',\n",
    "    'perpp',\n",
    "    #Learning\n",
    "    'autoknn',\n",
    "    'informer',\n",
    "    'tuned_localboosting',\n",
    "    'multillr',\n",
    "    'nbeats',\n",
    "    'prophet',\n",
    "    'salient',\n",
    "    'tuned_salient2',\n",
    "    #Ensembles\n",
    "    'linear_ensemble',  \n",
    "    'online_learning'\n",
    "]\n",
    "\n",
    "# Main experiment model names\n",
    "main_experiment_models = [\n",
    "    # Baselines\n",
    "    \"climatology\",   \n",
    "    'deb_cfsv2',\n",
    "    'persistence',\n",
    "    # ABC \n",
    "    'tuned_climpp',\n",
    "    'tuned_cfsv2pp',\n",
    "    'perpp_cfsv2',\n",
    "    #Learning\n",
    "    'autoknn',\n",
    "    'informer',\n",
    "    'tuned_localboosting',\n",
    "    'multillr',\n",
    "    'nbeats',\n",
    "    'prophet',\n",
    "    'tuned_salient2',\n",
    "    #Ensembles\n",
    "    'linear_ensemble',  \n",
    "    'online_learning'\n",
    "]\n",
    "\n",
    "# Rodeo experiment model names\n",
    "rodeo_experiment_models = [\n",
    "    # Baselines\n",
    "    \"climatology\",   \n",
    "    'deb_cfsv2',\n",
    "    'persistence',\n",
    "    # ABC \n",
    "    'tuned_climpp',\n",
    "    'tuned_cfsv2pp',\n",
    "    'perpp_cfsv2',\n",
    "    #Learning\n",
    "    'autoknn',\n",
    "    'tuned_localboosting',\n",
    "    'multillr',\n",
    "    'prophet',\n",
    "    'tuned_salient2',\n",
    "    #Ensembles\n",
    "    'linear_ensemble',\n",
    "    'online_learning',\n",
    "#     'linear_ensemble_localFalse_dynamicFalse_stepFalse_LtCtD',\n",
    "#     'linear_ensemble_localFalse_dynamicFalse_stepFalse_AMLPtCtDtKtS',  \n",
    "#     'online_learning-ah_rpNone_R1_recent_g_SC_LtCtD',\n",
    "#     'online_learning-ah_rpNone_R1_recent_g_SC_AMLPtCtDtKtS'\n",
    "]\n",
    "\n",
    "# Salient experiment model names\n",
    "salient_experiment_models = [\n",
    "    # Baselines   \n",
    "    'deb_cfsv2',\n",
    "    # ABC \n",
    "    'tuned_cfsv2pp',\n",
    "    #Learning\n",
    "    'tuned_salient2',\n",
    "]\n",
    "\n",
    "\n",
    "# ECMWF experiment model names\n",
    "ecmwf_experiment_models = [\n",
    "    # Baselines\n",
    "    \"climatology\", \n",
    "    'deb_cfsv2',\n",
    "    'persistence',\n",
    "    # ABC \n",
    "    'tuned_climpp',\n",
    "    'tuned_cfsv2pp',\n",
    "    'perpp_cfsv2',\n",
    "    # ECMWF\n",
    "#     \"deb_ecmwf\",\n",
    "    'ecmwf-years20_leads15-15_lossmse_forecastc_debiasp+c',\n",
    "    'ecmwf-years20_leads15-15_lossmse_forecastp_debiasp+c',\n",
    "    # Ensembles\n",
    "    \"online_learning\", \n",
    "    \"linear_ensemble\" \n",
    "]\n",
    "\n",
    "graphcast_experiment_models = [\n",
    "    #Baseline\n",
    "    \"deb_cfsv2\",\n",
    "    # Model   \n",
    "    'graphcast',\n",
    "    # Ensembles\n",
    "    \"online_learning\", \n",
    "    \"linear_ensemble\" \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Dictionaries mapping all model names and tasks to their display names\n",
    "#\n",
    "\n",
    "east_tasks = {\n",
    "    \"east_tmp2m_34w\": \"Temp. weeks 3-4\",\n",
    "    \"east_tmp2m_56w\": \"Temp. weeks 5-6\",\n",
    "    \"east_precip_34w\": \"Precip. weeks 3-4\",\n",
    "    \"east_precip_56w\": \"Precip. weeks 5-6\"\n",
    "}\n",
    "\n",
    "contest_tasks = {\n",
    "    \"contest_tmp2m_34w\": \"Temp. weeks 3-4\",\n",
    "    \"contest_tmp2m_56w\": \"Temp. weeks 5-6\",\n",
    "    \"contest_precip_34w\": \"Precip. weeks 3-4\",\n",
    "    \"contest_precip_56w\": \"Precip. weeks 5-6\"\n",
    "}\n",
    "us_tasks = {\n",
    "    \"us_tmp2m_34w\": \"Temp. weeks 3-4\",\n",
    "    \"us_tmp2m_56w\": \"Temp. weeks 5-6\",\n",
    "    \"us_precip_34w\": \"Precip. weeks 3-4\",\n",
    "    \"us_precip_56w\": \"Precip. weeks 5-6\",   \n",
    "}\n",
    "\n",
    "us_1_5_tasks = {\n",
    "    \"us_tmp2m_1.5x1.5_34w\": \"Temp. weeks 3-4\",\n",
    "    \"us_tmp2m_1.5x1.5_56w\": \"Temp. weeks 5-6\",\n",
    "    \"us_precip_1.5x1.5_34w\": \"Precip. weeks 3-4\",\n",
    "    \"us_precip_1.5x1.5_56w\": \"Precip. weeks 5-6\",   \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model list for generate predictions\n",
    "`tuned_climpp,tuned_cfsv2pp,tuned_localboosting,tuned_salient_fri,perpp,multillr,autoknn,raw_cfsv2,nbeats_final,prophet`\n",
    "## Model list for tuning\n",
    "`climpp,cfsv2,catboost,salient_fri`\n",
    "## Tuned models for metrics\n",
    "`tuned_climpp,tuned_cfsv2pp,tuned_localboosting,tuned_salient_fri`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in all metrics for all tasks and all models\n",
    "Reads metrics, generates a summary of missing data, and produces the `all_metrics` dictionary to be used in further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate a dictionary with metric values for all models and every combination of gt_id, \n",
    "horizon, and target dates\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# TODO: change to add us_gt_ids once metrics are ready\n",
    "all_metrics = {}\n",
    "\n",
    "# Get metrics for main experiment, rodeo experiment, salient experiment and ecmwf experiment\n",
    "for metric, gt_id, horizon, target_dates in \\\n",
    "        [x for x in product(['rmse', 'skill'], us_gt_ids, horizons, ['std_paper'])] \\\n",
    "        +[x for x in product(['rmse'], contest_gt_ids, horizons, ['std_contest'])] \\\n",
    "        +[x for x in product(['rmse'], contest_gt_ids, horizons, ['std_paper'])] \\\n",
    "        +[x for x in product(['rmse', 'skill'], us_gt_ids, ['34w'], ['std_paper_graphcast'])] \\\n",
    "        +[x for x in product(['rmse', 'skill'], us_1_5_gt_ids, horizons, ['std_ecmwf'])]: \n",
    "   \n",
    "    \n",
    "    #Set model names   \n",
    "    if 'us' in gt_id:\n",
    "        if 'graphcast' in target_dates:\n",
    "            model_names = graphcast_experiment_models \n",
    "            model_names_str = 'graphcast_experiment_models' \n",
    "        elif '1.5x1.5' in gt_id:\n",
    "            model_names = ecmwf_experiment_models \n",
    "            model_names_str = 'ecmwf_experiment_models' \n",
    "        else:\n",
    "            model_names =  main_experiment_models\n",
    "            model_names_str = 'main_experiment_models'\n",
    "    elif 'contest' in gt_id:\n",
    "        model_names = rodeo_experiment_models if 'contest' in target_dates else salient_experiment_models\n",
    "        model_names_str = 'rodeo_experiment_models' if 'contest' in target_dates else 'salient_experiment_models'\n",
    "    else:\n",
    "        model_names = all_models\n",
    "        model_names_str = 'all_models'\n",
    "\n",
    "\n",
    "    # Get task\n",
    "    task = f\"{gt_id}_{horizon}\"\n",
    "    display(Markdown(f\"### {model_names_str}: {metric}, {task}, {target_dates}\"))\n",
    "\n",
    "    # Get all metrics\n",
    "    if metric is 'skill' and 'climatology' in model_names:\n",
    "        display(Markdown(f\"#### ===> Warning: skill is not calculated for the climatology baseline model.\"))\n",
    "    df = get_metrics_df(gt_id, horizon, metric, target_dates, model_names=model_names)\n",
    "    \n",
    "        \n",
    "\n",
    "    # No models exist for this task    \n",
    "    if df is None: \n",
    "        continue\n",
    "\n",
    "    # Add yearly and quarterly columns to the dataframe\n",
    "    df = add_groupby_cols(df, horizon=horizon)\n",
    "    all_metrics[(metric, task, target_dates)] = copy.copy(df)\n",
    "\n",
    "    if metric in ['rmse', 'skill']:\n",
    "        key = (metric, task, target_dates)\n",
    "        try:        \n",
    "            missing_df = all_metrics[key].loc[(all_metrics[key][model_names].isnull().any(axis=1)),:]\n",
    "        except:        \n",
    "            missing_df = all_metrics[key].loc[(all_metrics[key].isnull().any(axis=1)),:]            \n",
    "#         if missing_df.shape[0] != 0:\n",
    "#             True\n",
    "#             display(Markdown(f\"#### Missing metrics\"))\n",
    "#             display(missing_df)    \n",
    "#         else:\n",
    "#             printf(\"All metrics present.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting metrics \n",
    "After generating the above `all_metrics` dictionary, you can get the average metric value for a set of models and specific period using the following function:\n",
    "```\n",
    "df = get_per_period_metrics_df(all_metrics, period=\"quarterly\", horizon=\"34w\", metric=\"score\", target_dates=\"std_paper\", model_names=all_models)\n",
    "```\n",
    "\n",
    "The period can be `quarterly`, `yearly`, or `quarterly_yearly` (returns average metrics values in YY1-Q1, YY1-Q2, ..., YY2-Q1, ... etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 1: \n",
    "#### Schematic of the SubseasonalClimateUSA data collection and processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printf(\"see subseasonal_toolkit/viz/benchmark/figures/flowcharts/perpp-get_weights.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 2: \n",
    "#### Example of SubseasonalClimateUSA observations and dynamical model forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate figure metrics\n",
    "figure_gt_ids = us_1_5_gt_ids\n",
    "figure_horizons = ['34w']\n",
    "figure_target_dates = 'std_paper_forecast'\n",
    "figure_metrics = ['lat_lon_pred']\n",
    "figure_models = [\n",
    "    # Baselines\n",
    "    \"gt\",\n",
    "    \"raw_cfsv2\",\n",
    "    \"raw_ecmwf\",\n",
    "    \"raw_ccsm4\", \n",
    "    \"raw_geos_v2p1\",\n",
    "    \"raw_nesm\",\n",
    "    \"raw_fimr1p1\",\n",
    "    \"raw_gefs\",\n",
    "    \"raw_gem\",    \n",
    "]\n",
    "\n",
    "metric_dfs_rda = {}\n",
    "for gt_id, horizon in product(figure_gt_ids, figure_horizons):\n",
    "    task = f\"{gt_id}_{horizon}\"\n",
    "    display(Markdown(f\"#### Getting metrics for {gt_id} {horizon}\"))\n",
    "    metric_dfs_rda[task] = get_models_metric_lat_lon(gt_id=gt_id, horizon=horizon, \n",
    "                                                     target_dates=figure_target_dates, \n",
    "                                                     metrics = figure_metrics, \n",
    "                                                     model_names=figure_models,\n",
    "                                                    first_target_date = True)\n",
    "    \n",
    "# Set figure parameter\n",
    "figure_gt_ids = us_1_5_gt_ids\n",
    "figure_horizons = ['34w']\n",
    "figure_metric = 'lat_lon_pred'\n",
    "figure_mean_metric_df = None\n",
    "figure_source_data = False\n",
    "figure_show = True\n",
    "\n",
    "\n",
    "figure_model_names = figure_models \n",
    "display(Markdown(f'#### Models: {\", \".join(figure_model_names)}'))\n",
    "for gt_id in figure_gt_ids:\n",
    "    display(Markdown(f\"#### {gt_id}\"))\n",
    "    figure_CB_minmax = (-20, 20) if 'tmp2m' in gt_id else (0, 80)\n",
    "    plot_metric_maps_task(metric_dfs_rda, model_names=figure_model_names,\n",
    "                         gt_ids=[gt_id],\n",
    "                         horizons=figure_horizons,\n",
    "                         metric=figure_metric,\n",
    "                         target_dates=figure_target_dates,\n",
    "                         mean_metric_df=figure_mean_metric_df,\n",
    "                         show=figure_show, \n",
    "                         scale_type='linear',\n",
    "                         CB_colors_customized=[\"white\", \"#dede00\", \"#ff7f00\", \"blueviolet\", \"indigo\", \"yellowgreen\", \"lightgreen\", \"darkgreen\"],\n",
    "                         CB_minmax = figure_CB_minmax,\n",
    "                         source_data = figure_source_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 1: \n",
    "#### Average percentage skill and percentage improvement over mean debiased CFSv2 RMSE across 2011-2020 in the contiguous U.S. along with a 95% bootstrap confidence interval. The best performing model in each model group is bolded, and the best performing model overall is shown in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Paper experiments; can be configured to generate metrics for any subset of models, averaged over\n",
    "a set of periods, for a given target data. Produces a table and saves to tex. \n",
    "\"\"\"\n",
    "\n",
    "target_dates = \"std_paper\" \n",
    "\n",
    "# quarterly (seasonal quarters), contest_quarterly (contest quarters), \n",
    "# monthly, yearly, individual (return full dataframe), overall (return mean of full dataframe)\n",
    "# monthly_yearly (every month in every year), quarterly_yearly (every quarter in every year)\n",
    "period = \"overall\" # <- must be overall to merge with rodeo dataframe\n",
    "figure_metrics = [\"rmse\", \"skill\"] \n",
    "table_models = main_experiment_models \n",
    "relative_to = 'deb_cfsv2' # compute value relative to climatology value: 1 - metric(model)/metric(climatology) \n",
    "dropna = True # if true, compute average metrics only on dates where predictions have all values \n",
    "task_ids = us_gt_ids # contest_gt_ids (for contest), us_gt_ids (for us), gt_ids (for all)\n",
    "horizons = horizons\n",
    "region = 'us' #either us, east or contest\n",
    "include_overall = True # include overall row in the dataframe\n",
    "\n",
    "\"\"\"\n",
    "End experiment parameters \n",
    "\"\"\"\n",
    "\n",
    "for metric in figure_metrics:\n",
    "    if metric is \"skill\":\n",
    "        table_models = [m for m in table_models if m is not \"climatology\"]\n",
    "        relative_to = None\n",
    "    if (metric is 'rmse' and relative_to is not None) or (metric is not 'rmse' and relative_to is None):\n",
    "        highlight_func = highlight_max\n",
    "        bold_func = bold_max\n",
    "    else:\n",
    "        highlight_func = highlight_min\n",
    "        bold_func = bold_min\n",
    "\n",
    "    \"\"\"\n",
    "    Display metric table\n",
    "    \"\"\"\n",
    "#     display(Markdown(f\"##### {metric} -- {target_dates}\"))\n",
    "    # Get set of tasks \n",
    "    tasks = [f\"{gt_id}_{horizon}\" for (gt_id, horizon) in product(task_ids, horizons)]\n",
    "    metrics_sub = None\n",
    "\n",
    "    # Generate metrics dataframe for each task\n",
    "    for i, (gt_id, horizon) in enumerate(product(task_ids, horizons)):\n",
    "        task = f\"{gt_id}_{horizon}\"\n",
    "#         printf(f\"Processing {task}\")\n",
    "        # Read in metrics and reset experiment_models based on avaliable metrics\n",
    "        m_sub, experiment_models = get_per_period_metrics_df(\n",
    "            all_metrics, period=period, gt_id=gt_id, horizon=horizon,\n",
    "            metric=metric, target_dates=target_dates, \n",
    "            relative_to=relative_to,\n",
    "            model_names=table_models, include_overall=include_overall, dropna=dropna)\n",
    "       # Create metrics dataframe template\n",
    "        if period == \"overall\":\n",
    "            index = pd.Index([task], name=\"task\")            \n",
    "        else:\n",
    "            index = pd.MultiIndex.from_product(\n",
    "                [[task], m_sub.index], \n",
    "                names=('task', 'period'))   \n",
    "        # Need to form task-by-task, since some tasks are missing target dates, so index differs\n",
    "        if metrics_sub is None:\n",
    "            metrics_sub = pd.DataFrame(index=index, columns=experiment_models)           \n",
    "        else:\n",
    "            metrics_sub = pd.concat([metrics_sub, pd.DataFrame(index=index, columns=experiment_models)])\n",
    "        if period == \"overall\":\n",
    "            metrics_sub.loc[task, :] = m_sub\n",
    "        else:\n",
    "            metrics_sub[metrics_sub.index.get_level_values(\"task\") == task] = m_sub.values\n",
    "\n",
    "    # maintain ordered list of model names\n",
    "    metrics_sub = metrics_sub[[m for m in main_experiment_models if m in table_models]]\n",
    "    if metric is \"skill\":\n",
    "        metrics_sub = metrics_sub.multiply(100)\n",
    "\n",
    "    if period is 'overall':\n",
    "        metrics_sub=metrics_sub.rename(us_tasks, axis=0).T\n",
    "        metrics_sub['model_type'] = [all_model_types[m] for m in metrics_sub.index]\n",
    "        metrics_sub = metrics_sub.rename(all_model_names, axis=0).reset_index().set_index(['model_type', 'index'])\n",
    "        metrics_sub.columns = metrics_sub.columns.get_level_values(0)\n",
    "    #         display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "        #for group in ['Baselines', 'ABC', 'Learning', 'Ensembles']:\n",
    "            #display(metrics_sub.loc[group].style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "    else:\n",
    "        metrics_sub = metrics_sub.reindex([m for m in table_models if m in metrics_sub.columns], axis=1).T\n",
    "    #         display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "\n",
    "    #save dataframe in latex table format\n",
    "    table_to_tex(metrics_sub.astype(float), tables_dir, f\"table_{region}_{period}_{metric}_over_{relative_to}_{target_dates}\", precision=2)\n",
    "#     printf(f\"Table saved in {os.path.join(tables_dir, f'table_{region}_{period}_{metric}_over_{relative_to}_{target_dates}.tex')}\\n\")\n",
    "\n",
    "    \"\"\"\n",
    "    Display metric table with standard error\n",
    "    \"\"\"\n",
    "    # Display and save table with standard error\n",
    "    display(Markdown(f\"##### {metric} +/- SE -- {target_dates}\"))         \n",
    "    np.random.seed(123)\n",
    "    # Read in metrics and reset experiment_models based on avaliable metrics\n",
    "    m_sub, experiment_models = get_per_period_metrics_df(\n",
    "        all_metrics, period=\"individual\", gt_id=gt_id, horizon=horizon,\n",
    "        metric=metric, target_dates=target_dates, \n",
    "        relative_to=relative_to,\n",
    "        model_names=table_models, include_overall=include_overall, dropna=dropna)\n",
    "    metrics_sub_se= metrics_sub.copy()\n",
    "    m_sub_se_or = pd.DataFrame(index=experiment_models, columns = metrics_sub.columns)\n",
    "\n",
    "    # Generate metrics dataframe for each task\n",
    "    for i, (gt_id, horizon) in enumerate(product(task_ids, horizons)):\n",
    "        task = f\"{gt_id}_{horizon}\"\n",
    "        printf(f\"Processing {task}\")  \n",
    "        # Read in metrics and reset experiment_models based on avaliable metrics\n",
    "        m_sub, experiment_models = get_per_period_metrics_df(\n",
    "            all_metrics, period=\"individual\", gt_id=gt_id, horizon=horizon,\n",
    "            metric=metric, target_dates=target_dates, \n",
    "            relative_to=relative_to,\n",
    "            model_names=table_models, include_overall=include_overall, dropna=dropna)\n",
    "        task = us_tasks[task]\n",
    "        # import scikits.bootstraps as bootstraps\n",
    "        for col in experiment_models:\n",
    "            mean_col = m_sub[col].mean()\n",
    "            std_col = m_sub[col].std()\n",
    "            n_col = m_sub[col].notna().sum()\n",
    "            t1 = [simulate_sample_mean(n_col, mean_col, std_col) for i in range(1000)]\n",
    "            summary1 = summarize(t1, digits=6)\n",
    "            m_sub_se_or.loc[col][task] = summary1['SE'][0]\n",
    "        # maintain ordered list of model names\n",
    "        m_sub_se = m_sub_se_or.T\n",
    "        m_sub_se = m_sub_se[[m for m in main_experiment_models if m in table_models]].T\n",
    "        if metric is \"skill\":\n",
    "            m_sub_se = m_sub_se.multiply(100) \n",
    "        if period is 'overall':\n",
    "            m_sub_se['model_type'] = [all_model_types[m] for m in m_sub_se.index]\n",
    "            m_sub_se = m_sub_se.rename(all_model_names, axis=0).reset_index().set_index(['model_type', 'index'])\n",
    "            m_sub_se.columns = m_sub_se.columns.get_level_values(0)\n",
    "\n",
    "    # display and save dataframe in latex table format\n",
    "    m_sub = metrics_sub.astype(float).round(2).astype(str).add(' $\\pm$ ').add(m_sub_se.astype(float).round(2).astype(str))\n",
    "    display(m_sub.style.apply(lambda x: metrics_sub.apply(highlight_func, axis=0), axis=None).apply(lambda x: metrics_sub.apply(bold_func, axis=0), axis=None))\n",
    "    filename_table = f\"table_{region}_{period}_{metric}_over_{relative_to}_{target_dates}_se\"\n",
    "    table_to_tex(m_sub, tables_dir, filename_table, precision=2)\n",
    "    printf(f\"Table saved in {os.path.join(tables_dir, f'{filename_table}.tex')}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 2: \n",
    "#### Average percentage skill and percentage improvement over mean debiased CFSv2 RMSE across 2016-2020 in the contiguous U.S. along with a 95% bootstrap confidence interval. The best performing model in each model group is bolded, and the best performing model overall is shown in green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dates = \"std_ecmwf\" \n",
    "period = \"overall\" \n",
    "figure_metrics = [\"rmse\", \"skill\"]\n",
    "table_models = ecmwf_experiment_models\n",
    "relative_to = 'deb_cfsv2'\n",
    "dropna = True  \n",
    "task_ids = us_1_5_gt_ids \n",
    "task_horizons_list = [[\"34w\"], [\"56w\"]]\n",
    "region = 'us_1.5x1.5' \n",
    "include_overall = True \n",
    "\n",
    "\"\"\"\n",
    "End experiment parameters \n",
    "\"\"\"\n",
    "for metric, task_horizons in product(figure_metrics, task_horizons_list):\n",
    "    \n",
    "    if metric is \"skill\":\n",
    "        table_models = [m for m in table_models if m is not \"climatology\"]\n",
    "        relative_to = None\n",
    "    if (metric is 'rmse' and relative_to is not None) or (metric is not 'rmse' and relative_to is None):\n",
    "        highlight_func = highlight_max\n",
    "        bold_func = bold_max\n",
    "    else:\n",
    "        highlight_func = highlight_min\n",
    "        bold_func = bold_min\n",
    "       \n",
    "    \"\"\"\n",
    "    Display metric table\n",
    "    \"\"\"\n",
    "#     display(Markdown(f\"##### {metric} -- {target_dates}\"))\n",
    "    # Get set of tasks \n",
    "    tasks = [f\"{gt_id}_{horizon}\" for (gt_id, horizon) in product(task_ids, task_horizons)]\n",
    "    metrics_sub = None\n",
    "\n",
    "    # Generate metrics dataframe for each task\n",
    "    for i, (gt_id, horizon) in enumerate(product(task_ids, task_horizons)):\n",
    "        task = f\"{gt_id}_{horizon}\"\n",
    "#         printf(f\"Processing {task}\")\n",
    "        # Read in metrics and reset experiment_models based on avaliable metrics\n",
    "        m_sub, experiment_models = get_per_period_metrics_df(\n",
    "            all_metrics, period=period, gt_id=gt_id, horizon=horizon,\n",
    "            metric=metric, target_dates=target_dates, \n",
    "            relative_to=relative_to,\n",
    "            model_names=table_models, include_overall=include_overall, dropna=dropna)\n",
    "\n",
    "       # Create metrics dataframe template\n",
    "        if period == \"overall\":\n",
    "            index = pd.Index([task], name=\"task\")            \n",
    "        else:\n",
    "            index = pd.MultiIndex.from_product(\n",
    "                [[task], m_sub.index], \n",
    "                names=('task', 'period'))   \n",
    "        # Need to form task-by-task, since some tasks are missing target dates, so index differs\n",
    "        if metrics_sub is None:\n",
    "            metrics_sub = pd.DataFrame(index=index, columns=experiment_models)           \n",
    "        else:\n",
    "            metrics_sub = pd.concat([metrics_sub, pd.DataFrame(index=index, columns=experiment_models)])\n",
    "        if period == \"overall\":\n",
    "            metrics_sub.loc[task, :] = m_sub\n",
    "        else:\n",
    "            metrics_sub[metrics_sub.index.get_level_values(\"task\") == task] = m_sub.values\n",
    "\n",
    "    # maintain ordered list of model names\n",
    "    metrics_sub = metrics_sub[table_models]\n",
    "\n",
    "    if metric is \"skill\":\n",
    "        metrics_sub = metrics_sub.multiply(100)\n",
    "\n",
    "    if period is 'overall':\n",
    "        metrics_sub=metrics_sub.rename(us_tasks, axis=0).T\n",
    "        metrics_sub['model_type'] = [all_model_types[m] for m in metrics_sub.index]\n",
    "        metrics_sub = metrics_sub.rename(all_model_names, axis=0).reset_index().set_index(['model_type', 'index'])\n",
    "        metrics_sub.columns = metrics_sub.columns.get_level_values(0)\n",
    "#         display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "    else:\n",
    "        metrics_sub = metrics_sub.reindex([m for m in table_models if m in metrics_sub.columns], axis=1).T\n",
    "#         display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "\n",
    "    #save dataframe in latex table format\n",
    "    table_to_tex(metrics_sub.astype(float), tables_dir, f\"table_{region}_{horizon}_{period}_{metric}_over_{relative_to}_{target_dates}\", precision=2)\n",
    "#     printf(f\"Table saved in {os.path.join(tables_dir, f'table_{region}_{horizon}_{period}_{metric}_over_{relative_to}_{target_dates}.tex')}\\n\")\n",
    "\n",
    "    \"\"\"\n",
    "    Display metric table with standard error\n",
    "    \"\"\"\n",
    "    # Display and save table with standard error\n",
    "    display(Markdown(f\"##### {metric} +/- SE -- {target_dates}\"))         \n",
    "    np.random.seed(123)\n",
    "    # Read in metrics and reset experiment_models based on avaliable metrics\n",
    "    m_sub, experiment_models = get_per_period_metrics_df(all_metrics, \n",
    "                                                         period=\"individual\", \n",
    "                                                         gt_id=gt_id, \n",
    "                                                         horizon=horizon,\n",
    "                                                         metric=metric, \n",
    "                                                         target_dates=target_dates, \n",
    "                                                         relative_to=relative_to,\n",
    "                                                         model_names=table_models, \n",
    "                                                         include_overall=include_overall, \n",
    "                                                         dropna=dropna)\n",
    "    m_sub_se_or = pd.DataFrame(index=experiment_models, columns = metrics_sub.columns)\n",
    "\n",
    "    # Generate metrics dataframe for each task\n",
    "    for i, (gt_id, horizon) in enumerate(product(task_ids, task_horizons)):\n",
    "        task = f\"{gt_id}_{horizon}\"\n",
    "        printf(f\"Processing {task}\")  \n",
    "        # Read in metrics and reset experiment_models based on avaliable metrics\n",
    "        m_sub, experiment_models = get_per_period_metrics_df(\n",
    "            all_metrics, period=\"individual\", gt_id=gt_id, horizon=horizon,\n",
    "            metric=metric, target_dates=target_dates, \n",
    "            relative_to=relative_to,\n",
    "            model_names=table_models, include_overall=include_overall, dropna=dropna)\n",
    "\n",
    "\n",
    "        for col in experiment_models:\n",
    "            mean_col = m_sub[col].mean()\n",
    "            std_col = m_sub[col].std()\n",
    "            n_col = m_sub[col].notna().sum()\n",
    "            t1 = [simulate_sample_mean(n_col, mean_col, std_col) for i in range(1000)]\n",
    "            summary1 = summarize(t1, digits=6)\n",
    "            m_sub_se_or.loc[col][task] = summary1['SE'][0]\n",
    "\n",
    "\n",
    "        # maintain ordered list of model names\n",
    "        m_sub_se = m_sub_se_or.T\n",
    "        m_sub_se = m_sub_se[table_models].T\n",
    "        if metric is \"skill\":\n",
    "            m_sub_se = m_sub_se.multiply(100) \n",
    "        if period is 'overall':\n",
    "            m_sub_se['model_type'] = [all_model_types[m] for m in m_sub_se.index]\n",
    "            m_sub_se = m_sub_se.rename(all_model_names, axis=0).reset_index().set_index(['model_type', 'index'])\n",
    "            m_sub_se.columns = m_sub_se.columns.get_level_values(0)\n",
    "\n",
    "    # display and save dataframe in latex table format\n",
    "    m_sub = metrics_sub.astype(float).round(2).astype(str).add(' $\\pm$ ').add(m_sub_se.astype(float).round(2).astype(str))\n",
    "    display(m_sub.style.apply(lambda x: metrics_sub.apply(highlight_func, axis=0), axis=None).apply(lambda x: metrics_sub.apply(bold_func, axis=0), axis=None))\n",
    "    filename_table = f\"table_{region}_{horizon}_{period}_{metric}_over_{relative_to}_{target_dates}_se\"\n",
    "    table_to_tex(m_sub, tables_dir, filename_table, precision=2)\n",
    "    printf(f\"Table saved in {os.path.join(tables_dir, f'{filename_table}.tex')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 3: \n",
    "### % improvement: ABC vs. LEARNING MODELS\n",
    "#### Per season and per year average skill and improvement over mean debiased CFSv2 RMSE across the contiguous U.S. and the years 2011-2020. Despite their simplicity, the ABC models (solid lines) consistently outperform debiased CFSv2 and the state-of-the-art learners (dotted lines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_models = [\n",
    "    #relative_to\n",
    "    'deb_cfsv2',\n",
    "    #ABC\n",
    "    'tuned_cfsv2pp',\n",
    "    'tuned_climpp',\n",
    "    'perpp_cfsv2',\n",
    "    #learner\n",
    "    'autoknn',\n",
    "    'multillr',\n",
    "    'prophet',\n",
    "    'tuned_localboosting',\n",
    "    'tuned_salient2',\n",
    "]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Paper experiments; can be configured to generate plots for any subset of models, averaged over\n",
    "a set of periods, for a given target data. Produces a figure and saves to pdf.. \n",
    "\"\"\"\n",
    "# Figure experiment parameters\n",
    "target_dates = \"std_paper\"\n",
    "task_ids = us_gt_ids \n",
    "task_horizons = horizons\n",
    "\n",
    "# RMSE improvement by season \n",
    "# Subfigure experiment parameters\n",
    "period = \"quarterly\" \n",
    "metric = 'rmse' \n",
    "relative_to = 'deb_cfsv2' # compute value relative to baseline value: 1 - metric(model)/metric(baseline)\n",
    "file_str = f\"{period}_over_{relative_to}\" # saves to file with suffix file_str\n",
    "# Generate figure\n",
    "print(target_dates)\n",
    "fh = partial(get_per_period_metrics_df, all_metrics, period, relative_to) \n",
    "plot_ABC_vs_learner_quadruple(get_metrics_fh=fh, \n",
    "                    gt_id_list=task_ids, \n",
    "                    horizon_list=task_horizons, \n",
    "                    metric=metric, \n",
    "                    target_dates=target_dates, \n",
    "                    model_names=figure_models,\n",
    "                    file_str=file_str)\n",
    "\n",
    "# RMSE improvement by year \n",
    "# Subfigure experiment parameters\n",
    "period = \"yearly\" \n",
    "metric = 'rmse' \n",
    "relative_to = 'deb_cfsv2' # compute value relative to baseline value: 1 - metric(model)/metric(baseline)\n",
    "file_str = f\"{period}_over_{relative_to}\" # saves to file with suffix file_str\n",
    "# Generate figure\n",
    "print(target_dates)\n",
    "fh = partial(get_per_period_metrics_df, all_metrics, period, relative_to) \n",
    "plot_ABC_vs_learner_quadruple(get_metrics_fh=fh, \n",
    "                    gt_id_list=task_ids, \n",
    "                    horizon_list=task_horizons, \n",
    "                    metric=metric, \n",
    "                    target_dates=target_dates, \n",
    "                    model_names=figure_models,\n",
    "                    file_str=file_str)\n",
    "\n",
    "# Skill improvement by season \n",
    "# Subfigure experiment parameters\n",
    "period = \"quarterly\" \n",
    "metric = 'skill' \n",
    "relative_to = None # compute value relative to baseline value: 1 - metric(model)/metric(baseline)\n",
    "file_str = f\"{period}_over_{relative_to}\" # saves to file with suffix file_str\n",
    "# Generate figure\n",
    "print(target_dates)\n",
    "fh = partial(get_per_period_metrics_df, all_metrics, period, relative_to) \n",
    "plot_ABC_vs_learner_quadruple(get_metrics_fh=fh, \n",
    "                    gt_id_list=task_ids, \n",
    "                    horizon_list=task_horizons, \n",
    "                    metric=metric, \n",
    "                    target_dates=target_dates, \n",
    "                    model_names=figure_models,\n",
    "                    file_str=file_str)\n",
    "\n",
    "# Skill improvement by season \n",
    "# Subfigure experiment parameters\n",
    "period = \"yearly\" \n",
    "metric = 'skill' \n",
    "relative_to = None # compute value relative to baseline value: 1 - metric(model)/metric(baseline)\n",
    "file_str = f\"{period}_over_{relative_to}\" # saves to file with suffix file_str\n",
    "# Generate figure\n",
    "print(target_dates)\n",
    "fh = partial(get_per_period_metrics_df, all_metrics, period, relative_to) \n",
    "plot_ABC_vs_learner_quadruple(get_metrics_fh=fh, \n",
    "                    gt_id_list=task_ids, \n",
    "                    horizon_list=task_horizons, \n",
    "                    metric=metric, \n",
    "                    target_dates=target_dates, \n",
    "                    model_names=figure_models,\n",
    "                    file_str=file_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 4: \n",
    "#### Percentage improvement over mean debiased CFSv2 RMSE in the contiguous U.S. over 2011-2020. White grid points indicate negative or 0% improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate figure metrics\n",
    "figure_gt_ids = us_gt_ids\n",
    "figure_horizons = horizons\n",
    "figure_target_dates = 'std_paper'\n",
    "figure_metrics = ['lat_lon_rmse']\n",
    "figure_models = [\n",
    "    # Baselines\n",
    "    'deb_cfsv2',\n",
    "    \"tuned_climpp\",\n",
    "    \"tuned_cfsv2pp\",\n",
    "    \"perpp_cfsv2\",\n",
    "    \"prophet\", \n",
    "    \"tuned_salient2\",\n",
    "    \"online_learning\",\n",
    "]\n",
    "\n",
    "metric_dfs_rda = {}\n",
    "for gt_id, horizon in product(figure_gt_ids, figure_horizons):\n",
    "    task = f\"{gt_id}_{horizon}\"\n",
    "    display(Markdown(f\"#### Getting metrics for {gt_id} {horizon}\"))\n",
    "    metric_dfs_rda[task] = get_models_metric_lat_lon(gt_id=gt_id, horizon=horizon, \n",
    "                                                     target_dates=figure_target_dates, \n",
    "                                                     metrics = figure_metrics, \n",
    "                                                     model_names=figure_models)\n",
    "    \n",
    "# Set figure parameter\n",
    "figure_gt_ids = us_gt_ids\n",
    "figure_horizons = horizons #['34w']\n",
    "figure_metric = 'lat_lon_rmse'\n",
    "figure_relative_to = 'deb_cfsv2'\n",
    "figure_mean_metric_df = None\n",
    "figure_cb_skip = 4\n",
    "figure_source_data = False\n",
    "figure_show = True\n",
    "\n",
    "\n",
    "\n",
    "figure_model_names = figure_models \n",
    "display(Markdown(f'#### Models: {\", \".join(figure_model_names)}'))\n",
    "for figure_gt_id, figure_horizon in product(figure_gt_ids, figure_horizons):\n",
    "    display(Markdown(f\"#### {figure_gt_id} {figure_horizon}\"))\n",
    "    figure_cb_minmax = (0, 20)# if 'tmp2m' in gt_id else (0, 80)\n",
    "    plot_metric_maps_task_ds(metric_dfs_rda, model_names=figure_model_names,\n",
    "                         gt_id=figure_gt_id,\n",
    "                         horizon=figure_horizon,\n",
    "                         metric=figure_metric,\n",
    "                         target_dates=figure_target_dates,\n",
    "                         relative_to=figure_relative_to,\n",
    "                         mean_metric_df=figure_mean_metric_df,\n",
    "                         show=figure_show, \n",
    "                         scale_type='linear',\n",
    "                         CB_colors_customized=[\"white\", \"green\", \"darkgreen\"],\n",
    "                         CB_minmax = figure_cb_minmax,\n",
    "                         CB_skip = figure_cb_skip,\n",
    "                         source_data = figure_source_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary figures\n",
    " \n",
    "## B. Model Implementation Details\n",
    "## Figure 5: \n",
    "#### Climatology++ hyperparameters automatically selected for each target date in 2011-2020.\n",
    "Tuning plots: This code produces plots to analyze submodels selected by the tuner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure parameters\n",
    "figure_models = [\"tuned_climpp\"]\n",
    "target_dates = \"std_paper\"\n",
    "task_ids = us_gt_ids\n",
    "task_horizons = horizons\n",
    "\n",
    "# Generate subfigures\n",
    "plot_tuning(gt_ids = task_ids,\n",
    "            horizons = task_horizons,\n",
    "            target_dates = target_dates,\n",
    "            model_names = figure_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 6: \n",
    "#### CFSv2++ hyperparameters automatically selected for each target date in 2011-2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure parameters\n",
    "figure_models = [\"tuned_cfsv2pp\"]\n",
    "target_dates = \"std_paper\"\n",
    "task_ids = us_gt_ids\n",
    "task_horizons = horizons\n",
    "\n",
    "# Generate subfigures\n",
    "plot_tuning(gt_ids = task_ids,\n",
    "            horizons = task_horizons,\n",
    "            target_dates = target_dates,\n",
    "            model_names = figure_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7: \n",
    "#### Spatial variation in Persistence++ learned regression weights when forecasting temperature in weeks 3-4 for the final target date, December 23, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printf(\"see subseasonal_toolkit/examples/benchmark/perpp-get_weights.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 8: \n",
    "#### Spatial variation in Persistence++ learned regression weights when forecasting temperature in weeks 5-6 for the final target date, December 23, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printf(\"see subseasonal_toolkit/examples/benchmark/perpp-get_weights.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 9: \n",
    "#### Spatial variation in Persistence++ learned regression weights when forecasting precipitation in weeks 3-4 for the final target date, December 23, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printf(\"see subseasonal_toolkit/examples/benchmark/perpp-get_weights.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10: \n",
    "#### Spatial variation in Persistence++ learned regression weights when forecasting precipitation in weeks 5-6 for the final target date, December 23, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printf(\"see subseasonal_toolkit/examples/benchmark/perpp-get_weights.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11: \n",
    "#### LocalBoosting hyperparameters automatically selected for each target date in 2011-2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure parameters\n",
    "figure_models = [\"tuned_localboosting\"]\n",
    "target_dates = \"std_paper\"\n",
    "task_ids = us_gt_ids\n",
    "task_horizons = horizons\n",
    "\n",
    "# Generate subfigures\n",
    "plot_tuning(gt_ids = task_ids,\n",
    "            horizons = task_horizons,\n",
    "            target_dates = target_dates,\n",
    "            model_names = figure_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 12: \n",
    "#### Salient 2.0 hyperparameters automatically selected for each target date in 2011-2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure parameters\n",
    "figure_models = ['tuned_salient2']\n",
    "target_dates = \"std_paper\"\n",
    "task_ids = us_gt_ids\n",
    "task_horizons = horizons\n",
    "\n",
    "# Generate subfigures\n",
    "plot_tuning(gt_ids = task_ids,\n",
    "            horizons = task_horizons,\n",
    "            target_dates = target_dates,\n",
    "            model_names = figure_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Supplementary Results\n",
    "## C.1 Percentage Improvement over Meteorological Baselines\n",
    "## Figure 13: \n",
    "#### Per season improvement of each ABC model over its corresponding baseline across the contiguous U.S. and the years 2011-2020. The learned ABC benchmarks yield consistent improvements in mean RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure experiment parameters\n",
    "target_dates = \"std_paper\" \n",
    "period = \"quarterly\" \n",
    "metric = \"rmse\" \n",
    "task_ids = us_gt_ids \n",
    "region = 'us'\n",
    "\n",
    "# RMSE improvement by season \n",
    "# Subfigure experiment parameters\n",
    "figure_models = [\n",
    "    'climatology', \n",
    "    'tuned_climpp',\n",
    "]\n",
    "relative_to = figure_models[0] \n",
    "file_str = f\"{region}_{period}_over_{relative_to}\" # saves to file with suffix file_str\n",
    "#Generate subfigure\n",
    "fh = partial(get_per_period_metrics_df, all_metrics, period, relative_to) \n",
    "plot_models_and_metrics_plus(get_metrics_fh=fh, \n",
    "                            gt_id_list=task_ids, \n",
    "                            horizon_list=horizons, \n",
    "                            metric=metric, \n",
    "                            target_dates=target_dates, \n",
    "                            model_names=figure_models, \n",
    "                            file_str=file_str)\n",
    "\n",
    "# RMSE improvement by season \n",
    "# Subfigure experiment parameters\n",
    "figure_models = [\n",
    "    'deb_cfsv2', \n",
    "    'tuned_cfsv2pp',\n",
    "]\n",
    "relative_to = figure_models[0] \n",
    "file_str = f\"{region}_{period}_over_{relative_to}\" # saves to file with suffix file_str\n",
    "#Generate subfigure\n",
    "fh = partial(get_per_period_metrics_df, all_metrics, period, relative_to) \n",
    "plot_models_and_metrics_plus(get_metrics_fh=fh, \n",
    "                            gt_id_list=task_ids, \n",
    "                            horizon_list=horizons, \n",
    "                            metric=metric, \n",
    "                            target_dates=target_dates, \n",
    "                            model_names=figure_models, \n",
    "                            file_str=file_str)\n",
    "\n",
    "# RMSE improvement by season \n",
    "# Subfigure experiment parameters\n",
    "figure_models = [\n",
    "    'persistence', \n",
    "    'perpp_cfsv2',\n",
    "]\n",
    "relative_to = figure_models[0] \n",
    "file_str = f\"{region}_{period}_over_{relative_to}\" # saves to file with suffix file_str\n",
    "#Generate subfigure\n",
    "fh = partial(get_per_period_metrics_df, all_metrics, period, relative_to) \n",
    "plot_models_and_metrics_plus(get_metrics_fh=fh, \n",
    "                            gt_id_list=task_ids, \n",
    "                            horizon_list=horizons, \n",
    "                            metric=metric, \n",
    "                            target_dates=target_dates, \n",
    "                            model_names=figure_models, \n",
    "                            file_str=file_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14: \n",
    "#### Per year improvement of each ABC model over its corresponding baseline across the contiguous U.S. and the years 2011-2020. The learned ABC benchmarks yield consistent improvements in mean RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure experiment parameters\n",
    "target_dates = \"std_paper\" \n",
    "period = \"yearly\" \n",
    "metric = \"rmse\" \n",
    "task_ids = us_gt_ids \n",
    "region = 'us'\n",
    "\n",
    "# RMSE improvement by season \n",
    "# Subfigure experiment parameters\n",
    "figure_models = [\n",
    "    'climatology', \n",
    "    'tuned_climpp',\n",
    "]\n",
    "relative_to = figure_models[0] \n",
    "file_str = f\"{region}_{period}_over_{relative_to}\" # saves to file with suffix file_str\n",
    "#Generate subfigure\n",
    "fh = partial(get_per_period_metrics_df, all_metrics, period, relative_to) \n",
    "plot_models_and_metrics_plus(get_metrics_fh=fh, \n",
    "                            gt_id_list=task_ids, \n",
    "                            horizon_list=horizons, \n",
    "                            metric=metric, \n",
    "                            target_dates=target_dates, \n",
    "                            model_names=figure_models, \n",
    "                            file_str=file_str)\n",
    "\n",
    "# RMSE improvement by season \n",
    "# Subfigure experiment parameters\n",
    "figure_models = [\n",
    "    'deb_cfsv2', \n",
    "    'tuned_cfsv2pp',\n",
    "]\n",
    "relative_to = figure_models[0] \n",
    "file_str = f\"{region}_{period}_over_{relative_to}\" # saves to file with suffix file_str\n",
    "#Generate subfigure\n",
    "fh = partial(get_per_period_metrics_df, all_metrics, period, relative_to) \n",
    "plot_models_and_metrics_plus(get_metrics_fh=fh, \n",
    "                            gt_id_list=task_ids, \n",
    "                            horizon_list=horizons, \n",
    "                            metric=metric, \n",
    "                            target_dates=target_dates, \n",
    "                            model_names=figure_models, \n",
    "                            file_str=file_str)\n",
    "\n",
    "# RMSE improvement by season \n",
    "# Subfigure experiment parameters\n",
    "figure_models = [\n",
    "    'persistence', \n",
    "    'perpp_cfsv2',\n",
    "]\n",
    "relative_to = figure_models[0] \n",
    "file_str = f\"{region}_{period}_over_{relative_to}\" # saves to file with suffix file_str\n",
    "#Generate subfigure\n",
    "fh = partial(get_per_period_metrics_df, all_metrics, period, relative_to) \n",
    "plot_models_and_metrics_plus(get_metrics_fh=fh, \n",
    "                            gt_id_list=task_ids, \n",
    "                            horizon_list=horizons, \n",
    "                            metric=metric, \n",
    "                            target_dates=target_dates, \n",
    "                            model_names=figure_models, \n",
    "                            file_str=file_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.2 Yearly Percentage Improvement over Mean Debiased CFSv2 RMSE\n",
    "## Table 3: \n",
    "#### Percentage improvement over mean debiased CFSv2 RMSE when forecasting temperature in the contiguous U.S. The best performing models within each class of models are shown in bold, while the best performing models overall are shown in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dates = \"std_paper\" \n",
    "period = \"yearly\" \n",
    "figure_metrics = [\"rmse\"]\n",
    "table_models = main_experiment_models\n",
    "relative_to = 'deb_cfsv2'\n",
    "dropna = True  \n",
    "task_ids = [g for g in us_gt_ids if \"tmp2m\" in g] \n",
    "task_horizons = horizons\n",
    "region = 'us' \n",
    "include_overall = True \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "End experiment parameters \n",
    "\"\"\"\n",
    "for metric, horizon in product(figure_metrics, task_horizons):\n",
    "    \n",
    "    if metric is \"skill\":\n",
    "        table_models = [m for m in table_models if m is not \"climatology\"]\n",
    "        relative_to = None\n",
    "    if (metric is 'rmse' and relative_to is not None) or (metric is not 'rmse' and relative_to is None):\n",
    "        highlight_func = highlight_max\n",
    "        bold_func = bold_max\n",
    "    else:\n",
    "        highlight_func = highlight_min\n",
    "        bold_func = bold_min\n",
    "      \n",
    "    \"\"\"\n",
    "    Display metric table\n",
    "    \"\"\"\n",
    "#     display(Markdown(f\"##### {metric} -- {target_dates}\"))\n",
    "    # Get set of tasks \n",
    "    tasks = [f\"{gt_id}_{horizon}\" for (gt_id, horizon) in product(task_ids, task_horizons)]\n",
    "    \n",
    "\n",
    "    # Generate metrics dataframe for each task\n",
    "    for i, gt_id in enumerate(task_ids):\n",
    "        task = f\"{gt_id}_{horizon}\"\n",
    "        metrics_sub = None\n",
    "#         printf(f\"Processing {task}\")\n",
    "        # Read in metrics and reset experiment_models based on avaliable metrics\n",
    "        m_sub, experiment_models = get_per_period_metrics_df(\n",
    "            all_metrics, period=period, gt_id=gt_id, horizon=horizon,\n",
    "            metric=metric, target_dates=target_dates, \n",
    "            relative_to=relative_to,\n",
    "            model_names=table_models, include_overall=include_overall, dropna=dropna)\n",
    "\n",
    "       # Create metrics dataframe template\n",
    "        if period == \"overall\":\n",
    "            index = pd.Index([task], name=\"task\")            \n",
    "        else:\n",
    "            index = pd.MultiIndex.from_product(\n",
    "                [[task], m_sub.index], \n",
    "                names=('task', 'period'))   \n",
    "        # Need to form task-by-task, since some tasks are missing target dates, so index differs\n",
    "        if metrics_sub is None:\n",
    "            metrics_sub = pd.DataFrame(index=index, columns=experiment_models)           \n",
    "        else:\n",
    "            metrics_sub = pd.concat([metrics_sub, pd.DataFrame(index=index, columns=experiment_models)])\n",
    "        if period == \"overall\":\n",
    "            metrics_sub.loc[task, :] = m_sub\n",
    "        else:\n",
    "            metrics_sub[metrics_sub.index.get_level_values(\"task\") == task] = m_sub.values\n",
    "\n",
    "    # maintain ordered list of model names\n",
    "    metrics_sub = metrics_sub[table_models]\n",
    "\n",
    "    if metric is \"skill\":\n",
    "        metrics_sub = metrics_sub.multiply(100)\n",
    "\n",
    "    if period is 'overall':\n",
    "        metrics_sub=metrics_sub.rename(us_tasks, axis=0).T\n",
    "        metrics_sub['model_type'] = [all_model_types[m] for m in metrics_sub.index]\n",
    "        metrics_sub = metrics_sub.rename(all_model_names, axis=0).reset_index().set_index(['model_type', 'index'])\n",
    "        metrics_sub.columns = metrics_sub.columns.get_level_values(0)\n",
    "#         display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "    else:\n",
    "        metrics_sub = metrics_sub.reindex([m for m in table_models if m in metrics_sub.columns], axis=1).T\n",
    "#         display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "\n",
    "    #save dataframe in latex table format\n",
    "    table_to_tex(metrics_sub.astype(float), tables_dir, f\"table_{region}_{horizon}_{period}_{metric}_over_{relative_to}_{target_dates}\", precision=2)\n",
    "#     printf(f\"Table saved in {os.path.join(tables_dir, f'table_{region}_{horizon}_{period}_{metric}_over_{relative_to}_{target_dates}.tex')}\\n\")\n",
    "\n",
    "    \"\"\"\n",
    "    Display metric table with standard error\n",
    "    \"\"\"\n",
    "    # Display and save table with standard error\n",
    "    display(Markdown(f\"##### {metric} +/- SE -- {target_dates}\"))         \n",
    "    np.random.seed(123)\n",
    "    # Read in metrics and reset experiment_models based on avaliable metrics\n",
    "    m_sub, experiment_models = get_per_period_metrics_df(all_metrics, \n",
    "                                                         period=\"individual\", \n",
    "                                                         gt_id=gt_id, \n",
    "                                                         horizon=horizon,\n",
    "                                                         metric=metric, \n",
    "                                                         target_dates=target_dates, \n",
    "                                                         relative_to=relative_to,\n",
    "                                                         model_names=table_models, \n",
    "                                                         include_overall=include_overall, \n",
    "                                                         dropna=dropna)\n",
    "    m_sub_se_or = pd.DataFrame(index=experiment_models, columns = metrics_sub.columns)\n",
    "\n",
    "    # Generate metrics dataframe for each task\n",
    "    for i, gt_id in enumerate(task_ids):\n",
    "        task = f\"{gt_id}_{horizon}\"\n",
    "        printf(f\"Processing {task}\")  \n",
    "        # Read in metrics and reset experiment_models based on avaliable metrics\n",
    "        m_sub, experiment_models = get_per_period_metrics_df(\n",
    "            all_metrics, period=\"individual\", gt_id=gt_id, horizon=horizon,\n",
    "            metric=metric, target_dates=target_dates, \n",
    "            relative_to=relative_to,\n",
    "            model_names=table_models, include_overall=include_overall, dropna=dropna)\n",
    "\n",
    "\n",
    "        for col in experiment_models:\n",
    "            mean_col = m_sub[col].mean()\n",
    "            std_col = m_sub[col].std()\n",
    "            n_col = m_sub[col].notna().sum()\n",
    "            t1 = [simulate_sample_mean(n_col, mean_col, std_col) for i in range(1000)]\n",
    "            summary1 = summarize(t1, digits=6)\n",
    "            m_sub_se_or.loc[col][task] = summary1['SE'][0]\n",
    "\n",
    "\n",
    "        # maintain ordered list of model names\n",
    "        m_sub_se = m_sub_se_or.T\n",
    "        m_sub_se = m_sub_se[table_models].T\n",
    "        if metric is \"skill\":\n",
    "            m_sub_se = m_sub_se.multiply(100) \n",
    "        if period is 'overall':\n",
    "            m_sub_se['model_type'] = [all_model_types[m] for m in m_sub_se.index]\n",
    "            m_sub_se = m_sub_se.rename(all_model_names, axis=0).reset_index().set_index(['model_type', 'index'])\n",
    "            m_sub_se.columns = m_sub_se.columns.get_level_values(0)\n",
    "\n",
    "    # display and save dataframe in latex table format\n",
    "    m_sub = metrics_sub.astype(float).round(2).astype(str).add(' $\\pm$ ').add(m_sub_se.astype(float).round(2).astype(str))\n",
    "    display(m_sub.style.apply(lambda x: metrics_sub.apply(highlight_func, axis=0), axis=None).apply(lambda x: metrics_sub.apply(bold_func, axis=0), axis=None))\n",
    "    filename_table = f\"table_{region}_{horizon}_{period}_{metric}_over_{relative_to}_{target_dates}_se\"\n",
    "    table_to_tex(m_sub, tables_dir, filename_table, precision=2)\n",
    "    printf(f\"Table saved in {os.path.join(tables_dir, f'{filename_table}.tex')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 4: \n",
    "#### Percentage improvement over mean debiased CFSv2 RMSE when forecasting precipitation in the contiguous U.S. The best performing models within each class of models are shown in bold, while the best performing models overall are shown in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Paper experiments; can be configured to generate metrics for any subset of models, averaged over\n",
    "a set of periods, for a given target data. Produces a table and saves to tex. \n",
    "\"\"\"\n",
    "target_dates = \"std_paper\" \n",
    "period = \"yearly\" \n",
    "figure_metrics = [\"rmse\"]\n",
    "table_models = main_experiment_models\n",
    "relative_to = 'deb_cfsv2'\n",
    "dropna = True  \n",
    "task_ids = [g for g in us_gt_ids if \"precip\" in g] \n",
    "task_horizons = horizons\n",
    "region = 'us' \n",
    "include_overall = True \n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "End experiment parameters \n",
    "\"\"\"\n",
    "for metric, horizon in product(figure_metrics, task_horizons):\n",
    "    \n",
    "    if metric is \"skill\":\n",
    "        table_models = [m for m in table_models if m is not \"climatology\"]\n",
    "        relative_to = None\n",
    "    if (metric is 'rmse' and relative_to is not None) or (metric is not 'rmse' and relative_to is None):\n",
    "        highlight_func = highlight_max\n",
    "        bold_func = bold_max\n",
    "    else:\n",
    "        highlight_func = highlight_min\n",
    "        bold_func = bold_min\n",
    "      \n",
    "    \"\"\"\n",
    "    Display metric table\n",
    "    \"\"\"\n",
    "#     display(Markdown(f\"##### {metric} -- {target_dates}\"))\n",
    "    # Get set of tasks \n",
    "    tasks = [f\"{gt_id}_{horizon}\" for (gt_id, horizon) in product(task_ids, task_horizons)]\n",
    "    \n",
    "\n",
    "    # Generate metrics dataframe for each task\n",
    "    for i, gt_id in enumerate(task_ids):\n",
    "        task = f\"{gt_id}_{horizon}\"\n",
    "        metrics_sub = None\n",
    "#         printf(f\"Processing {task}\")\n",
    "        # Read in metrics and reset experiment_models based on avaliable metrics\n",
    "        m_sub, experiment_models = get_per_period_metrics_df(\n",
    "            all_metrics, period=period, gt_id=gt_id, horizon=horizon,\n",
    "            metric=metric, target_dates=target_dates, \n",
    "            relative_to=relative_to,\n",
    "            model_names=table_models, include_overall=include_overall, dropna=dropna)\n",
    "\n",
    "       # Create metrics dataframe template\n",
    "        if period == \"overall\":\n",
    "            index = pd.Index([task], name=\"task\")            \n",
    "        else:\n",
    "            index = pd.MultiIndex.from_product(\n",
    "                [[task], m_sub.index], \n",
    "                names=('task', 'period'))   \n",
    "        # Need to form task-by-task, since some tasks are missing target dates, so index differs\n",
    "        if metrics_sub is None:\n",
    "            metrics_sub = pd.DataFrame(index=index, columns=experiment_models)           \n",
    "        else:\n",
    "            metrics_sub = pd.concat([metrics_sub, pd.DataFrame(index=index, columns=experiment_models)])\n",
    "        if period == \"overall\":\n",
    "            metrics_sub.loc[task, :] = m_sub\n",
    "        else:\n",
    "            metrics_sub[metrics_sub.index.get_level_values(\"task\") == task] = m_sub.values\n",
    "\n",
    "    # maintain ordered list of model names\n",
    "    metrics_sub = metrics_sub[table_models]\n",
    "\n",
    "    if metric is \"skill\":\n",
    "        metrics_sub = metrics_sub.multiply(100)\n",
    "\n",
    "    if period is 'overall':\n",
    "        metrics_sub=metrics_sub.rename(us_tasks, axis=0).T\n",
    "        metrics_sub['model_type'] = [all_model_types[m] for m in metrics_sub.index]\n",
    "        metrics_sub = metrics_sub.rename(all_model_names, axis=0).reset_index().set_index(['model_type', 'index'])\n",
    "        metrics_sub.columns = metrics_sub.columns.get_level_values(0)\n",
    "#         display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "    else:\n",
    "        metrics_sub = metrics_sub.reindex([m for m in table_models if m in metrics_sub.columns], axis=1).T\n",
    "#         display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "\n",
    "    #save dataframe in latex table format\n",
    "    table_to_tex(metrics_sub.astype(float), tables_dir, f\"table_{region}_{horizon}_{period}_{metric}_over_{relative_to}_{target_dates}\", precision=2)\n",
    "#     printf(f\"Table saved in {os.path.join(tables_dir, f'table_{region}_{horizon}_{period}_{metric}_over_{relative_to}_{target_dates}.tex')}\\n\")\n",
    "\n",
    "    \"\"\"\n",
    "    Display metric table with standard error\n",
    "    \"\"\"\n",
    "    # Display and save table with standard error\n",
    "    display(Markdown(f\"##### {metric} +/- SE -- {target_dates}\"))         \n",
    "    np.random.seed(123)\n",
    "    # Read in metrics and reset experiment_models based on avaliable metrics\n",
    "    m_sub, experiment_models = get_per_period_metrics_df(all_metrics, \n",
    "                                                         period=\"individual\", \n",
    "                                                         gt_id=gt_id, \n",
    "                                                         horizon=horizon,\n",
    "                                                         metric=metric, \n",
    "                                                         target_dates=target_dates, \n",
    "                                                         relative_to=relative_to,\n",
    "                                                         model_names=table_models, \n",
    "                                                         include_overall=include_overall, \n",
    "                                                         dropna=dropna)\n",
    "    m_sub_se_or = pd.DataFrame(index=experiment_models, columns = metrics_sub.columns)\n",
    "\n",
    "    # Generate metrics dataframe for each task\n",
    "    for i, gt_id in enumerate(task_ids):\n",
    "        task = f\"{gt_id}_{horizon}\"\n",
    "        printf(f\"Processing {task}\")  \n",
    "        # Read in metrics and reset experiment_models based on avaliable metrics\n",
    "        m_sub, experiment_models = get_per_period_metrics_df(\n",
    "            all_metrics, period=\"individual\", gt_id=gt_id, horizon=horizon,\n",
    "            metric=metric, target_dates=target_dates, \n",
    "            relative_to=relative_to,\n",
    "            model_names=table_models, include_overall=include_overall, dropna=dropna)\n",
    "\n",
    "\n",
    "        for col in experiment_models:\n",
    "            mean_col = m_sub[col].mean()\n",
    "            std_col = m_sub[col].std()\n",
    "            n_col = m_sub[col].notna().sum()\n",
    "            t1 = [simulate_sample_mean(n_col, mean_col, std_col) for i in range(1000)]\n",
    "            summary1 = summarize(t1, digits=6)\n",
    "            m_sub_se_or.loc[col][task] = summary1['SE'][0]\n",
    "\n",
    "\n",
    "        # maintain ordered list of model names\n",
    "        m_sub_se = m_sub_se_or.T\n",
    "        m_sub_se = m_sub_se[table_models].T\n",
    "        if metric is \"skill\":\n",
    "            m_sub_se = m_sub_se.multiply(100) \n",
    "        if period is 'overall':\n",
    "            m_sub_se['model_type'] = [all_model_types[m] for m in m_sub_se.index]\n",
    "            m_sub_se = m_sub_se.rename(all_model_names, axis=0).reset_index().set_index(['model_type', 'index'])\n",
    "            m_sub_se.columns = m_sub_se.columns.get_level_values(0)\n",
    "\n",
    "    # display and save dataframe in latex table format\n",
    "    m_sub = metrics_sub.astype(float).round(2).astype(str).add(' $\\pm$ ').add(m_sub_se.astype(float).round(2).astype(str))\n",
    "    display(m_sub.style.apply(lambda x: metrics_sub.apply(highlight_func, axis=0), axis=None).apply(lambda x: metrics_sub.apply(bold_func, axis=0), axis=None))\n",
    "    filename_table = f\"table_{region}_{horizon}_{period}_{metric}_over_{relative_to}_{target_dates}_se\"\n",
    "    table_to_tex(m_sub, tables_dir, filename_table, precision=2)\n",
    "    printf(f\"Table saved in {os.path.join(tables_dir, f'{filename_table}.tex')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.3 Yearly Average Skill\n",
    "## Table 5: \n",
    "#### Average percentage skill when forecasting temperature in the contiguous U.S. The best performing models within each group are shown in bold, while the best performing models overall are shown in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dates = \"std_paper\" \n",
    "period = \"yearly\" \n",
    "figure_metrics = [\"skill\"]\n",
    "table_models = main_experiment_models\n",
    "relative_to = 'deb_cfsv2'\n",
    "dropna = True  \n",
    "task_ids = [g for g in us_gt_ids if \"tmp2m\" in g] \n",
    "task_horizons = horizons\n",
    "region = 'us' \n",
    "include_overall = True \n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "End experiment parameters \n",
    "\"\"\"\n",
    "for metric, horizon in product(figure_metrics, task_horizons):\n",
    "    \n",
    "    if metric is \"skill\":\n",
    "        table_models = [m for m in table_models if m is not \"climatology\"]\n",
    "        relative_to = None\n",
    "    if (metric is 'rmse' and relative_to is not None) or (metric is not 'rmse' and relative_to is None):\n",
    "        highlight_func = highlight_max\n",
    "        bold_func = bold_max\n",
    "    else:\n",
    "        highlight_func = highlight_min\n",
    "        bold_func = bold_min\n",
    "      \n",
    "    \"\"\"\n",
    "    Display metric table\n",
    "    \"\"\"\n",
    "#     display(Markdown(f\"##### {metric} -- {target_dates}\"))\n",
    "    # Get set of tasks \n",
    "    tasks = [f\"{gt_id}_{horizon}\" for (gt_id, horizon) in product(task_ids, task_horizons)]\n",
    "    \n",
    "\n",
    "    # Generate metrics dataframe for each task\n",
    "    for i, gt_id in enumerate(task_ids):\n",
    "        task = f\"{gt_id}_{horizon}\"\n",
    "        metrics_sub = None\n",
    "#         printf(f\"Processing {task}\")\n",
    "        # Read in metrics and reset experiment_models based on avaliable metrics\n",
    "        m_sub, experiment_models = get_per_period_metrics_df(\n",
    "            all_metrics, period=period, gt_id=gt_id, horizon=horizon,\n",
    "            metric=metric, target_dates=target_dates, \n",
    "            relative_to=relative_to,\n",
    "            model_names=table_models, include_overall=include_overall, dropna=dropna)\n",
    "\n",
    "       # Create metrics dataframe template\n",
    "        if period == \"overall\":\n",
    "            index = pd.Index([task], name=\"task\")            \n",
    "        else:\n",
    "            index = pd.MultiIndex.from_product(\n",
    "                [[task], m_sub.index], \n",
    "                names=('task', 'period'))   \n",
    "        # Need to form task-by-task, since some tasks are missing target dates, so index differs\n",
    "        if metrics_sub is None:\n",
    "            metrics_sub = pd.DataFrame(index=index, columns=experiment_models)           \n",
    "        else:\n",
    "            metrics_sub = pd.concat([metrics_sub, pd.DataFrame(index=index, columns=experiment_models)])\n",
    "        if period == \"overall\":\n",
    "            metrics_sub.loc[task, :] = m_sub\n",
    "        else:\n",
    "            metrics_sub[metrics_sub.index.get_level_values(\"task\") == task] = m_sub.values\n",
    "\n",
    "    # maintain ordered list of model names\n",
    "    metrics_sub = metrics_sub[table_models]\n",
    "\n",
    "    if metric is \"skill\":\n",
    "        metrics_sub = metrics_sub.multiply(100)\n",
    "\n",
    "    if period is 'overall':\n",
    "        metrics_sub=metrics_sub.rename(us_tasks, axis=0).T\n",
    "        metrics_sub['model_type'] = [all_model_types[m] for m in metrics_sub.index]\n",
    "        metrics_sub = metrics_sub.rename(all_model_names, axis=0).reset_index().set_index(['model_type', 'index'])\n",
    "        metrics_sub.columns = metrics_sub.columns.get_level_values(0)\n",
    "#         display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "    else:\n",
    "        metrics_sub = metrics_sub.reindex([m for m in table_models if m in metrics_sub.columns], axis=1).T\n",
    "#         display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "\n",
    "    #save dataframe in latex table format\n",
    "    table_to_tex(metrics_sub.astype(float), tables_dir, f\"table_{region}_{horizon}_{period}_{metric}_over_{relative_to}_{target_dates}\", precision=2)\n",
    "#     printf(f\"Table saved in {os.path.join(tables_dir, f'table_{region}_{horizon}_{period}_{metric}_over_{relative_to}_{target_dates}.tex')}\\n\")\n",
    "\n",
    "    \"\"\"\n",
    "    Display metric table with standard error\n",
    "    \"\"\"\n",
    "    # Display and save table with standard error\n",
    "    display(Markdown(f\"##### {metric} +/- SE -- {target_dates}\"))         \n",
    "    np.random.seed(123)\n",
    "    # Read in metrics and reset experiment_models based on avaliable metrics\n",
    "    m_sub, experiment_models = get_per_period_metrics_df(all_metrics, \n",
    "                                                         period=\"individual\", \n",
    "                                                         gt_id=gt_id, \n",
    "                                                         horizon=horizon,\n",
    "                                                         metric=metric, \n",
    "                                                         target_dates=target_dates, \n",
    "                                                         relative_to=relative_to,\n",
    "                                                         model_names=table_models, \n",
    "                                                         include_overall=include_overall, \n",
    "                                                         dropna=dropna)\n",
    "    m_sub_se_or = pd.DataFrame(index=experiment_models, columns = metrics_sub.columns)\n",
    "\n",
    "    # Generate metrics dataframe for each task\n",
    "    for i, gt_id in enumerate(task_ids):\n",
    "        task = f\"{gt_id}_{horizon}\"\n",
    "        printf(f\"Processing {task}\")  \n",
    "        # Read in metrics and reset experiment_models based on avaliable metrics\n",
    "        m_sub, experiment_models = get_per_period_metrics_df(\n",
    "            all_metrics, period=\"individual\", gt_id=gt_id, horizon=horizon,\n",
    "            metric=metric, target_dates=target_dates, \n",
    "            relative_to=relative_to,\n",
    "            model_names=table_models, include_overall=include_overall, dropna=dropna)\n",
    "\n",
    "\n",
    "        for col in experiment_models:\n",
    "            mean_col = m_sub[col].mean()\n",
    "            std_col = m_sub[col].std()\n",
    "            n_col = m_sub[col].notna().sum()\n",
    "            t1 = [simulate_sample_mean(n_col, mean_col, std_col) for i in range(1000)]\n",
    "            summary1 = summarize(t1, digits=6)\n",
    "            m_sub_se_or.loc[col][task] = summary1['SE'][0]\n",
    "\n",
    "\n",
    "        # maintain ordered list of model names\n",
    "        m_sub_se = m_sub_se_or.T\n",
    "        m_sub_se = m_sub_se[table_models].T\n",
    "        if metric is \"skill\":\n",
    "            m_sub_se = m_sub_se.multiply(100) \n",
    "        if period is 'overall':\n",
    "            m_sub_se['model_type'] = [all_model_types[m] for m in m_sub_se.index]\n",
    "            m_sub_se = m_sub_se.rename(all_model_names, axis=0).reset_index().set_index(['model_type', 'index'])\n",
    "            m_sub_se.columns = m_sub_se.columns.get_level_values(0)\n",
    "\n",
    "    # display and save dataframe in latex table format\n",
    "    m_sub = metrics_sub.astype(float).round(2).astype(str).add(' $\\pm$ ').add(m_sub_se.astype(float).round(2).astype(str))\n",
    "    display(m_sub.style.apply(lambda x: metrics_sub.apply(highlight_func, axis=0), axis=None).apply(lambda x: metrics_sub.apply(bold_func, axis=0), axis=None))\n",
    "    filename_table = f\"table_{region}_{horizon}_{period}_{metric}_over_{relative_to}_{target_dates}_se\"\n",
    "    table_to_tex(m_sub, tables_dir, filename_table, precision=2)\n",
    "    printf(f\"Table saved in {os.path.join(tables_dir, f'{filename_table}.tex')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 6: \n",
    "#### Average percentage skill when forecasting precipitation in the contiguous U.S. The best performing models within each group are shown in bold, while the best performing models overall are shown in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dates = \"std_paper\" \n",
    "period = \"yearly\" \n",
    "figure_metrics = [\"skill\"]\n",
    "table_models = main_experiment_models\n",
    "relative_to = 'deb_cfsv2'\n",
    "dropna = True  \n",
    "task_ids = [g for g in us_gt_ids if \"precip\" in g] \n",
    "task_horizons = horizons\n",
    "region = 'us' \n",
    "include_overall = True \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "End experiment parameters \n",
    "\"\"\"\n",
    "for metric, horizon in product(figure_metrics, task_horizons):\n",
    "    \n",
    "    if metric is \"skill\":\n",
    "        table_models = [m for m in table_models if m is not \"climatology\"]\n",
    "        relative_to = None\n",
    "    if (metric is 'rmse' and relative_to is not None) or (metric is not 'rmse' and relative_to is None):\n",
    "        highlight_func = highlight_max\n",
    "        bold_func = bold_max\n",
    "    else:\n",
    "        highlight_func = highlight_min\n",
    "        bold_func = bold_min\n",
    "      \n",
    "    \"\"\"\n",
    "    Display metric table\n",
    "    \"\"\"\n",
    "#     display(Markdown(f\"##### {metric} -- {target_dates}\"))\n",
    "    # Get set of tasks \n",
    "    tasks = [f\"{gt_id}_{horizon}\" for (gt_id, horizon) in product(task_ids, task_horizons)]\n",
    "    \n",
    "\n",
    "    # Generate metrics dataframe for each task\n",
    "    for i, gt_id in enumerate(task_ids):\n",
    "        task = f\"{gt_id}_{horizon}\"\n",
    "        metrics_sub = None\n",
    "#         printf(f\"Processing {task}\")\n",
    "        # Read in metrics and reset experiment_models based on avaliable metrics\n",
    "        m_sub, experiment_models = get_per_period_metrics_df(\n",
    "            all_metrics, period=period, gt_id=gt_id, horizon=horizon,\n",
    "            metric=metric, target_dates=target_dates, \n",
    "            relative_to=relative_to,\n",
    "            model_names=table_models, include_overall=include_overall, dropna=dropna)\n",
    "\n",
    "       # Create metrics dataframe template\n",
    "        if period == \"overall\":\n",
    "            index = pd.Index([task], name=\"task\")            \n",
    "        else:\n",
    "            index = pd.MultiIndex.from_product(\n",
    "                [[task], m_sub.index], \n",
    "                names=('task', 'period'))   \n",
    "        # Need to form task-by-task, since some tasks are missing target dates, so index differs\n",
    "        if metrics_sub is None:\n",
    "            metrics_sub = pd.DataFrame(index=index, columns=experiment_models)           \n",
    "        else:\n",
    "            metrics_sub = pd.concat([metrics_sub, pd.DataFrame(index=index, columns=experiment_models)])\n",
    "        if period == \"overall\":\n",
    "            metrics_sub.loc[task, :] = m_sub\n",
    "        else:\n",
    "            metrics_sub[metrics_sub.index.get_level_values(\"task\") == task] = m_sub.values\n",
    "\n",
    "    # maintain ordered list of model names\n",
    "    metrics_sub = metrics_sub[table_models]\n",
    "\n",
    "    if metric is \"skill\":\n",
    "        metrics_sub = metrics_sub.multiply(100)\n",
    "\n",
    "    if period is 'overall':\n",
    "        metrics_sub=metrics_sub.rename(us_tasks, axis=0).T\n",
    "        metrics_sub['model_type'] = [all_model_types[m] for m in metrics_sub.index]\n",
    "        metrics_sub = metrics_sub.rename(all_model_names, axis=0).reset_index().set_index(['model_type', 'index'])\n",
    "        metrics_sub.columns = metrics_sub.columns.get_level_values(0)\n",
    "#         display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "    else:\n",
    "        metrics_sub = metrics_sub.reindex([m for m in table_models if m in metrics_sub.columns], axis=1).T\n",
    "#         display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "\n",
    "    #save dataframe in latex table format\n",
    "    table_to_tex(metrics_sub.astype(float), tables_dir, f\"table_{region}_{horizon}_{period}_{metric}_over_{relative_to}_{target_dates}\", precision=2)\n",
    "#     printf(f\"Table saved in {os.path.join(tables_dir, f'table_{region}_{horizon}_{period}_{metric}_over_{relative_to}_{target_dates}.tex')}\\n\")\n",
    "\n",
    "    \"\"\"\n",
    "    Display metric table with standard error\n",
    "    \"\"\"\n",
    "    # Display and save table with standard error\n",
    "    display(Markdown(f\"##### {metric} +/- SE -- {target_dates}\"))         \n",
    "    np.random.seed(123)\n",
    "    # Read in metrics and reset experiment_models based on avaliable metrics\n",
    "    m_sub, experiment_models = get_per_period_metrics_df(all_metrics, \n",
    "                                                         period=\"individual\", \n",
    "                                                         gt_id=gt_id, \n",
    "                                                         horizon=horizon,\n",
    "                                                         metric=metric, \n",
    "                                                         target_dates=target_dates, \n",
    "                                                         relative_to=relative_to,\n",
    "                                                         model_names=table_models, \n",
    "                                                         include_overall=include_overall, \n",
    "                                                         dropna=dropna)\n",
    "    m_sub_se_or = pd.DataFrame(index=experiment_models, columns = metrics_sub.columns)\n",
    "\n",
    "    # Generate metrics dataframe for each task\n",
    "    for i, gt_id in enumerate(task_ids):\n",
    "        task = f\"{gt_id}_{horizon}\"\n",
    "        printf(f\"Processing {task}\")  \n",
    "        # Read in metrics and reset experiment_models based on avaliable metrics\n",
    "        m_sub, experiment_models = get_per_period_metrics_df(\n",
    "            all_metrics, period=\"individual\", gt_id=gt_id, horizon=horizon,\n",
    "            metric=metric, target_dates=target_dates, \n",
    "            relative_to=relative_to,\n",
    "            model_names=table_models, include_overall=include_overall, dropna=dropna)\n",
    "\n",
    "\n",
    "        for col in experiment_models:\n",
    "            mean_col = m_sub[col].mean()\n",
    "            std_col = m_sub[col].std()\n",
    "            n_col = m_sub[col].notna().sum()\n",
    "            t1 = [simulate_sample_mean(n_col, mean_col, std_col) for i in range(1000)]\n",
    "            summary1 = summarize(t1, digits=6)\n",
    "            m_sub_se_or.loc[col][task] = summary1['SE'][0]\n",
    "\n",
    "\n",
    "        # maintain ordered list of model names\n",
    "        m_sub_se = m_sub_se_or.T\n",
    "        m_sub_se = m_sub_se[table_models].T\n",
    "        if metric is \"skill\":\n",
    "            m_sub_se = m_sub_se.multiply(100) \n",
    "        if period is 'overall':\n",
    "            m_sub_se['model_type'] = [all_model_types[m] for m in m_sub_se.index]\n",
    "            m_sub_se = m_sub_se.rename(all_model_names, axis=0).reset_index().set_index(['model_type', 'index'])\n",
    "            m_sub_se.columns = m_sub_se.columns.get_level_values(0)\n",
    "\n",
    "    # display and save dataframe in latex table format\n",
    "    m_sub = metrics_sub.astype(float).round(2).astype(str).add(' $\\pm$ ').add(m_sub_se.astype(float).round(2).astype(str))\n",
    "    display(m_sub.style.apply(lambda x: metrics_sub.apply(highlight_func, axis=0), axis=None).apply(lambda x: metrics_sub.apply(bold_func, axis=0), axis=None))\n",
    "    filename_table = f\"table_{region}_{horizon}_{period}_{metric}_over_{relative_to}_{target_dates}_se\"\n",
    "    table_to_tex(m_sub, tables_dir, filename_table, precision=2)\n",
    "    printf(f\"Table saved in {os.path.join(tables_dir, f'{filename_table}.tex')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.4 Spatial Improvement over Mean Debiased CFSv2 RMSE\n",
    "## Figure 15: \n",
    "#### Percentage improvement over mean debiased CFSv2 RMSE when forecasting temperature in the contiguous U.S. over 2011-2020. White grid points indicate negative or 0% improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate figure metrics\n",
    "figure_gt_ids = [g for g in us_gt_ids if 'tmp2m' in g]\n",
    "figure_horizons = horizons\n",
    "figure_target_dates = 'std_paper'\n",
    "figure_metrics = ['lat_lon_rmse']\n",
    "figure_models = [\n",
    "    #relative_to\n",
    "    'deb_cfsv2',\n",
    "    # Row 1\n",
    "    'tuned_cfsv2pp', \n",
    "    'tuned_climpp', \n",
    "    'perpp_cfsv2', \n",
    "    'online_learning',                                                     \n",
    "    # Row 2\n",
    "    'autoknn', \n",
    "    'climatology', \n",
    "    'persistence',  \n",
    "    'linear_ensemble',\n",
    "    # Row 3\n",
    "    'informer', \n",
    "    'tuned_localboosting', \n",
    "    'multillr', \n",
    "    'nbeats',                                                                          \n",
    "    # Row 4\n",
    "    'prophet', \n",
    "    'tuned_salient2', \n",
    "]\n",
    "\n",
    "metric_dfs_rda = {}\n",
    "for gt_id, horizon in product(figure_gt_ids, figure_horizons):\n",
    "    task = f\"{gt_id}_{horizon}\"\n",
    "    display(Markdown(f\"#### Getting metrics for {gt_id} {horizon}\"))\n",
    "    metric_dfs_rda[task] = get_models_metric_lat_lon(gt_id=gt_id, horizon=horizon, \n",
    "                                                     target_dates=figure_target_dates, \n",
    "                                                     metrics = figure_metrics, \n",
    "                                                     model_names=figure_models)\n",
    "\n",
    "# Set figure parameter\n",
    "figure_gt_ids = [g for g in us_gt_ids if 'tmp2m' in g]\n",
    "figure_horizons = horizons\n",
    "figure_metric = 'lat_lon_rmse'\n",
    "figure_relative_to = 'deb_cfsv2'\n",
    "figure_mean_metric_df = None\n",
    "figure_cb_minmax = (0, 20)\n",
    "figure_cb_skip = 4\n",
    "figure_source_data = False\n",
    "figure_show = True\n",
    "\n",
    "\n",
    "\n",
    "figure_model_names = figure_models \n",
    "display(Markdown(f'#### Models: {\", \".join(figure_model_names)}'))\n",
    "for figure_gt_id, figure_horizon in product(figure_gt_ids, figure_horizons):\n",
    "    display(Markdown(f\"#### {figure_gt_id} {figure_horizon}\"))\n",
    "    plot_metric_maps_task_ds(metric_dfs_rda, model_names=figure_model_names,\n",
    "                         gt_id=figure_gt_id,\n",
    "                         horizon=figure_horizon,\n",
    "                         metric=figure_metric,\n",
    "                         target_dates=figure_target_dates,\n",
    "                         relative_to=figure_relative_to,\n",
    "                         mean_metric_df=figure_mean_metric_df,\n",
    "                         show=figure_show, \n",
    "                         scale_type='linear',\n",
    "                         CB_colors_customized=[\"white\", \"green\", \"darkgreen\"],\n",
    "                         CB_minmax = figure_cb_minmax,\n",
    "                         CB_skip = figure_cb_skip,\n",
    "                         source_data = figure_source_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 16: \n",
    "#### Percentage improvement over mean debiased CFSv2 RMSE when forecasting precipitation in the contiguous U.S. over 2011-2020. White grid points indicate negative or 0% improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate figure metrics\n",
    "figure_gt_ids = [g for g in us_gt_ids if 'precip' in g]\n",
    "figure_horizons = horizons\n",
    "figure_target_dates = 'std_paper'\n",
    "figure_metrics = ['lat_lon_rmse']\n",
    "figure_models = [\n",
    "    #relative_to\n",
    "    'deb_cfsv2',\n",
    "    # Row 1\n",
    "    'tuned_cfsv2pp', \n",
    "    'tuned_climpp', \n",
    "    'perpp_cfsv2', \n",
    "    'online_learning',                                                     \n",
    "    # Row 2\n",
    "    'autoknn', \n",
    "    'climatology', \n",
    "    'persistence',  \n",
    "    'linear_ensemble',\n",
    "    # Row 3\n",
    "    'informer', \n",
    "    'tuned_localboosting', \n",
    "    'multillr', \n",
    "    'nbeats',                                                                          \n",
    "    # Row 4\n",
    "    'prophet', \n",
    "    'tuned_salient2', \n",
    "]\n",
    "\n",
    "metric_dfs_rda = {}\n",
    "for gt_id, horizon in product(figure_gt_ids, figure_horizons):\n",
    "    task = f\"{gt_id}_{horizon}\"\n",
    "    display(Markdown(f\"#### Getting metrics for {gt_id} {horizon}\"))\n",
    "    metric_dfs_rda[task] = get_models_metric_lat_lon(gt_id=gt_id, horizon=horizon, \n",
    "                                                     target_dates=figure_target_dates, \n",
    "                                                     metrics = figure_metrics, \n",
    "                                                     model_names=figure_models)\n",
    "# Set figure parameter\n",
    "figure_gt_ids = [g for g in us_gt_ids if 'precip' in g]\n",
    "figure_horizons = horizons\n",
    "figure_metric = 'lat_lon_rmse'\n",
    "figure_relative_to = 'deb_cfsv2'\n",
    "figure_mean_metric_df = None\n",
    "figure_cb_minmax = (0, 20)\n",
    "figure_cb_skip = 4\n",
    "figure_source_data = False\n",
    "figure_show = True\n",
    "\n",
    "\n",
    "\n",
    "figure_model_names = figure_models \n",
    "display(Markdown(f'#### Models: {\", \".join(figure_model_names)}'))\n",
    "for figure_gt_id, figure_horizon in product(figure_gt_ids, figure_horizons):\n",
    "    display(Markdown(f\"#### {figure_gt_id} {figure_horizon}\"))\n",
    "    plot_metric_maps_task_ds(metric_dfs_rda, model_names=figure_model_names,\n",
    "                         gt_id=figure_gt_id,\n",
    "                         horizon=figure_horizon,\n",
    "                         metric=figure_metric,\n",
    "                         target_dates=figure_target_dates,\n",
    "                         relative_to=figure_relative_to,\n",
    "                         mean_metric_df=figure_mean_metric_df,\n",
    "                         show=figure_show, \n",
    "                         scale_type='linear',\n",
    "                         CB_colors_customized=[\"white\", \"green\", \"darkgreen\"],\n",
    "                         CB_minmax = figure_cb_minmax,\n",
    "                         CB_skip = figure_cb_skip,\n",
    "                         source_data = figure_source_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.5 Spatial Bias Maps\n",
    "## Figure 17: \n",
    "#### Model bias when forecasting temperature in the contiguous U.S. over 2011-2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate figure metrics\n",
    "figure_gt_ids = [g for g in us_gt_ids if 'tmp2m' in g]\n",
    "figure_horizons = horizons\n",
    "figure_target_dates = 'std_paper'\n",
    "figure_metrics = ['lat_lon_error']\n",
    "figure_models = [\n",
    "    # Row 1\n",
    "    'tuned_cfsv2pp', \n",
    "    'tuned_climpp', \n",
    "    'perpp_cfsv2', \n",
    "    'online_learning',                                                     \n",
    "    # Row 2\n",
    "    'deb_cfsv2',\n",
    "    'climatology', \n",
    "    'persistence',  \n",
    "    'linear_ensemble',\n",
    "    # Row 3\n",
    "    'autoknn', \n",
    "    'informer', \n",
    "    'tuned_localboosting', \n",
    "    'multillr',                                                                          \n",
    "    # Row 4\n",
    "    'nbeats', \n",
    "    'prophet', \n",
    "    'tuned_salient2', \n",
    "]\n",
    "\n",
    "metric_dfs_rda = {}\n",
    "for gt_id, horizon in product(figure_gt_ids, figure_horizons):\n",
    "    task = f\"{gt_id}_{horizon}\"\n",
    "    display(Markdown(f\"#### Getting metrics for {gt_id} {horizon}\"))\n",
    "    metric_dfs_rda[task] = get_models_metric_lat_lon(gt_id=gt_id, horizon=horizon, \n",
    "                                                     target_dates=figure_target_dates, \n",
    "                                                     metrics = figure_metrics, \n",
    "                                                     model_names=figure_models)\n",
    "\n",
    "# Set figure parameter\n",
    "figure_gt_ids = [g for g in us_gt_ids if 'tmp2m' in g]\n",
    "figure_horizons = horizons\n",
    "figure_metric = 'lat_lon_error'\n",
    "figure_relative_to = None\n",
    "figure_mean_metric_df = None\n",
    "figure_cb_minmax = (-5, 5)\n",
    "figure_cb_skip = 1\n",
    "figure_source_data = False\n",
    "figure_show = True\n",
    "\n",
    "\n",
    "\n",
    "figure_model_names = figure_models \n",
    "display(Markdown(f'#### Models: {\", \".join(figure_model_names)}'))\n",
    "for figure_gt_id, figure_horizon in product(figure_gt_ids, figure_horizons):\n",
    "    display(Markdown(f\"#### {figure_gt_id} {figure_horizon}\"))\n",
    "    plot_metric_maps_task_ds(metric_dfs_rda, model_names=figure_model_names,\n",
    "                         gt_id=figure_gt_id,\n",
    "                         horizon=figure_horizon,\n",
    "                         metric=figure_metric,\n",
    "                         target_dates=figure_target_dates,\n",
    "                         relative_to=figure_relative_to,\n",
    "                         mean_metric_df=figure_mean_metric_df,\n",
    "                         show=figure_show, \n",
    "                         scale_type='linear',\n",
    "                         CB_colors_customized=[\"darkblue\", \"blue\", \"white\", \"red\", \"darkred\"],\n",
    "                         CB_minmax = figure_cb_minmax,\n",
    "                         CB_skip = figure_cb_skip,\n",
    "                         source_data = figure_source_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 18: \n",
    "#### Model bias when forecasting precipitation in the contiguous U.S. over 2011-2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate figure metrics\n",
    "figure_gt_ids = [g for g in us_gt_ids if 'precip' in g]\n",
    "figure_horizons = horizons\n",
    "figure_target_dates = 'std_paper'\n",
    "figure_metrics = ['lat_lon_error']\n",
    "figure_models = [\n",
    "    # Row 1\n",
    "    'tuned_cfsv2pp', \n",
    "    'tuned_climpp', \n",
    "    'perpp_cfsv2', \n",
    "    'online_learning',                                                     \n",
    "    # Row 2\n",
    "    'deb_cfsv2',\n",
    "    'climatology', \n",
    "    'persistence',  \n",
    "    'linear_ensemble',\n",
    "    # Row 3\n",
    "    'autoknn', \n",
    "    'informer', \n",
    "    'tuned_localboosting', \n",
    "    'multillr',                                                                          \n",
    "    # Row 4\n",
    "    'nbeats', \n",
    "    'prophet', \n",
    "    'tuned_salient2', \n",
    "]\n",
    "\n",
    "metric_dfs_rda = {}\n",
    "for gt_id, horizon in product(figure_gt_ids, figure_horizons):\n",
    "    task = f\"{gt_id}_{horizon}\"\n",
    "    display(Markdown(f\"#### Getting metrics for {gt_id} {horizon}\"))\n",
    "    metric_dfs_rda[task] = get_models_metric_lat_lon(gt_id=gt_id, horizon=horizon, \n",
    "                                                     target_dates=figure_target_dates, \n",
    "                                                     metrics = figure_metrics, \n",
    "                                                     model_names=figure_models)\n",
    "\n",
    "# Set figure parameter\n",
    "figure_gt_ids = [g for g in us_gt_ids if 'precip' in g]\n",
    "figure_horizons = horizons\n",
    "figure_metric = 'lat_lon_error'\n",
    "figure_relative_to = None\n",
    "figure_mean_metric_df = None\n",
    "figure_cb_minmax = (-15, 15)\n",
    "figure_cb_skip = 5\n",
    "figure_source_data = False\n",
    "figure_show = True\n",
    "\n",
    "\n",
    "\n",
    "figure_model_names = figure_models \n",
    "display(Markdown(f'#### Models: {\", \".join(figure_model_names)}'))\n",
    "for figure_gt_id, figure_horizon in product(figure_gt_ids, figure_horizons):\n",
    "    display(Markdown(f\"#### {figure_gt_id} {figure_horizon}\"))\n",
    "    plot_metric_maps_task_ds(metric_dfs_rda, model_names=figure_model_names,\n",
    "                         gt_id=figure_gt_id,\n",
    "                         horizon=figure_horizon,\n",
    "                         metric=figure_metric,\n",
    "                         target_dates=figure_target_dates,\n",
    "                         relative_to=figure_relative_to,\n",
    "                         mean_metric_df=figure_mean_metric_df,\n",
    "                         show=figure_show, \n",
    "                         scale_type='linear',\n",
    "                         CB_colors_customized=[\"orangered\", \"orange\", \"white\", \"green\", \"darkgreen\"],\n",
    "                         CB_minmax = figure_cb_minmax,\n",
    "                         CB_skip = figure_cb_skip,\n",
    "                         source_data = figure_source_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.6 GraphCast Comparison Details\n",
    "## Table 7: \n",
    "#### Average percentage skill and percentage improvement over mean debiased CFSv2 RMSE across 20182020 in the contiguous U.S. along with a 95% bootstrap confidence interval. The best performing model overall is shown in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dates = \"std_paper_graphcast\" \n",
    "period = \"overall\" # <- must be overall to merge with rodeo dataframe\n",
    "figure_metrics = [\"rmse\", \"skill\"] \n",
    "table_models = graphcast_experiment_models \n",
    "relative_to = 'deb_cfsv2' # compute value relative to climatology value: 1 - metric(model)/metric(climatology) \n",
    "dropna = True # if true, compute average metrics only on dates where predictions have all values \n",
    "task_ids = us_gt_ids # contest_gt_ids (for contest), us_gt_ids (for us), gt_ids (for all)\n",
    "horizons = ['34w']\n",
    "region = 'us' #either us, east or contest\n",
    "include_overall = True # include overall row in the dataframe\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "End experiment parameters \n",
    "\"\"\"\n",
    "\n",
    "for metric in figure_metrics:\n",
    "    if metric is \"skill\":\n",
    "        table_models = [m for m in table_models if m is not \"climatology\"]\n",
    "        relative_to = None\n",
    "    if (metric is 'rmse' and relative_to is not None) or (metric is not 'rmse' and relative_to is None):\n",
    "        highlight_func = highlight_max\n",
    "        bold_func = bold_max\n",
    "    else:\n",
    "        highlight_func = highlight_min\n",
    "        bold_func = bold_min\n",
    "\n",
    "    \"\"\"\n",
    "    Display metric table\n",
    "    \"\"\"\n",
    "#     display(Markdown(f\"##### {metric} -- {target_dates}\"))\n",
    "    # Get set of tasks \n",
    "    tasks = [f\"{gt_id}_{horizon}\" for (gt_id, horizon) in product(task_ids, horizons)]\n",
    "    metrics_sub = None\n",
    "\n",
    "    # Generate metrics dataframe for each task\n",
    "    for i, (gt_id, horizon) in enumerate(product(task_ids, horizons)):\n",
    "        task = f\"{gt_id}_{horizon}\"\n",
    "#         printf(f\"Processing {task}\")\n",
    "        # Read in metrics and reset experiment_models based on avaliable metrics\n",
    "        m_sub, experiment_models = get_per_period_metrics_df(\n",
    "            all_metrics, period=period, gt_id=gt_id, horizon=horizon,\n",
    "            metric=metric, target_dates=target_dates, \n",
    "            relative_to=relative_to,\n",
    "            model_names=table_models, include_overall=include_overall, dropna=dropna)\n",
    "       \n",
    "        # Create metrics dataframe template\n",
    "        if period == \"overall\":\n",
    "            index = pd.Index([task], name=\"task\")            \n",
    "        else:\n",
    "            index = pd.MultiIndex.from_product(\n",
    "                [[task], m_sub.index], \n",
    "                names=('task', 'period'))   \n",
    "        # Need to form task-by-task, since some tasks are missing target dates, so index differs\n",
    "        if metrics_sub is None:\n",
    "            metrics_sub = pd.DataFrame(index=index, columns=experiment_models)           \n",
    "        else:\n",
    "            metrics_sub = pd.concat([metrics_sub, pd.DataFrame(index=index, columns=experiment_models)])\n",
    "        if period == \"overall\":\n",
    "            metrics_sub.loc[task, :] = m_sub\n",
    "        else:\n",
    "            metrics_sub[metrics_sub.index.get_level_values(\"task\") == task] = m_sub.values\n",
    "\n",
    "    # maintain ordered list of model names\n",
    "    metrics_sub = metrics_sub[table_models]\n",
    "    if metric is \"skill\":\n",
    "        metrics_sub = metrics_sub.multiply(100)\n",
    "\n",
    "    if period is 'overall':\n",
    "        metrics_sub=metrics_sub.rename(us_tasks, axis=0).T\n",
    "        metrics_sub['model_type'] = [all_model_types[m] for m in metrics_sub.index]\n",
    "        metrics_sub = metrics_sub.rename(all_model_names, axis=0).reset_index().set_index(['model_type', 'index'])\n",
    "        metrics_sub.columns = metrics_sub.columns.get_level_values(0)\n",
    "    #         display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "        #for group in ['Baselines', 'ABC', 'Learning', 'Ensembles']:\n",
    "            #display(metrics_sub.loc[group].style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "    else:\n",
    "        metrics_sub = metrics_sub.reindex([m for m in table_models if m in metrics_sub.columns], axis=1).T\n",
    "    #         display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "\n",
    "    #save dataframe in latex table format\n",
    "    table_to_tex(metrics_sub.astype(float), tables_dir, f\"table_{region}_{period}_{metric}_over_{relative_to}_{target_dates}\", precision=2)\n",
    "#     printf(f\"Table saved in {os.path.join(tables_dir, f'table_{region}_{period}_{metric}_over_{relative_to}_{target_dates}.tex')}\\n\")\n",
    "\n",
    "    \"\"\"\n",
    "    Display metric table with standard error\n",
    "    \"\"\"\n",
    "    # Display and save table with standard error\n",
    "    display(Markdown(f\"##### {metric} +/- SE -- {target_dates}\"))         \n",
    "    np.random.seed(123)\n",
    "    # Read in metrics and reset experiment_models based on avaliable metrics\n",
    "    m_sub, experiment_models = get_per_period_metrics_df(\n",
    "        all_metrics, period=\"individual\", gt_id=gt_id, horizon=horizon,\n",
    "        metric=metric, target_dates=target_dates, \n",
    "        relative_to=relative_to,\n",
    "        model_names=table_models, include_overall=include_overall, dropna=dropna)\n",
    "    metrics_sub_se= metrics_sub.copy()\n",
    "    m_sub_se_or = pd.DataFrame(index=experiment_models, columns = metrics_sub.columns)\n",
    "\n",
    "    # Generate metrics dataframe for each task\n",
    "    for i, (gt_id, horizon) in enumerate(product(task_ids, horizons)):\n",
    "        task = f\"{gt_id}_{horizon}\"\n",
    "        printf(f\"Processing {task}\")  \n",
    "        # Read in metrics and reset experiment_models based on avaliable metrics\n",
    "        m_sub, experiment_models = get_per_period_metrics_df(\n",
    "            all_metrics, period=\"individual\", gt_id=gt_id, horizon=horizon,\n",
    "            metric=metric, target_dates=target_dates, \n",
    "            relative_to=relative_to,\n",
    "            model_names=table_models, include_overall=include_overall, dropna=dropna)\n",
    "        task = us_tasks[task]\n",
    "        # import scikits.bootstraps as bootstraps\n",
    "        for col in experiment_models:\n",
    "            mean_col = m_sub[col].mean()\n",
    "            std_col = m_sub[col].std()\n",
    "            n_col = m_sub[col].notna().sum()\n",
    "            t1 = [simulate_sample_mean(n_col, mean_col, std_col) for i in range(1000)]\n",
    "            summary1 = summarize(t1, digits=6)\n",
    "            m_sub_se_or.loc[col][task] = summary1['SE'][0]\n",
    "        # maintain ordered list of model names\n",
    "        m_sub_se = m_sub_se_or.T\n",
    "        m_sub_se = m_sub_se[table_models].T\n",
    "        if metric is \"skill\":\n",
    "            m_sub_se = m_sub_se.multiply(100) \n",
    "        if period is 'overall':\n",
    "            m_sub_se['model_type'] = [all_model_types[m] for m in m_sub_se.index]\n",
    "            m_sub_se = m_sub_se.rename(all_model_names, axis=0).reset_index().set_index(['model_type', 'index'])\n",
    "            m_sub_se.columns = m_sub_se.columns.get_level_values(0)\n",
    "\n",
    "    # display and save dataframe in latex table format\n",
    "    m_sub = metrics_sub.astype(float).round(2).astype(str).add(' $\\pm$ ').add(m_sub_se.astype(float).round(2).astype(str))\n",
    "    display(m_sub.style.apply(lambda x: metrics_sub.apply(highlight_func, axis=0), axis=None).apply(lambda x: metrics_sub.apply(bold_func, axis=0), axis=None))\n",
    "    filename_table = f\"table_{region}_{period}_{metric}_over_{relative_to}_{target_dates}_se\"\n",
    "    table_to_tex(m_sub, tables_dir, filename_table, precision=2)\n",
    "    printf(f\"Table saved in {os.path.join(tables_dir, f'{filename_table}.tex')}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 19: \n",
    "#### Percentage improvement over mean debiased CFSv2 RMSE when forecasting temperature in the contiguous U.S. over 2018-2020. White grid points indicate negative or 0% improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate figure metrics\n",
    "figure_gt_ids = us_gt_ids\n",
    "figure_horizons = ['34w']\n",
    "figure_target_dates = 'std_paper_graphcast'\n",
    "figure_metrics = ['lat_lon_rmse']\n",
    "figure_models = [\n",
    "    #relative_to\n",
    "    'deb_cfsv2',\n",
    "    # Model 1\n",
    "    'graphcast',                                                      \n",
    "    # Model 2\n",
    "    'linear_ensemble',\n",
    "    # Model 3\n",
    "    'online_learning',   \n",
    "]\n",
    "\n",
    "metric_dfs_rda = {}\n",
    "for gt_id, horizon in product(figure_gt_ids, figure_horizons):\n",
    "    task = f\"{gt_id}_{horizon}\"\n",
    "    display(Markdown(f\"#### Getting metrics for {gt_id} {horizon}\"))\n",
    "    metric_dfs_rda[task] = get_models_metric_lat_lon(gt_id=gt_id, horizon=horizon, \n",
    "                                                     target_dates=figure_target_dates, \n",
    "                                                     metrics = figure_metrics, \n",
    "                                                     model_names=figure_models)\n",
    "    \n",
    "# Set figure parameter\n",
    "figure_gt_ids = us_gt_ids\n",
    "figure_horizons = ['34w']\n",
    "figure_metric = 'lat_lon_rmse'\n",
    "figure_relative_to = 'deb_cfsv2'\n",
    "figure_mean_metric_df = None\n",
    "figure_cb_minmax = (0, 30)\n",
    "figure_cb_skip = 5\n",
    "figure_source_data = False\n",
    "figure_show = True\n",
    "\n",
    "\n",
    "\n",
    "figure_model_names = figure_models \n",
    "display(Markdown(f'#### Models: {\", \".join(figure_model_names)}'))\n",
    "for figure_gt_id, figure_horizon in product(figure_gt_ids, figure_horizons):\n",
    "    display(Markdown(f\"#### {figure_gt_id} {figure_horizon}\"))\n",
    "    plot_metric_maps_task_ds(metric_dfs_rda, model_names=figure_model_names,\n",
    "                         gt_id=figure_gt_id,\n",
    "                         horizon=figure_horizon,\n",
    "                         metric=figure_metric,\n",
    "                         target_dates=figure_target_dates,\n",
    "                         relative_to=figure_relative_to,\n",
    "                         mean_metric_df=figure_mean_metric_df,\n",
    "                         show=figure_show, \n",
    "                         scale_type='linear',\n",
    "                         CB_colors_customized=[\"white\", \"green\", \"darkgreen\"],\n",
    "                         CB_minmax = figure_cb_minmax,\n",
    "                         CB_skip = figure_cb_skip,\n",
    "                         source_data = figure_source_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 20: \n",
    "#### Model bias when forecasting temperature in the contiguous U.S. over 2018-2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate figure metrics\n",
    "figure_gt_ids = us_gt_ids\n",
    "figure_horizons = ['34w']\n",
    "figure_target_dates = 'std_paper_graphcast'\n",
    "figure_metrics = ['lat_lon_error']\n",
    "figure_models = [\n",
    "    # Model 1\n",
    "    'graphcast',                                                      \n",
    "    # Model 2\n",
    "    'linear_ensemble',\n",
    "    # Model 3\n",
    "    'online_learning',   \n",
    "]\n",
    "\n",
    "\n",
    "metric_dfs_rda = {}\n",
    "for gt_id, horizon in product(figure_gt_ids, figure_horizons):\n",
    "    task = f\"{gt_id}_{horizon}\"\n",
    "    display(Markdown(f\"#### Getting metrics for {gt_id} {horizon}\"))\n",
    "    metric_dfs_rda[task] = get_models_metric_lat_lon(gt_id=gt_id, horizon=horizon, \n",
    "                                                     target_dates=figure_target_dates, \n",
    "                                                     metrics = figure_metrics, \n",
    "                                                     model_names=figure_models)\n",
    "\n",
    "# Set figure parameter\n",
    "figure_gt_ids = us_gt_ids\n",
    "figure_horizons = ['34w']\n",
    "figure_metric = 'lat_lon_error'\n",
    "figure_relative_to = None\n",
    "figure_mean_metric_df = None\n",
    "figure_source_data = False\n",
    "figure_show = True\n",
    "\n",
    "\n",
    "\n",
    "figure_model_names = figure_models \n",
    "display(Markdown(f'#### Models: {\", \".join(figure_model_names)}'))\n",
    "for figure_gt_id, figure_horizon in product(figure_gt_ids, figure_horizons):\n",
    "    if 'tmp2m' in gt_id:\n",
    "        figure_cb_minmax = (-6, 6)\n",
    "        figure_cb_skip = 3\n",
    "    elif 'precip' in gt_id:\n",
    "        figure_cb_minmax = (-15, 15)\n",
    "        figure_cb_skip = 5\n",
    "\n",
    "                 \n",
    "    display(Markdown(f\"#### {figure_gt_id} {figure_horizon}\"))\n",
    "    plot_metric_maps_task_ds(metric_dfs_rda, model_names=figure_model_names,\n",
    "                         gt_id=figure_gt_id,\n",
    "                         horizon=figure_horizon,\n",
    "                         metric=figure_metric,\n",
    "                         target_dates=figure_target_dates,\n",
    "                         relative_to=figure_relative_to,\n",
    "                         mean_metric_df=figure_mean_metric_df,\n",
    "                         show=figure_show, \n",
    "                         scale_type='linear',\n",
    "                         CB_colors_customized=[\"darkblue\", \"blue\", \"white\", \"red\", \"darkred\"],\n",
    "                         CB_minmax = figure_cb_minmax,\n",
    "                         CB_skip = figure_cb_skip,\n",
    "                         source_data = figure_source_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.7 Western U.S. Competition Results\n",
    "## Table 8: \n",
    "#### Percentage improvement over mean debiased CFSv2 RMSE over 26 contest dates (2019-2020) in the Western U.S. The best performing models within each class of models are shown in bold, while the best performing models overall are shown in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Rodeo experiment tables\n",
    "\"\"\"\n",
    "target_dates = \"std_contest\" \n",
    "period = \"overall\" # <- must be overall to merge with rodeo dataframe\n",
    "metric = \"rmse\" \n",
    "table_models = rodeo_experiment_models\n",
    "relative_to = 'TC_CFSv2'\n",
    "dropna = True # if true, compute average metrics only on dates where predictions have all values \n",
    "tasks = contest_tasks\n",
    "horizons = ['34w', '56w']\n",
    "region = 'contest'\n",
    "include_overall = True # include overall row in the dataframe\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "End experiment parameters \n",
    "\"\"\"\n",
    "if (metric is 'rmse' and relative_to is not None) or (metric is not 'rmse' and relative_to is None):\n",
    "    highlight_func = highlight_max\n",
    "    bold_func = bold_max\n",
    "else:\n",
    "    highlight_func = highlight_min\n",
    "    bold_func = bold_min\n",
    "\n",
    "\n",
    "# Create metrics dataframe template\n",
    "metric_tasks = pd.DataFrame(index=tasks, columns=table_models)\n",
    "\n",
    "# Get Topcoder CFSv2 baseline \n",
    "baseline_TC_df = get_leaderboard(metric, drop_columns=['Mouatadid'])[relative_to]\n",
    "\n",
    "\n",
    "# Generate metrics dataframe for each task \n",
    "for gt_id, horizon in product(contest_gt_ids, horizons):\n",
    "    task = f\"{gt_id}_{horizon}\"\n",
    "    \n",
    "    # Read in metrics and reset table_models based on avaliable metrics\n",
    "    metric_tasks.loc[task], table_models = get_per_period_metrics_df(\n",
    "        all_metrics, period=period, gt_id=gt_id, horizon=horizon,\n",
    "        metric=metric, target_dates=target_dates, \n",
    "        relative_to=None,\n",
    "        model_names=table_models, include_overall=include_overall, dropna=dropna)   \n",
    "\n",
    "# Get leaderboard dataframe\n",
    "leaderboard = get_leaderboard(metric, \n",
    "                              relative_to=relative_to, \n",
    "                              baseline_df=baseline_TC_df,\n",
    "                              drop_columns=['Mouatadid'])\n",
    "\n",
    "#Calculate percentage improvement over Topcoder CFSv2 RMSE\n",
    "metric_tasks = pd.merge(baseline_TC_df, metric_tasks, left_index=True, right_index=True).astype(float)\n",
    "metric_tasks = metric_tasks.apply(partial(bss_score, metric_tasks.columns, relative_to), axis=1)\n",
    "\n",
    "# Concat metric dataframe for model_names with leaderboard dataframe\n",
    "metric_tasks = pd.merge(leaderboard, metric_tasks, left_index=True, right_index=True).T.astype(float)\n",
    "metric_tasks = metric_tasks[[t for t in tasks]]\n",
    "\n",
    "# Map input names to display names\n",
    "metric_tasks = metric_tasks.rename(contest_tasks, axis=1).rename(all_model_names, axis=0)\n",
    "# printf(metric_tasks)\n",
    "metric_tasks = metric_tasks.loc[['TC_Salient', 'TC_Climatology', \n",
    "'1st place', '2nd place', '3rd place',\n",
    "'AutoKNN', 'LocalBoosting', 'MultiLLR', 'Prophet', 'Salient 2.0', \n",
    "'Climatology++', 'CFSv2++', 'Persistence++', \n",
    "'Uniform ABC', 'Online ABC']]\n",
    "\n",
    "print(f\"{period} {target_dates}\")\n",
    "\n",
    "if period is 'overall':\n",
    "    display(metric_tasks.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "else:\n",
    "    display(metric_tasks.style.apply(highlight_func, axis=1).apply(bold_func, axis=1).set_table_styles(styles))\n",
    "\n",
    "#save dataframe in latex table format\n",
    "table_to_tex(metric_tasks.astype(float).round(2), out_dir, f\"table_{region}_{period}_{metric}_over_{relative_to}_{target_dates}\", precision=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.8 Salient 2.0 Dry Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 21: \n",
    "#### Temporal plot (left) and scatter plot (right) of yearly total precipitation and percentage improvement over mean debiased CFSv2 RMSE in the Western U.S. across 2011-2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate a dictionary with anomalies values for all models and every combination of gt_id, \n",
    "horizon, and target dates\n",
    "\"\"\"\n",
    "all_preds = {}\n",
    "\n",
    "# Populate dictionaries for each gt_id, horizon, and target_dates for main experiment and salient experiment\n",
    "for gt_id, horizon, target_dates in \\\n",
    "            [x for x in product(contest_gt_ids[-1:], horizons[:1], ['std_paper'])] :\n",
    "    model_names = [\"deb_cfsv2\", \"tuned_cfsv2pp\", \"tuned_salient2\"]#all_models\n",
    "    model_names_str = \"salient_dry_bias_models\"#'all_models'   \n",
    "        \n",
    "    # Get task\n",
    "    task = f\"{gt_id}_{horizon}\"\n",
    "    \n",
    "    display(Markdown(f\"### {model_names_str}: {task}, {target_dates}\"))\n",
    "    \n",
    "    # Get all anoms\n",
    "    print(f\"Creating dataframes for models:\\n {model_names}\\n\")    \n",
    "    df_preds, _, _ = get_trio_df(gt_id=gt_id, horizon=horizon, target_dates=target_dates,\n",
    "                                              model_names=model_names)\n",
    "    print(f\"DONE!\\n\")\n",
    "    # No models exist for this task    \n",
    "    if df_preds is None: \n",
    "        continue\n",
    "    \n",
    "    # Add yearly and quarterly columns to the dataframe\n",
    "    print(f\"\\nAdding group-by columns to dataframes...\")\n",
    "    tic()\n",
    "    df_preds = add_groupby_cols(df_preds, horizon=horizon)\n",
    "    toc()\n",
    "    print(f\"DONE!\\n\")\n",
    "    \n",
    "    all_preds[(task, target_dates)] = copy.copy(df_preds.reset_index('start_date')) \n",
    "    \n",
    "\n",
    "# Figure parameters\n",
    "figure_models = [\n",
    "    'deb_cfsv2',\n",
    "    'tuned_cfsv2pp',\n",
    "    'gt',\n",
    "    'tuned_salient2'\n",
    "]\n",
    "target_dates = \"std_paper\" \n",
    "period = \"yearly\" \n",
    "metric = \"rmse\" \n",
    "relative_to = 'deb_cfsv2' \n",
    "task_ids = ['contest_precip'] \n",
    "task_horizons = ['34w']\n",
    "file_str = f\"contest_{period}_over_{relative_to}\" \n",
    "\n",
    "\"\"\"\n",
    "End experiment parameters \n",
    "\"\"\"\n",
    "print(target_dates)\n",
    "# Plot subfigure\n",
    "plot_models_metrics_preds_line(all_metrics,\n",
    "                              all_preds,\n",
    "                              gt_id_list=task_ids, \n",
    "                              horizon_list=task_horizons, \n",
    "                              target_dates=target_dates, \n",
    "                              model_names=figure_models,\n",
    "                              period=period,\n",
    "                              relative_to=relative_to,\n",
    "                              metric=metric,\n",
    "                              file_str=file_str)\n",
    "# Plot subfigure\n",
    "plot_models_metrics_preds_scatter(all_metrics,\n",
    "                              all_preds,\n",
    "                              gt_id_list=task_ids, \n",
    "                              horizon_list=task_horizons, \n",
    "                              target_dates=target_dates, \n",
    "                              model_names=figure_models,\n",
    "                              period=period,\n",
    "                              relative_to=relative_to,\n",
    "                              metric=metric,\n",
    "                              file_str=file_str)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
