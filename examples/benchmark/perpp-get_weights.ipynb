{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch regression\n",
    "\n",
    "Carry out regression experiment for a fixed set of predictors and a set of target dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure notebook is being run from base repository directory\n",
    "import os, sys\n",
    "try:\n",
    "    os.chdir(\"/home/franklyn/forecast_rodeo_ii\")\n",
    "except Exception as err:\n",
    "    print(f\"Warning: unable to change directory; {repr(err)}\")\n",
    "#sys.path.insert(0, \"/home/franklyn/rodeo_ii/forecast_rodeo_ii\")\n",
    "from src.utils.notebook_util import isnotebook\n",
    "if isnotebook():\n",
    "    # Autoreload packages that are modified\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "else:\n",
    "    from argparse import ArgumentParser\n",
    "\n",
    "# Imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "import sys\n",
    "import json\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "from functools import partial\n",
    "from multiprocessing import cpu_count\n",
    "from src.utils.general_util import tic, toc, printf\n",
    "from src.utils.experiments_util import *\n",
    "from subseasonal_data.utils import get_measurement_variable\n",
    "from src.utils.eval_util import get_target_dates, mean_rmse_to_score\n",
    "from src.utils.fit_and_predict import apply_parallel\n",
    "from src.utils.models_util import *\n",
    "from src.models.llr.llr_util import fit_and_predict_wgt, fit_and_predict, years_ago\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Specify model parameters\n",
    "#\n",
    "model_name = \"llr\"\n",
    "if not isnotebook():\n",
    "    # If notebook run as a script, parse command-line arguments\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"pos_vars\",nargs=\"*\")  # gt_id and horizon                                                                                  \n",
    "    parser.add_argument('--target_dates', '-t', default=\"std_contest\")\n",
    "    \n",
    "    # Number of years to use in training (\"all\" or integer)\n",
    "    parser.add_argument('--train_years', '-y', default=\"all\")\n",
    "    \n",
    "    # Number of month-day combinations on either side of the target combination \n",
    "    # to include when training\n",
    "    # Set to 0 to include only target month-day combo\n",
    "    # Set to \"None\" to include entire year\n",
    "    parser.add_argument('--margin_in_days', '-m', default=\"None\")\n",
    "    \n",
    "    # If True, use cfsv2 ensemble forecast as a feature\n",
    "    parser.add_argument('--use_cfsv2', '-c', default=\"False\")\n",
    "    args, opt = parser.parse_known_args()\n",
    "    \n",
    "    # Assign variables                                                                                                                                     \n",
    "    gt_id = args.pos_vars[0] # \"contest_precip\" or \"contest_tmp2m\"                                                                            \n",
    "    horizon = args.pos_vars[1] # \"34w\" or \"56w\"                                                                                        \n",
    "    target_dates = args.target_dates\n",
    "    train_years = args.train_years\n",
    "    if train_years != \"all\":\n",
    "        train_years = int(train_years)\n",
    "    if args.margin_in_days == \"None\":\n",
    "        margin_in_days = None\n",
    "    else:\n",
    "        margin_in_days = int(args.margin_in_days)\n",
    "        \n",
    "    use_best_cfsv2 = (args.use_cfsv2 == \"True\")\n",
    "    \n",
    "else:\n",
    "    # Otherwise, specify arguments interactively \n",
    "    gt_id = \"us_tmp2m\"\n",
    "    horizon = \"56w\"\n",
    "    target_dates = \"std_paper\"\n",
    "    train_years = \"all\"\n",
    "    margin_in_days = None\n",
    "    use_best_cfsv2 = True\n",
    "\n",
    "#\n",
    "# Process model parameters\n",
    "#\n",
    "\n",
    "# Get list of target date objects\n",
    "target_date_objs = pd.Series(get_target_dates(date_str=target_dates,horizon=horizon))\n",
    "\n",
    "# Sort target_date_objs by day of week\n",
    "target_date_objs = target_date_objs[target_date_objs.dt.weekday.argsort(kind='stable')]\n",
    "\n",
    "# Identify measurement variable name\n",
    "measurement_variable = get_measurement_variable(gt_id) # 'tmp2m' or 'precip'\n",
    "\n",
    "# Column names for gt_col, clim_col and anom_col \n",
    "gt_col = measurement_variable\n",
    "clim_col = measurement_variable+\"_clim\"\n",
    "anom_col = get_measurement_variable(gt_id)+\"_anom\" # 'tmp2m_anom' or 'precip_anom'\n",
    "\n",
    "# Store delta between target date and forecast issuance date\n",
    "start_delta =  timedelta(days=get_start_delta(horizon, gt_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Choose regression parameters\n",
    "#\n",
    "# Record standard settings of these parameters\n",
    "base_col = \"zeros\"    \n",
    "if (gt_id.endswith(\"tmp2m\")) and (horizon == \"34w\"):\n",
    "    cfsv2_col = 'subx_cfsv2_tmp2m' if use_best_cfsv2 else 'subx_cfsv2_tmp2m-28.5d_shift15'\n",
    "    x_cols = [\n",
    "    'tmp2m_shift29',\n",
    "    'tmp2m_shift58',\n",
    "    cfsv2_col,\n",
    "    clim_col\n",
    "    ] \n",
    "elif (gt_id.endswith(\"precip\")) and (horizon == \"34w\"):\n",
    "    cfsv2_col = 'subx_cfsv2_precip' if use_best_cfsv2 else 'subx_cfsv2_precip-14.5d_shift15'\n",
    "    x_cols = [\n",
    "    'precip_shift29',\n",
    "    'precip_shift58',\n",
    "    cfsv2_col,\n",
    "    clim_col\n",
    "    ] \n",
    "elif (gt_id.endswith(\"tmp2m\")) and (horizon == \"56w\"):\n",
    "    cfsv2_col = 'subx_cfsv2_tmp2m' if use_best_cfsv2 else 'subx_cfsv2_tmp2m-28.5d_shift29'\n",
    "    x_cols = [\n",
    "    'tmp2m_shift43',\n",
    "    'tmp2m_shift86',\n",
    "    cfsv2_col,\n",
    "    clim_col\n",
    "    ] \n",
    "elif (gt_id.endswith(\"precip\")) and (horizon == \"56w\"):\n",
    "    cfsv2_col = 'subx_cfsv2_precip' if use_best_cfsv2 else 'subx_cfsv2_precip-28.5d_shift29'\n",
    "    x_cols = [\n",
    "    'precip_shift43',\n",
    "    'precip_shift86',\n",
    "    cfsv2_col,\n",
    "    clim_col\n",
    "    ]\n",
    "group_by_cols = ['lat', 'lon']\n",
    "\n",
    "# Record submodel names for llr model\n",
    "submodel_name = get_submodel_name(\n",
    "    model_name, train_years=train_years, margin_in_days=margin_in_days,\n",
    "    use_cfsv2=use_best_cfsv2)\n",
    "\n",
    "printf(f\"Submodel name {submodel_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load lat lon date data\n",
    "# Exclude clim col when loading data initially\n",
    "#\n",
    "pred_cols = x_cols+[base_col]\n",
    "\n",
    "if use_best_cfsv2:\n",
    "    non_pred_cols = set([clim_col, cfsv2_col, 'zeros'])    \n",
    "else:\n",
    "    non_pred_cols = set([clim_col, 'zeros']) \n",
    "    \n",
    "cols_to_load = set(['lat','lon','start_date',gt_col]+pred_cols) - non_pred_cols   \n",
    "lld_data = load_combined_data(\n",
    "    \"lat_lon_date_data\",gt_id,horizon,model='default',\n",
    "    columns=cols_to_load)\n",
    "\n",
    "if 'zeros' in pred_cols:\n",
    "    lld_data['zeros'] = 0\n",
    "    \n",
    "# Drop rows with empty pred_cols\n",
    "lld_data = lld_data.dropna(subset=set(pred_cols) - non_pred_cols)\n",
    "if clim_col in pred_cols:\n",
    "    printf(\"Merging in climatology\")\n",
    "    tic()\n",
    "    lld_data = clim_merge(lld_data, get_climatology(gt_id))\n",
    "    toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_best_cfsv2:\n",
    "    #\n",
    "    # Add cfsv2 ensemble forecast as feature\n",
    "    #\n",
    "    printf(f\"Forming cfsv2 ensemble forecast...\")\n",
    "    shift = 15 if horizon == \"34w\" else 29\n",
    "    first_lead = 14 if horizon == \"34w\" else 28\n",
    "    last_lead = 29\n",
    "    suffix = \"-us\" if gt_id.startswith(\"us_\") else \"\"\n",
    "    tic(); data = get_forecast(\"subx_cfsv2-\"+gt_id.split(\"_\")[1]+suffix, shift=shift); toc()\n",
    "    printf(f\"Aggregating lead {first_lead} with shift {shift}\")\n",
    "    tic()\n",
    "    cfsv2 = data[['lat','lon','start_date',f'subx_cfsv2_{gt_col}-{first_lead}.5d_shift{shift}']].set_index(\n",
    "        ['lat','lon','start_date']).squeeze().unstack(['lat','lon']).copy()\n",
    "    toc()\n",
    "    for lead in range(first_lead+1,last_lead+1):\n",
    "        printf(f\"Aggregating lead {lead} with shift {shift+lead-first_lead}\")\n",
    "        tic()\n",
    "        cfsv2 += data[['lat','lon','start_date',f'subx_cfsv2_{gt_col}-{lead}.5d_shift{shift}']].set_index(\n",
    "            ['lat','lon','start_date']).squeeze().unstack(['lat','lon']).shift(lead-first_lead)\n",
    "        toc()\n",
    "    num_leads = last_lead - first_lead + 1\n",
    "    cfsv2 /= num_leads \n",
    "    # Drop dates with no forecasts and reshape\n",
    "    cfsv2 = cfsv2.dropna().unstack().rename(cfsv2_col)\n",
    "    \n",
    "    ##lld_data = lld_data.drop(columns=[cfsv2_col])\n",
    "    # Merge cfsv2 forecast with lld_data\n",
    "    printf(f\"Merging {cfsv2_col} with lld_data\")\n",
    "    tic()\n",
    "    lld_data = pd.merge(lld_data, cfsv2, left_on=['lat','lon','start_date'], \n",
    "                        right_index=True)\n",
    "    toc()\n",
    "    del cfsv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify regression model\n",
    "fit_intercept = True\n",
    "model = linear_model.LinearRegression(fit_intercept=fit_intercept, normalize=True)\n",
    "\n",
    "# Form predictions for each grid point (in parallel) using train / test split\n",
    "# and the selected model\n",
    "prediction_func = partial(fit_and_predict, model=model)\n",
    "prediction_func_weights = partial(fit_and_predict_wgt, model=model)\n",
    "num_cores = cpu_count()\n",
    "\n",
    "# Store rmses\n",
    "rmses = pd.Series(index=target_date_objs, dtype='float64')\n",
    "\n",
    "# Restrict data to relevant columns and rows for which predictions can be made\n",
    "relevant_cols = set(\n",
    "    ['start_date','lat','lon',gt_col,base_col]+x_cols).intersection(lld_data.columns)\n",
    "lld_data = lld_data[relevant_cols].dropna(subset=x_cols+[base_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_filename(target_date, gt_id, horizon):\n",
    "    return f\"weights/{target_date.year}{target_date.month:02d}{target_date.day:02d}_{gt_id}_{horizon}_normalized_weights.h5\"\n",
    "\n",
    "def save_file(df, path):\n",
    "    df.to_hdf(path, key=\"df\")\n",
    "\n",
    "last_date = None\n",
    "for target_date_obj in target_date_objs:\n",
    "    last_date = target_date_obj\n",
    "\n",
    "for target_date_obj in target_date_objs:\n",
    "    if target_date_obj != last_date:\n",
    "        continue\n",
    "    printf(target_date_obj)\n",
    "    if not any(lld_data.start_date.isin([target_date_obj])):\n",
    "        printf(f\"warning: some features unavailable for target={target_date_obj}; skipping\")\n",
    "        continue    \n",
    "        \n",
    "    target_date_str = datetime.strftime(target_date_obj, '%Y%m%d')\n",
    "    \n",
    "    # Skip if forecast already produced for this target\n",
    "\n",
    "    \n",
    "    if True:\n",
    "        printf(f'target={target_date_str}')\n",
    "        \n",
    "        # Subset data based on margin\n",
    "        if margin_in_days is not None:\n",
    "            tic()\n",
    "            sub_data = month_day_subset(lld_data, target_date_obj, margin_in_days)\n",
    "            toc()\n",
    "        else:\n",
    "            sub_data = lld_data\n",
    "            \n",
    "        # Find the last observable training date for this target\n",
    "        last_train_date = target_date_obj - start_delta \n",
    "        \n",
    "        # Only train on train_years worth of data\n",
    "        if train_years != \"all\":\n",
    "            tic()\n",
    "            sub_data = sub_data.loc[sub_data.start_date >= years_ago(last_train_date, train_years)]\n",
    "            toc()\n",
    "            \n",
    "        tic()\n",
    "        weights = apply_parallel(\n",
    "            sub_data.groupby(group_by_cols),\n",
    "            prediction_func_weights, \n",
    "            num_cores=num_cores,\n",
    "            gt_col=gt_col,\n",
    "            x_cols=x_cols, \n",
    "            base_col=base_col, \n",
    "            last_train_date=last_train_date,\n",
    "            test_dates=[target_date_obj])\n",
    "        path = get_filename(target_date_obj, gt_id, horizon)\n",
    "        print(path)\n",
    "        print(weights)\n",
    "        save_file(weights, path)\n",
    "        #print(weights)\n",
    "        # Ensure raw precipitation predictions are never less than zero\n",
    "        #if gt_id.endswith(\"precip\"):\n",
    "        #    tic()\n",
    "        #    preds['pred'] = np.maximum(preds['pred'],0)\n",
    "        #    toc()\n",
    "            \n",
    "        \n",
    "        toc()\n",
    "    \n",
    "    # Evaluate and store error\n",
    "    #rmse = np.sqrt(np.square(preds.pred - preds.truth).mean())\n",
    "    #rmses.loc[target_date_obj] = rmse\n",
    "    #print(\"-rmse: {}, score: {}\".format(rmse, mean_rmse_to_score(rmse)))\n",
    "    #mean_rmse = rmses.mean()\n",
    "    #print(\"-mean rmse: {}, running score: {}\".format(mean_rmse, mean_rmse_to_score(mean_rmse)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SCRATCH SPACE\n",
    "\n",
    "# # Will load cfsv2 forecasts from this model\n",
    "# cfsv2_model = \"tuned_cfsv2\"\n",
    "# # TODO: can remove loading of subx data in prior cell\n",
    "# lld_data = lld_data.drop(columns=[cfsv2_col])\n",
    "# x_cols = [cfsv2_model if x==cfsv2_col else x for x in x_cols]\n",
    "# # Restrict data to years in which cfsv2 available\n",
    "# tic()\n",
    "# first_year = get_first_year(\"subx_cfsv2\")\n",
    "# printf(f\"Restricting to years >= {first_year}\")\n",
    "# lld_data = lld_data[lld_data.start_date.dt.year >= first_year]\n",
    "# toc()\n",
    "# # Preallocate dataframe for storing cfsv2_model forecasts\n",
    "# printf(f\"Allocating space for storing {cfsv2_model} forecasts\")\n",
    "# tic()\n",
    "# cfsv2 = lld_data[['lat','lon','start_date',gt_col]].set_index(['lat','lon','start_date']).squeeze().unstack(['lat','lon'])\n",
    "# cfsv2.loc[:] = np.nan\n",
    "# toc()\n",
    "\n",
    "# # Identify selected submodel of cfsv2_model\n",
    "# cfsv2_submodel = get_selected_submodel_name(\n",
    "#     cfsv2_model, gt_id=gt_id, horizon=horizon)\n",
    "# # Prepare function for looking up forecast filename for a given target date\n",
    "# get_filename_fn = partial(get_forecast_filename, gt_id=gt_id, horizon=horizon,\n",
    "#                           model=cfsv2_model, submodel=cfsv2_submodel)\n",
    "# # Load cfsv2_model forecasts\n",
    "# printf(f\"Loading {cfsv2_model} forecasts\")\n",
    "# tic()\n",
    "# for date_obj in cfsv2.index:\n",
    "#     if date_obj.day == 1 and date_obj.month == 1:\n",
    "#         toc(); printf(f\"-Loading {date_obj.year} forecasts\"); tic()\n",
    "#     try:\n",
    "#         date_str = datetime.strftime(date_obj, '%Y%m%d')\n",
    "#         filename = get_filename_fn(target_date_str=date_str)\n",
    "#         cfsv2.loc[date_obj] = pd.read_hdf(filename).set_index(['lat','lon']).pred\n",
    "#     except FileNotFoundError as e:\n",
    "#         pass #printf(f\"-warning: no forecast for {date_str}; skipping\")\n",
    "# toc()\n",
    "# # Drop dates with no forecasts and reshape\n",
    "# cfsv2 = cfsv2.dropna().unstack().rename(cfsv2_model)\n",
    "\n",
    "# # Merge cfsv2 forecast with lld_data\n",
    "# printf(f\"Merging {cfsv2_model} with lld_data\")\n",
    "# tic()\n",
    "# lld_data = pd.merge(lld_data, cfsv2, left_on=['lat','lon','start_date'], \n",
    "#                     right_index=True)\n",
    "# toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SCRATCH SPACE\n",
    "\n",
    "# if use_best_cfsv2:\n",
    "#     # Load and process data\n",
    "#     printf(\"Loading cfsv2 data and averaging leads\")\n",
    "    \n",
    "#     # Choose data shift based on horizon and first day to be averaged\n",
    "#     base_shift = (15 if horizon == \"34w\" else 29) + cfsv2_first_day - 1\n",
    "#     tic()\n",
    "#     if gt_id.startswith('us'):\n",
    "#         data = get_forecast(\"subx_cfsv2-\"+gt_id.split(\"_\")[1]+'-us', shift=base_shift)  \n",
    "#     else:\n",
    "#         data = get_forecast(\"subx_cfsv2-\"+gt_id.split(\"_\")[1], shift=base_shift)  \n",
    "#     cols = [\"subx_cfsv2_\"+gt_id.split(\"_\")[1]+\"-{}.5d_shift{}\".format(col,base_shift) \n",
    "#             for col in range(cfsv2_first_lead, cfsv2_last_lead+1)]\n",
    "#     data[cfsv2_col] = data[cols].mean(axis=1)\n",
    "#     toc()    \n",
    "\n",
    "#     printf('Pivoting dataframe to have one row per start_date')\n",
    "#     tic()\n",
    "#     data = data[['lat','lon','start_date', cfsv2_col]].set_index(['lat','lon','start_date']).squeeze().unstack(['lat','lon'])\n",
    "#     toc()\n",
    "\n",
    "#     printf(f\"Computing rolling mean over days {cfsv2_first_day}-{cfsv2_last_day}\")\n",
    "#     days = cfsv2_last_day - cfsv2_first_day + 1\n",
    "#     tic()\n",
    "#     data = data.rolling(f\"{days}d\").mean().dropna(how='any')\n",
    "#     toc()\n",
    "\n",
    "#     printf(\"Pivoting dataframe to long format and resetting index\")\n",
    "#     tic()\n",
    "#     data = data.unstack().rename(cfsv2_col)\n",
    "#     toc()    \n",
    "    \n",
    "#     printf(\"Merging cfsv2 data into lld data\")\n",
    "#     tic()\n",
    "#     lld_data = lld_data.merge(data, on=['lat','lon','start_date'], how='inner')\n",
    "#     del data\n",
    "#     toc()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
