{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Generates figures and latex tables for paper experiments\n",
    "\"\"\"\n",
    "# Ensure notebook is being run from base repository directory\n",
    "import os, sys\n",
    "try:\n",
    "    os.chdir(\"/home/{}/forecast_rodeo_ii/\".format(os.environ[\"USER\"]))\n",
    "except Exception as err:\n",
    "    print(f\"Warning: unable to change directory; {repr(err)}\")\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline    \n",
    "    \n",
    "import itertools\n",
    "import importlib\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import copy\n",
    "import pdb\n",
    "import calendar \n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib   \n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "from subseasonal_toolkit.utils.experiments_util import pandas2hdf\n",
    "from subseasonal_toolkit.utils.general_util import printf\n",
    "from subseasonal_toolkit.utils.eval_util import get_target_dates, score_to_mean_rmse, contest_quarter_start_dates, contest_quarter\n",
    "from subseasonal_toolkit.utils.models_util import get_selected_submodel_name\n",
    "from subseasonal_toolkit.utils.viz_util import *\n",
    "\n",
    "# set figure and font sizes for seaborn plots\n",
    "sns.set(rc={'figure.figsize':(8,6)}, font_scale=1)\n",
    "\n",
    "#\n",
    "# Directory for saving output\n",
    "#\n",
    "out_dir = \"/home/{}/forecast_rodeo_ii/subseasonal_toolkit/viz\".format(os.environ[\"USER\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Full set of regions, times, and tasks to evaluate\n",
    "#\n",
    "metrics = [\"rmse\", \"skill\", \"score\"]\n",
    "\n",
    "contest_gt_ids = [\"contest_tmp2m\", \"contest_precip\"]\n",
    "us_gt_ids = [\"us_tmp2m\", \"us_precip\"]\n",
    "east_gt_ids = [\"east_tmp2m\", \"east_precip\"]\n",
    "us_1_5_gt_ids = [\"us_tmp2m_1.5x1.5\", \"us_precip_1.5x1.5\"]\n",
    "\n",
    "# All ground truth ids\n",
    "gt_ids = contest_gt_ids + us_gt_ids \n",
    "\n",
    "horizons = [\"34w\", \"56w\"]\n",
    "target_eval_dates = [\"std_paper\", \"std_contest\"]\n",
    "\n",
    "# The full set of models we to evaluate in some\n",
    "# experiment \n",
    "all_models = [\n",
    "    # Raw Baselines\n",
    "    'raw_cfsv2', \n",
    "    # Baselines\n",
    "    \"climatology\",   \n",
    "    'deb_cfsv2',\n",
    "    'persistence',\n",
    "    # ECMWF\n",
    "    'ecmwf'\n",
    "    # Toolkit \n",
    "    'tuned_climpp',\n",
    "    'tuned_cfsv2pp',\n",
    "    'perpp',\n",
    "    #Learning\n",
    "    'autoknn',\n",
    "    'informer',\n",
    "    'tuned_localboosting',\n",
    "    'multillr',\n",
    "    'nbeats',\n",
    "    'prophet',\n",
    "    'salient',\n",
    "    'tuned_salient2',\n",
    "    #Ensembles\n",
    "    'linear_ensemble',  \n",
    "    'online_learning'\n",
    "]\n",
    "\n",
    "# Main experiment model names\n",
    "main_experiment_models = [\n",
    "    # Baselines\n",
    "    \"climatology\",   \n",
    "    'deb_cfsv2',\n",
    "    'persistence',\n",
    "    # Toolkit \n",
    "    'tuned_climpp',\n",
    "    'tuned_cfsv2pp',\n",
    "    'perpp',\n",
    "    #Learning\n",
    "    'autoknn',\n",
    "    'tuned_localboosting',\n",
    "    'multillr',\n",
    "    'nbeats',\n",
    "    'informer',\n",
    "    'prophet',\n",
    "    'tuned_salient2',\n",
    "    #Ensembles\n",
    "    'linear_ensemble',  \n",
    "    'online_learning'\n",
    "]\n",
    "\n",
    "# Rodeo experiment model names\n",
    "rodeo_experiment_models = [\n",
    "    # Baselines\n",
    "    \"climatology\",   \n",
    "    'deb_cfsv2',\n",
    "    'persistence',\n",
    "    # Toolkit \n",
    "    'tuned_climpp',\n",
    "    'tuned_cfsv2pp',\n",
    "    'perpp',\n",
    "    #Learning\n",
    "    'autoknn',\n",
    "    'tuned_localboosting',\n",
    "    'multillr',\n",
    "    'prophet',\n",
    "    'tuned_salient2',\n",
    "    #Ensembles\n",
    "    'linear_ensemble_localFalse_dynamicFalse_stepFalse_LtCtD',\n",
    "    'linear_ensemble_localFalse_dynamicFalse_stepFalse_AMLPtCtDtKtS',  \n",
    "    'online_learning-ah_rpNone_R1_recent_g_SC_LtCtD',\n",
    "    'online_learning-ah_rpNone_R1_recent_g_SC_AMLPtCtDtKtS'\n",
    "]\n",
    "\n",
    "# Salient experiment model names\n",
    "salient_experiment_models = [\n",
    "    # Baselines   \n",
    "    'deb_cfsv2',\n",
    "    # Toolkit \n",
    "    'tuned_cfsv2pp',\n",
    "    #Learning\n",
    "    'tuned_salient2',\n",
    "]\n",
    "\n",
    "\n",
    "# ECMWF experiment model names\n",
    "ecmwf_experiment_models = [\n",
    "    # Baselines\n",
    "    \"climatology\", \n",
    "    'deb_cfsv2',\n",
    "    'persistence',\n",
    "    # Toolkit \n",
    "    'tuned_climpp',\n",
    "    'tuned_cfsv2pp',\n",
    "    'perpp',\n",
    "    # ECMWF\n",
    "    'ecmwf-years20_leads15-15_lossmse_forecastc_debiasp+c',\n",
    "    'ecmwf-years20_leads15-15_lossmse_forecastp_debiasp+c',\n",
    "    # Ensembles\n",
    "    \"online_learning\", \n",
    "    \"linear_ensemble\" \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Dictionaries mapping all model names and tasks to their display names\n",
    "#\n",
    "\n",
    "east_tasks = {\n",
    "    \"east_tmp2m_34w\": \"Temp. weeks 3-4\",\n",
    "    \"east_tmp2m_56w\": \"Temp. weeks 5-6\",\n",
    "    \"east_precip_34w\": \"Precip. weeks 3-4\",\n",
    "    \"east_precip_56w\": \"Precip. weeks 5-6\"\n",
    "}\n",
    "\n",
    "contest_tasks = {\n",
    "    \"contest_tmp2m_34w\": \"Temp. weeks 3-4\",\n",
    "    \"contest_tmp2m_56w\": \"Temp. weeks 5-6\",\n",
    "    \"contest_precip_34w\": \"Precip. weeks 3-4\",\n",
    "    \"contest_precip_56w\": \"Precip. weeks 5-6\"\n",
    "}\n",
    "us_tasks = {\n",
    "    \"us_tmp2m_34w\": \"Temp. weeks 3-4\",\n",
    "    \"us_tmp2m_56w\": \"Temp. weeks 5-6\",\n",
    "    \"us_precip_34w\": \"Precip. weeks 3-4\",\n",
    "    \"us_precip_56w\": \"Precip. weeks 5-6\",   \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model list for generate predictions\n",
    "`tuned_climpp,tuned_cfsv2pp,tuned_localboosting,tuned_salient_fri,perpp,multillr,autoknn,raw_cfsv2,nbeats_final,prophet`\n",
    "## Model list for tuning\n",
    "`climpp,cfsv2,catboost,salient_fri`\n",
    "## Tuned models for metrics\n",
    "`tuned_climpp,tuned_cfsv2pp,tuned_localboosting,tuned_salient_fri`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in all metrics for all tasks and all models\n",
    "Reads metrics, generates a summary of missing data, and produces the `all_metrics` dictionary to be used in further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate a dictionary with metric values for all models and every combination of gt_id, \n",
    "horizon, and target dates\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# TODO: change to add us_gt_ids once metrics are ready\n",
    "all_metrics = {}\n",
    "\n",
    "# Get metrics for main experiment, rodeo experiment, salient experiment and ecmwf experiment\n",
    "for metric, gt_id, horizon, target_dates in \\\n",
    "        [x for x in product(['rmse', 'skill'], us_gt_ids, horizons, ['std_paper'])] \\\n",
    "        +[x for x in product(['rmse'], contest_gt_ids, horizons, ['std_contest'])] \\\n",
    "        +[x for x in product(['rmse'], contest_gt_ids, horizons, ['std_paper'])] \\\n",
    "        +[x for x in product(['rmse', 'skill'], us_1_5_gt_ids, horizons, ['std_ecmwf'])]: \n",
    "   \n",
    "    \n",
    "    #Set model names   \n",
    "    if 'us' in gt_id:\n",
    "        model_names = ecmwf_experiment_models if '1.5x1.5' in gt_id else main_experiment_models\n",
    "        model_names_str = 'ecmwf_experiment_models' if '1.5x1.5' in gt_id else 'main_experiment_models'\n",
    "    elif 'contest' in gt_id:\n",
    "        model_names = rodeo_experiment_models if 'contest' in target_dates else salient_experiment_models\n",
    "        model_names_str = 'rodeo_experiment_models' if 'contest' in target_dates else 'salient_experiment_models'\n",
    "    else:\n",
    "        model_names = all_models\n",
    "        model_names_str = 'all_models'\n",
    "\n",
    "\n",
    "    # Get task\n",
    "    task = f\"{gt_id}_{horizon}\"\n",
    "\n",
    "    #display(Markdown(f\"### Loading metric {metric} for task {task} and dates {target_dates}\"))\n",
    "    display(Markdown(f\"### {model_names_str}: {metric}, {task}, {target_dates}\"))\n",
    "\n",
    "    # Get all metrics\n",
    "    df = get_metrics_df(gt_id, horizon, metric, target_dates, model_names=model_names)\n",
    "\n",
    "    # No models exist for this task    \n",
    "    if df is None: \n",
    "        continue\n",
    "\n",
    "    # Add yearly and quarterly columns to the dataframe\n",
    "    df = add_groupby_cols(df, horizon=horizon)\n",
    "\n",
    "    all_metrics[(metric, task, target_dates)] = copy.copy(df)\n",
    "    #print(all_metrics)\n",
    "\n",
    "    if metric in ['rmse', 'skill']:\n",
    "        key = (metric, task, target_dates)\n",
    "        try:        \n",
    "            missing_df = all_metrics[key].loc[(all_metrics[key][model_names].isnull().any(axis=1)),:]\n",
    "        except:        \n",
    "            missing_df = all_metrics[key].loc[(all_metrics[key].isnull().any(axis=1)),:]            \n",
    "        if missing_df.shape[0] != 0:\n",
    "            True\n",
    "            display(Markdown(f\"#### Missing metrics\"))\n",
    "            display(missing_df)    \n",
    "        else:\n",
    "            printf(\"All metrics present.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting metrics \n",
    "After generating the above `all_metrics` dictionary, you can get the average metric value for a set of models and specific period using the following function:\n",
    "```\n",
    "df = get_per_period_metrics_df(all_metrics, period=\"quarterly\", horizon=\"34w\", metric=\"score\", target_dates=\"std_paper\", model_names=all_models)\n",
    "```\n",
    "\n",
    "The period can be `quarterly`, `yearly`, or `quarterly_yearly` (returns average metrics values in YY1-Q1, YY1-Q2, ..., YY2-Q1, ... etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TABLES\n",
    "## Table 1: \n",
    "### U.S. 2011-2020: % improvement over deb. CFSv2\n",
    "This code produces tables to analyze model performance over differnt periods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Paper experiments; can be configured to generate metrics for any subset of models, averaged over\n",
    "a set of periods, for a given target data. Produces a table and saves to tex. \n",
    "\"\"\"\n",
    "\n",
    "target_dates = \"std_paper\" \n",
    "\n",
    "# quarterly (seasonal quarters), contest_quarterly (contest quarters), \n",
    "# monthly, yearly, individual (return full dataframe), overall (return mean of full dataframe)\n",
    "# monthly_yearly (every month in every year), quarterly_yearly (every quarter in every year)\n",
    "period = \"overall\" # <- must be overall to merge with rodeo dataframe\n",
    "metric = \"rmse\" \n",
    "table_models = main_experiment_models \n",
    "relative_to = 'deb_cfsv2' # compute value relative to climatology value: 1 - metric(model)/metric(climatology) \n",
    "dropna = True # if true, compute average metrics only on dates where predictions have all values \n",
    "task_ids = us_gt_ids # contest_gt_ids (for contest), us_gt_ids (for us), gt_ids (for all)\n",
    "horizons = horizons\n",
    "region = 'us' #either us, east or contest\n",
    "include_overall = True # include overall row in the dataframe\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "End experiment parameters \n",
    "\"\"\"\n",
    "if (metric is 'rmse' and relative_to is not None) or (metric is not 'rmse' and relative_to is None):\n",
    "    highlight_func = highlight_max\n",
    "    bold_func = bold_max\n",
    "else:\n",
    "    highlight_func = highlight_min\n",
    "    bold_func = bold_min\n",
    "\n",
    "\n",
    "# Get set of tasks \n",
    "tasks = [f\"{gt_id}_{horizon}\" for (gt_id, horizon) in product(task_ids, horizons)]\n",
    "\n",
    "metrics_sub = None\n",
    "\n",
    "# Generate metrics dataframe for each task\n",
    "for i, (gt_id, horizon) in enumerate(product(task_ids, horizons)):\n",
    "    task = f\"{gt_id}_{horizon}\"\n",
    "    print(task)\n",
    "    \n",
    "    \n",
    "    # Read in metrics and reset experiment_models based on avaliable metrics\n",
    "    m_sub, experiment_models = get_per_period_metrics_df(\n",
    "        all_metrics, period=period, gt_id=gt_id, horizon=horizon,\n",
    "        metric=metric, target_dates=target_dates, \n",
    "        relative_to=relative_to,\n",
    "        model_names=table_models, include_overall=include_overall, dropna=dropna)\n",
    "    \n",
    "    \n",
    "   # Create metrics dataframe template\n",
    "    if period == \"overall\":\n",
    "        index = pd.Index([task], name=\"task\")            \n",
    "    else:\n",
    "        index = pd.MultiIndex.from_product(\n",
    "            [[task], m_sub.index], \n",
    "            names=('task', 'period'))   \n",
    "        \n",
    "    # Need to form task-by-task, since some tasks are missing target dates, so index differs\n",
    "    if metrics_sub is None:\n",
    "        metrics_sub = pd.DataFrame(index=index, columns=experiment_models)           \n",
    "    else:\n",
    "        metrics_sub = pd.concat([metrics_sub, pd.DataFrame(index=index, columns=experiment_models)])\n",
    "        \n",
    "    if period == \"overall\":\n",
    "        metrics_sub.loc[task, :] = m_sub\n",
    "    else:\n",
    "        metrics_sub[metrics_sub.index.get_level_values(\"task\") == task] = m_sub.values\n",
    "\n",
    "print(f\"{metric} - {target_dates}\");\n",
    "#if relative_to is not None:\n",
    "#    metrics_sub.drop(relative_to, axis=1, inplace=True)\n",
    "\n",
    "# maintain ordered list of model names\n",
    "metrics_sub = metrics_sub[[m for m in main_experiment_models if m in table_models]]\n",
    "\n",
    "if period is 'overall':\n",
    "    metrics_sub=metrics_sub.rename(us_tasks, axis=0).T\n",
    "    metrics_sub['model_type'] = [all_model_types[m] for m in metrics_sub.index]\n",
    "    metrics_sub = metrics_sub.rename(all_model_names, axis=0).reset_index().set_index(['model_type', 'index'])\n",
    "    metrics_sub.columns = metrics_sub.columns.get_level_values(0)\n",
    "    display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "    #for group in ['Baselines', 'Toolkit', 'Learning', 'Ensembles']:\n",
    "        #display(metrics_sub.loc[group].style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "else:\n",
    "    metrics_sub = metrics_sub.reindex([m for m in table_models if m in metrics_sub.columns], axis=1).T\n",
    "    display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "\n",
    "#save dataframe in latex table format\n",
    "table_to_tex(metrics_sub.astype(float), out_dir, f\"table_{region}_{period}_{metric}_over_{relative_to}_{target_dates}\", precision=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Table 1: \n",
    "### U.S. 2011-2020: % skill\n",
    "This code produces tables to analyze model performance over differnt periods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Paper experiments; can be configured to generate metrics for any subset of models, averaged over\n",
    "a set of periods, for a given target data. Produces a table and saves to tex. \n",
    "\"\"\"\n",
    "\n",
    "target_dates = \"std_paper\" \n",
    "\n",
    "# quarterly (seasonal quarters), contest_quarterly (contest quarters), \n",
    "# monthly, yearly, individual (return full dataframe), overall (return mean of full dataframe)\n",
    "# monthly_yearly (every month in every year), quarterly_yearly (every quarter in every year)\n",
    "period = \"overall\" # <- must be overall to merge with rodeo dataframe\n",
    "metric = \"skill\" # rmse, skill, score\n",
    "table_models = main_experiment_models #rodeo_main_models # rodeo_appendix_models (all models), rodeo_main_models (toolkit models)\n",
    "relative_to = None # compute value relative to climatology value: 1 - metric(model)/metric(climatology) \n",
    "dropna = True # if true, compute average metrics only on dates where predictions have all values \n",
    "task_ids = us_gt_ids # contest_gt_ids (for contest), us_gt_ids (for us), gt_ids (for all)\n",
    "horizons = horizons\n",
    "region = 'us' #either us, east or contest\n",
    "include_overall = True # include overall row in the dataframe\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "End experiment parameters \n",
    "\"\"\"\n",
    "if (metric is 'rmse' and relative_to is not None) or (metric is not 'rmse' and relative_to is None):\n",
    "    highlight_func = highlight_max\n",
    "    bold_func = bold_max\n",
    "else:\n",
    "    highlight_func = highlight_min\n",
    "    bold_func = bold_min\n",
    "\n",
    "    \n",
    "    \n",
    "# Include overall performance?\n",
    "include_overall = True\n",
    "\n",
    "# Get set of tasks \n",
    "tasks = [f\"{gt_id}_{horizon}\" for (gt_id, horizon) in product(task_ids, horizons)]\n",
    "\n",
    "metrics_sub = None\n",
    "\n",
    "# Generate metrics dataframe for each task\n",
    "for i, (gt_id, horizon) in enumerate(product(task_ids, horizons)):\n",
    "    task = f\"{gt_id}_{horizon}\"\n",
    "    print(task)\n",
    "    \n",
    "    \n",
    "    # Read in metrics and reset table_models based on avaliable metrics\n",
    "    m_sub, table_models = get_per_period_metrics_df(\n",
    "        all_metrics, period=period, gt_id=gt_id, horizon=horizon,\n",
    "        metric=metric, target_dates=target_dates, \n",
    "        relative_to=relative_to,\n",
    "        model_names=table_models, include_overall=include_overall, dropna=dropna)\n",
    "    \n",
    "    \n",
    "   # Create metrics dataframe template\n",
    "    if period == \"overall\":\n",
    "        index = pd.Index([task], name=\"task\")            \n",
    "    else:\n",
    "        index = pd.MultiIndex.from_product(\n",
    "            [[task], m_sub.index], \n",
    "            names=('task', 'period'))   \n",
    "        \n",
    "    # Need to form task-by-task, since some tasks are missing target dates, so index differs\n",
    "    if metrics_sub is None:\n",
    "        metrics_sub = pd.DataFrame(index=index, columns=table_models)           \n",
    "    else:\n",
    "        metrics_sub = pd.concat([metrics_sub, pd.DataFrame(index=index, columns=table_models)])\n",
    "        \n",
    "    if period == \"overall\":\n",
    "        metrics_sub.loc[task, :] = m_sub\n",
    "    else:\n",
    "        metrics_sub[metrics_sub.index.get_level_values(\"task\") == task] = m_sub.values\n",
    "\n",
    "print(f\"{metric} - {target_dates}\");\n",
    "\n",
    "metrics_sub = metrics_sub[[m for m in main_experiment_models if m in table_models]]\n",
    "\n",
    "if period is 'overall':\n",
    "    metrics_sub=metrics_sub.rename(us_tasks, axis=0).T\n",
    "    metrics_sub['model_type'] = [all_model_types[m] for m in metrics_sub.index]\n",
    "    metrics_sub = metrics_sub.rename(all_model_names, axis=0).reset_index().set_index(['model_type', 'index'])\n",
    "    metrics_sub.columns = metrics_sub.columns.get_level_values(0)\n",
    "    metrics_sub = metrics_sub.apply(lambda x: 100*x)#T\n",
    "    #print(metrics_sub.round(2))\n",
    "    display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "    #for group in ['Baselines', 'Toolkit', 'Learning', 'Ensembles']:\n",
    "        #display(metrics_sub.loc[group].style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "\n",
    "else:\n",
    "    metrics_sub = metrics_sub.reindex([m for m in main_experiment_models if m in metrics_sub.columns], axis=1).T\n",
    "    display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "\n",
    "#save dataframe in latex table format\n",
    "table_to_tex(metrics_sub.astype(float), out_dir, f\"table_{region}_{period}_{metric}_over_{relative_to}_{target_dates}\", precision=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 2: \n",
    "### ECMWF U.S. 2011-2020: % improvement over deb. CFSv2\n",
    "This code produces tables to analyze model performance over differnt periods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Paper experiments; can be configured to generate metrics for any subset of models, averaged over\n",
    "a set of periods, for a given target data. Produces a table and saves to tex. \n",
    "\"\"\"\n",
    "\n",
    "target_dates = \"std_ecmwf\" \n",
    "\n",
    "# quarterly (seasonal quarters), contest_quarterly (contest quarters), \n",
    "# monthly, yearly, individual (return full dataframe), overall (return mean of full dataframe)\n",
    "# monthly_yearly (every month in every year), quarterly_yearly (every quarter in every year)\n",
    "period = \"overall\" # <- must be overall to merge with rodeo dataframe\n",
    "metric = \"rmse\" # rmse, skill, score\n",
    "table_models = ecmwf_experiment_models #rodeo_main_models # rodeo_appendix_models (all models), rodeo_main_models (toolkit models)\n",
    "relative_to = 'deb_cfsv2' # compute value relative to climatology value: 1 - metric(model)/metric(climatology) \n",
    "dropna = False # if true, compute average metrics only on dates where predictions have all values \n",
    "task_ids = us_1_5_gt_ids # contest_gt_ids (for contest), us_gt_ids (for us), gt_ids (for all)\n",
    "task_horizons = horizons\n",
    "region = 'us_1.5x1.5' #either us, east or contest\n",
    "include_overall = True # include overall row in the dataframe\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "End experiment parameters \n",
    "\"\"\"\n",
    "if (metric is 'rmse' and relative_to is not None) or (metric is not 'rmse' and relative_to is None):\n",
    "    highlight_func = highlight_max\n",
    "    bold_func = bold_max\n",
    "else:\n",
    "    highlight_func = highlight_min\n",
    "    bold_func = bold_min\n",
    "\n",
    "    \n",
    "    \n",
    "# Include overall performance?\n",
    "include_overall = True\n",
    "\n",
    "# Get set of tasks \n",
    "tasks = [f\"{gt_id}_{horizon}\" for (gt_id, horizon) in product(task_ids, task_horizons)]\n",
    "\n",
    "metrics_sub = None\n",
    "\n",
    "# Generate metrics dataframe for each task\n",
    "for i, (gt_id, horizon) in enumerate(product(task_ids, task_horizons)):\n",
    "    task = f\"{gt_id}_{horizon}\"\n",
    "    print(task)\n",
    "    \n",
    "    \n",
    "    # Read in metrics and reset table_models based on avaliable metrics\n",
    "    m_sub, table_models = get_per_period_metrics_df(\n",
    "        all_metrics, period=period, gt_id=gt_id, horizon=horizon,\n",
    "        metric=metric, target_dates=target_dates, \n",
    "        relative_to=relative_to,\n",
    "        model_names=table_models, include_overall=include_overall, dropna=dropna)\n",
    "\n",
    "    \n",
    "   # Create metrics dataframe template\n",
    "    if period == \"overall\":\n",
    "        index = pd.Index([task], name=\"task\")            \n",
    "    else:\n",
    "        index = pd.MultiIndex.from_product(\n",
    "            [[task], m_sub.index], \n",
    "            names=('task', 'period'))   \n",
    "        \n",
    "    # Need to form task-by-task, since some tasks are missing target dates, so index differs\n",
    "    if metrics_sub is None:\n",
    "        metrics_sub = pd.DataFrame(index=index, columns=table_models)           \n",
    "    else:\n",
    "        metrics_sub = pd.concat([metrics_sub, pd.DataFrame(index=index, columns=table_models)])\n",
    "     \n",
    "    \n",
    "#    print(metrics_sub)\n",
    "    \n",
    "    if period == \"overall\":\n",
    "        metrics_sub.loc[task, :] = m_sub\n",
    "    else:\n",
    "        metrics_sub[metrics_sub.index.get_level_values(\"task\") == task] = m_sub.values\n",
    "\n",
    "print(f\"{metric} - {target_dates}\");\n",
    "\n",
    "metrics_sub = metrics_sub[[m for m in ecmwf_experiment_models if 'ecmwf' not in m]+[m for m in metrics_sub.columns if 'ecmwf' in m]] \n",
    "\n",
    "\n",
    "\n",
    "if period is 'overall':\n",
    "    metrics_sub=metrics_sub.rename(us_tasks, axis=0).T\n",
    "    metrics_sub['model_type'] = [all_model_types[m] for m in metrics_sub.index]\n",
    "    metrics_sub = metrics_sub.rename(all_model_names, axis=0).reset_index().set_index(['model_type', 'index'])\n",
    "    metrics_sub.columns = metrics_sub.columns.get_level_values(0)\n",
    "    #metrics_sub = metrics_sub.apply(lambda x: 100*x)#T\n",
    "    display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "    #for group in ['Baselines', 'Toolkit', 'ECMWF', 'Ensembles']:\n",
    "    #    display(metrics_sub.loc[group].style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "else:\n",
    "    metrics_sub = metrics_sub.reindex([m for m in ecmwf_experiment_models if m in metrics_sub.columns], axis=1).T\n",
    "    display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "\n",
    "#save dataframe in latex table format\n",
    "table_to_tex(metrics_sub.astype(float), out_dir, f\"table_{region}_{period}_{metric}_over_{relative_to}_{target_dates}\", precision=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 2:\n",
    "### ECMWF U.S. 2011-2020: % skill\n",
    "This code produces tables to analyze model performance over differnt periods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Paper experiments; can be configured to generate metrics for any subset of models, averaged over\n",
    "a set of periods, for a given target data. Produces a table and saves to tex. \n",
    "\"\"\"\n",
    "\n",
    "target_dates = \"std_ecmwf\" \n",
    "\n",
    "# quarterly (seasonal quarters), contest_quarterly (contest quarters), \n",
    "# monthly, yearly, individual (return full dataframe), overall (return mean of full dataframe)\n",
    "# monthly_yearly (every month in every year), quarterly_yearly (every quarter in every year)\n",
    "period = \"overall\" # <- must be overall to merge with rodeo dataframe\n",
    "metric = \"skill\" # rmse, skill, score\n",
    "table_models = ecmwf_experiment_models #rodeo_main_models # rodeo_appendix_models (all models), rodeo_main_models (toolkit models)\n",
    "relative_to = None # compute value relative to climatology value: 1 - metric(model)/metric(climatology) \n",
    "dropna = True # if true, compute average metrics only on dates where predictions have all values \n",
    "task_ids = us_1_5_gt_ids # contest_gt_ids (for contest), us_gt_ids (for us), gt_ids (for all)\n",
    "task_horizons = horizons\n",
    "region = 'us_1.5x1.5' #either us, east or contest\n",
    "include_overall = True # include overall row in the dataframe\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "End experiment parameters \n",
    "\"\"\"\n",
    "if (metric is 'rmse' and relative_to is not None) or (metric is not 'rmse' and relative_to is None):\n",
    "    highlight_func = highlight_max\n",
    "    bold_func = bold_max\n",
    "else:\n",
    "    highlight_func = highlight_min\n",
    "    bold_func = bold_min\n",
    "\n",
    "    \n",
    "    \n",
    "# Include overall performance?\n",
    "include_overall = True\n",
    "\n",
    "# Get set of tasks \n",
    "tasks = [f\"{gt_id}_{horizon}\" for (gt_id, horizon) in product(task_ids, task_horizons)]\n",
    "\n",
    "metrics_sub = None\n",
    "\n",
    "# Generate metrics dataframe for each task\n",
    "for i, (gt_id, horizon) in enumerate(product(task_ids, task_horizons)):\n",
    "    task = f\"{gt_id}_{horizon}\"\n",
    "    print(task)\n",
    "    \n",
    "    \n",
    "    # Read in metrics and reset table_models based on avaliable metrics\n",
    "    m_sub, table_models = get_per_period_metrics_df(\n",
    "        all_metrics, period=period, gt_id=gt_id, horizon=horizon,\n",
    "        metric=metric, target_dates=target_dates, \n",
    "        relative_to=relative_to,\n",
    "        model_names=table_models, include_overall=include_overall, dropna=dropna)\n",
    "    \n",
    "    \n",
    "   # Create metrics dataframe template\n",
    "    if period == \"overall\":\n",
    "        index = pd.Index([task], name=\"task\")            \n",
    "    else:\n",
    "        index = pd.MultiIndex.from_product(\n",
    "            [[task], m_sub.index], \n",
    "            names=('task', 'period'))   \n",
    "        \n",
    "    # Need to form task-by-task, since some tasks are missing target dates, so index differs\n",
    "    if metrics_sub is None:\n",
    "        metrics_sub = pd.DataFrame(index=index, columns=table_models)           \n",
    "    else:\n",
    "        metrics_sub = pd.concat([metrics_sub, pd.DataFrame(index=index, columns=table_models)])\n",
    "        \n",
    "    if period == \"overall\":\n",
    "        metrics_sub.loc[task, :] = m_sub\n",
    "    else:\n",
    "        metrics_sub[metrics_sub.index.get_level_values(\"task\") == task] = m_sub.values\n",
    "\n",
    "print(f\"{metric} - {target_dates}\");\n",
    "\n",
    "metrics_sub = metrics_sub[[m for m in ecmwf_experiment_models if 'ecmwf' not in m and 'climatology' not in m]+[m for m in metrics_sub.columns if 'ecmwf' in m]] \n",
    "\n",
    "\n",
    "if period is 'overall':\n",
    "    metrics_sub=metrics_sub.rename(us_tasks, axis=0).T\n",
    "    metrics_sub['model_type'] = [all_model_types[m] for m in metrics_sub.index]\n",
    "    metrics_sub = metrics_sub.rename(all_model_names, axis=0).reset_index().set_index(['model_type', 'index'])\n",
    "    metrics_sub.columns = metrics_sub.columns.get_level_values(0)\n",
    "    metrics_sub = metrics_sub.apply(lambda x: 100*x)\n",
    "    display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "    #for group in ['Baselines', 'Toolkit', 'ECMWF', 'Ensembles']:\n",
    "    #    display(metrics_sub.loc[group].style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "\n",
    "else:\n",
    "    metrics_sub = metrics_sub.reindex([m for m in ecmwf_experiment_models if m in metrics_sub.columns], axis=1).T\n",
    "    display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "\n",
    "#save dataframe in latex table format\n",
    "table_to_tex(metrics_sub.astype(float), out_dir, f\"table_{region}_{period}_{metric}_over_{relative_to}_{target_dates}\", precision=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table C1, C2:  \n",
    "### %RMSE improv. per year\n",
    "This code produces tables to analyze model performance over differnt periods for a single task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Paper experiments; can be configured to generate metrics for any subset of models, averaged over\n",
    "a set of periods, for a given target data. Produces a table and saves to tex. \n",
    "\"\"\"\n",
    "\n",
    "target_dates = \"std_paper\" \n",
    "period = \"yearly\" # <- must be overall to merge with rodeo dataframe\n",
    "metric = \"rmse\"\n",
    "table_models = main_experiment_models \n",
    "relative_to = 'deb_cfsv2' # compute value relative to climatology value: 1 - metric(model)/metric(climatology) \n",
    "dropna = True # if true, compute average metrics only on dates where predictions have all values \n",
    "task_ids = us_gt_ids \n",
    "task_horizons = horizons\n",
    "region = 'us' \n",
    "include_overall = False # include overall row in the dataframe\n",
    "\n",
    "\"\"\"\n",
    "End experiment parameters \n",
    "\"\"\"\n",
    "if (metric is 'skill') or (metric is 'rmse' and relative_to is not None) or (metric is not 'rmse' and relative_to is None):\n",
    "    highlight_func = highlight_max\n",
    "    bold_func = bold_max\n",
    "else:\n",
    "    highlight_func = highlight_min\n",
    "    bold_func = bold_min\n",
    "    \n",
    "    \n",
    "# Include overall performance?\n",
    "include_overall = True\n",
    "\n",
    "# Get set of tasks \n",
    "tasks = [f\"{gt_id}_{horizon}\" for (gt_id, horizon) in product(task_ids, horizons)]\n",
    "\n",
    "\n",
    "\n",
    "# Generate metrics dataframe for each task\n",
    "for i, (gt_id, horizon) in enumerate(product(task_ids, task_horizons)):\n",
    "    task = f\"{gt_id}_{horizon}\"\n",
    "    print(task)\n",
    "    print(metric)\n",
    "    \n",
    "    metrics_sub = None\n",
    "    \n",
    "    # Read in metrics and reset table_models based on avaliable metrics\n",
    "    m_sub, table_models = get_per_period_metrics_df(\n",
    "        all_metrics, period=period, gt_id=gt_id, horizon=horizon,\n",
    "        metric=metric, target_dates=target_dates, \n",
    "        relative_to=relative_to,\n",
    "        model_names=table_models, include_overall=include_overall, dropna=dropna)\n",
    "    \n",
    "   # Create metrics dataframe template\n",
    "    if period == \"overall\":\n",
    "        index = pd.Index([task], name=\"task\")            \n",
    "    else:\n",
    "        index = pd.MultiIndex.from_product(\n",
    "            [[task], m_sub.index], \n",
    "            names=('task', 'period'))   \n",
    "        \n",
    "    # Need to form task-by-task, since some tasks are missing target dates, so index differs\n",
    "    if metrics_sub is None:\n",
    "        metrics_sub = pd.DataFrame(index=index, columns=table_models)           \n",
    "    else:\n",
    "        metrics_sub = pd.concat([metrics_sub, pd.DataFrame(index=index, columns=table_models)])\n",
    "        \n",
    "    if period == \"overall\":\n",
    "        metrics_sub.loc[task, :] = m_sub\n",
    "    else:\n",
    "        metrics_sub[metrics_sub.index.get_level_values(\"task\") == task] = m_sub.values\n",
    "\n",
    "    print(f\"{target_dates}\");\n",
    "    #if relative_to is not None:\n",
    "    #   metrics_sub.drop(relative_to, axis=1, inplace=True)\n",
    "\n",
    "    metrics_sub = metrics_sub[[m for m in main_experiment_models if m in table_models]]\n",
    "    if period is 'overall':\n",
    "        metrics_sub=metrics_sub.rename(us_tasks, axis=0).T\n",
    "        metrics_sub['model_type'] = [all_model_types[m] for m in metrics_sub.index]\n",
    "        metrics_sub = metrics_sub.rename(all_model_names, axis=0).reset_index().set_index(['model_type', 'index'])\n",
    "        metrics_sub.columns = metrics_sub.columns.get_level_values(0)\n",
    "        display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "    else:\n",
    "        metrics_sub = metrics_sub.reindex([m for m in main_experiment_models if m in metrics_sub.columns], axis=1).reset_index().drop('task', axis=1).set_index('period').T    \n",
    "        metrics_sub['model_type'] = [all_model_types[m] for m in metrics_sub.index]\n",
    "        metrics_sub = metrics_sub.rename(all_model_names, axis=0).reset_index()\n",
    "        metrics_sub.set_index(['model_type', 'index'], inplace=True)\n",
    "        metrics_sub.columns = metrics_sub.columns.get_level_values(0)\n",
    "        metrics_sub = metrics_sub\n",
    "        #for group in ['Baselines', 'Toolkit', 'Learning', 'Ensembles']:\n",
    "         #   display(metrics_sub.loc[group].style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "        display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "    #save dataframe in latex table format\n",
    "    table_to_tex(metrics_sub.astype(float).round(2), out_dir, f\"table_{task}_{period}_{metric}_over_{relative_to}_{target_dates}\", precision=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table C3, C4: \n",
    "### %SKILL per year\n",
    "This code produces tables to analyze model performance over differnt periods for a single task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Paper experiments; can be configured to generate metrics for any subset of models, averaged over\n",
    "a set of periods, for a given target data. Produces a table and saves to tex. \n",
    "\"\"\"\n",
    "\n",
    "target_dates = \"std_paper\" \n",
    "period = \"yearly\" # <- must be overall to merge with rodeo dataframe\n",
    "metric = \"skill\" \n",
    "table_models = main_experiment_models \n",
    "relative_to = None \n",
    "dropna = True # if true, compute average metrics only on dates where predictions have all values \n",
    "task_ids = us_gt_ids \n",
    "task_horizons = horizons\n",
    "region = 'us' \n",
    "include_overall = False # include overall row in the dataframe\n",
    "\n",
    "\"\"\"\n",
    "End experiment parameters \n",
    "\"\"\"\n",
    "if (metric is 'skill') or (metric is 'rmse' and relative_to is not None) or (metric is not 'rmse' and relative_to is None):\n",
    "    highlight_func = highlight_max\n",
    "    bold_func = bold_max\n",
    "else:\n",
    "    highlight_func = highlight_min\n",
    "    bold_func = bold_min\n",
    "    \n",
    "    \n",
    "# Include overall performance?\n",
    "include_overall = True\n",
    "\n",
    "# Get set of tasks \n",
    "tasks = [f\"{gt_id}_{horizon}\" for (gt_id, horizon) in product(task_ids, horizons)]\n",
    "\n",
    "\n",
    "\n",
    "# Generate metrics dataframe for each task\n",
    "for i, (gt_id, horizon) in enumerate(product(task_ids, task_horizons)):\n",
    "    task = f\"{gt_id}_{horizon}\"\n",
    "    print(task)\n",
    "    print(metric)\n",
    "    \n",
    "    metrics_sub = None\n",
    "    \n",
    "    # Read in metrics and reset table_models based on avaliable metrics\n",
    "    m_sub, table_models = get_per_period_metrics_df(\n",
    "        all_metrics, period=period, gt_id=gt_id, horizon=horizon,\n",
    "        metric=metric, target_dates=target_dates, \n",
    "        relative_to=relative_to,\n",
    "        model_names=table_models, include_overall=include_overall, dropna=dropna)\n",
    "    \n",
    "   # Create metrics dataframe template\n",
    "    if period == \"overall\":\n",
    "        index = pd.Index([task], name=\"task\")            \n",
    "    else:\n",
    "        index = pd.MultiIndex.from_product(\n",
    "            [[task], m_sub.index], \n",
    "            names=('task', 'period'))   \n",
    "        \n",
    "    # Need to form task-by-task, since some tasks are missing target dates, so index differs\n",
    "    if metrics_sub is None:\n",
    "        metrics_sub = pd.DataFrame(index=index, columns=table_models)           \n",
    "    else:\n",
    "        metrics_sub = pd.concat([metrics_sub, pd.DataFrame(index=index, columns=table_models)])\n",
    "        \n",
    "    if period == \"overall\":\n",
    "        metrics_sub.loc[task, :] = m_sub\n",
    "    else:\n",
    "        metrics_sub[metrics_sub.index.get_level_values(\"task\") == task] = m_sub.values\n",
    "\n",
    "    print(f\"{target_dates}\");\n",
    "    #if relative_to is not None:\n",
    "    #    metrics_sub.drop(relative_to, axis=1, inplace=True)\n",
    "\n",
    "    metrics_sub = metrics_sub[[m for m in main_experiment_models if m in table_models]]\n",
    "    if period is 'overall':\n",
    "        metrics_sub=metrics_sub.rename(us_tasks, axis=0).T\n",
    "        metrics_sub['model_type'] = [all_model_types[m] for m in metrics_sub.index]\n",
    "        metrics_sub = metrics_sub.rename(all_model_names, axis=0).reset_index().set_index(['model_type', 'index'])\n",
    "        metrics_sub.columns = metrics_sub.columns.get_level_values(0)\n",
    "        display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "    else:\n",
    "        metrics_sub = metrics_sub.reindex([m for m in main_experiment_models if m in metrics_sub.columns], axis=1).reset_index().drop('task', axis=1).set_index('period').T    \n",
    "        metrics_sub['model_type'] = [all_model_types[m] for m in metrics_sub.index]\n",
    "        metrics_sub = metrics_sub.rename(all_model_names, axis=0).reset_index()\n",
    "        metrics_sub.set_index(['model_type', 'index'], inplace=True)\n",
    "        metrics_sub.columns = metrics_sub.columns.get_level_values(0)\n",
    "        metrics_sub = metrics_sub#.T\n",
    "        metrics_sub = metrics_sub.apply(lambda x: 100*x)\n",
    "        #print(metrics_sub.round(2))\n",
    "        #for group in ['Baselines', 'Toolkit', 'Learning', 'Ensembles']:\n",
    "         #   display(metrics_sub.loc[group].style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "        display(metrics_sub.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "    #save dataframe in latex table format\n",
    "    table_to_tex(metrics_sub.astype(float).round(2), out_dir, f\"table_{task}_{period}_{metric}_over_{relative_to}_{target_dates}\", precision=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table C5:\n",
    "### Rodeo II Analysis - Table relative to Topcoder CFSv2\n",
    "This code produces tables for model performance and merges with Rodeo performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Rodeo experiment tables\n",
    "\"\"\"\n",
    "target_dates = \"std_contest\" \n",
    "period = \"overall\" # <- must be overall to merge with rodeo dataframe\n",
    "metric = \"rmse\" \n",
    "table_models = rodeo_experiment_models\n",
    "relative_to = 'TC_CFSv2'\n",
    "dropna = True # if true, compute average metrics only on dates where predictions have all values \n",
    "tasks = contest_tasks\n",
    "region = 'contest'\n",
    "include_overall = True # include overall row in the dataframe\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "End experiment parameters \n",
    "\"\"\"\n",
    "if (metric is 'rmse' and relative_to is not None) or (metric is not 'rmse' and relative_to is None):\n",
    "    highlight_func = highlight_max\n",
    "    bold_func = bold_max\n",
    "else:\n",
    "    highlight_func = highlight_min\n",
    "    bold_func = bold_min\n",
    "\n",
    "\n",
    "# Create metrics dataframe template\n",
    "metric_tasks = pd.DataFrame(index=tasks, columns=table_models)\n",
    "\n",
    "# Get Topcoder CFSv2 baseline \n",
    "baseline_TC_df = get_leaderboard(metric, drop_columns=['Mouatadid'])[relative_to]\n",
    "\n",
    "\n",
    "# Generate metrics dataframe for each task \n",
    "for gt_id, horizon in product(contest_gt_ids, horizons):\n",
    "    task = f\"{gt_id}_{horizon}\"\n",
    "    \n",
    "    # Read in metrics and reset table_models based on avaliable metrics\n",
    "    metric_tasks.loc[task], table_models = get_per_period_metrics_df(\n",
    "        all_metrics, period=period, gt_id=gt_id, horizon=horizon,\n",
    "        metric=metric, target_dates=target_dates, \n",
    "        relative_to=None,\n",
    "        model_names=table_models, include_overall=include_overall, dropna=dropna)\n",
    "    \n",
    "\n",
    "# Get leaderboard dataframe\n",
    "leaderboard = get_leaderboard(metric, \n",
    "                              relative_to=relative_to, \n",
    "                              baseline_df=baseline_TC_df,\n",
    "                              drop_columns=['Mouatadid'])\n",
    "\n",
    "\n",
    "#Calculate percentage improvement over Topcoder CFSv2 RMSE\n",
    "metric_tasks = pd.merge(baseline_TC_df, metric_tasks, left_index=True, right_index=True).astype(float)\n",
    "metric_tasks = metric_tasks.apply(partial(bss_score, metric_tasks.columns, relative_to), axis=1)\n",
    "\n",
    "# Concat metric dataframe for model_names with leaderboard dataframe\n",
    "metric_tasks = pd.merge(leaderboard, metric_tasks, left_index=True, right_index=True).T.astype(float)\n",
    "metric_tasks = metric_tasks[[t for t in tasks]]\n",
    "\n",
    "# Map input names to display names\n",
    "metric_tasks = metric_tasks.rename(contest_tasks, axis=1).rename(all_model_names, axis=0)\n",
    "\n",
    "print(f\"{period} {target_dates}\")\n",
    "\n",
    "if period is 'overall':\n",
    "    display(metric_tasks.style.apply(highlight_func, axis=0).apply(bold_func, axis=0).set_table_styles(styles))\n",
    "else:\n",
    "    display(metric_tasks.style.apply(highlight_func, axis=1).apply(bold_func, axis=1).set_table_styles(styles))\n",
    "\n",
    "#save dataframe in latex table format\n",
    "table_to_tex(metric_tasks.astype(float).round(2), out_dir, f\"table_{region}_{period}_{metric}_over_{relative_to}_{target_dates}\", precision=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIGURES\n",
    "## Figure 1 :  \n",
    "### % improvement: TOOLKIT vs. LEARNING MODELS\n",
    "This code produces plots to analyze model performance over differnt periods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_models = [\n",
    "    #relative_to\n",
    "    'deb_cfsv2',\n",
    "    #toolkit\n",
    "    'tuned_cfsv2pp',\n",
    "    'tuned_climpp',\n",
    "    'perpp',\n",
    "    #learner\n",
    "    'autoknn',\n",
    "    'multillr',\n",
    "    'prophet',\n",
    "    'tuned_localboosting',\n",
    "    'tuned_salient2',\n",
    "]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Paper experiments; can be configured to generate plots for any subset of models, averaged over\n",
    "a set of periods, for a given target data. Produces a figure and saves to pdf.. \n",
    "\"\"\"\n",
    "# Figure experiment parameters\n",
    "target_dates = \"std_paper\"\n",
    "task_ids = us_gt_ids \n",
    "task_horizons = horizons\n",
    "\n",
    "# RMSE improvement by season \n",
    "# Subfigure experiment parameters\n",
    "period = \"quarterly\" \n",
    "metric = 'rmse' \n",
    "relative_to = 'deb_cfsv2' # compute value relative to baseline value: 1 - metric(model)/metric(baseline)\n",
    "file_str = f\"{period}_over_{relative_to}\" # saves to file with suffix file_str\n",
    "# Generate figure\n",
    "print(target_dates)\n",
    "fh = partial(get_per_period_metrics_df, all_metrics, period, relative_to) \n",
    "plot_toolkit_vs_learner_quadruple(get_metrics_fh=fh, \n",
    "                    gt_id_list=task_ids, \n",
    "                    horizon_list=task_horizons, \n",
    "                    metric=metric, \n",
    "                    target_dates=target_dates, \n",
    "                    model_names=figure_models,\n",
    "                    file_str=file_str)\n",
    "\n",
    "# RMSE improvement by year \n",
    "# Subfigure experiment parameters\n",
    "period = \"yearly\" \n",
    "metric = 'rmse' \n",
    "relative_to = 'deb_cfsv2' # compute value relative to baseline value: 1 - metric(model)/metric(baseline)\n",
    "file_str = f\"{period}_over_{relative_to}\" # saves to file with suffix file_str\n",
    "# Generate figure\n",
    "print(target_dates)\n",
    "fh = partial(get_per_period_metrics_df, all_metrics, period, relative_to) \n",
    "plot_toolkit_vs_learner_quadruple(get_metrics_fh=fh, \n",
    "                    gt_id_list=task_ids, \n",
    "                    horizon_list=task_horizons, \n",
    "                    metric=metric, \n",
    "                    target_dates=target_dates, \n",
    "                    model_names=figure_models,\n",
    "                    file_str=file_str)\n",
    "\n",
    "# Skill improvement by season \n",
    "# Subfigure experiment parameters\n",
    "period = \"quarterly\" \n",
    "metric = 'skill' \n",
    "relative_to = None # compute value relative to baseline value: 1 - metric(model)/metric(baseline)\n",
    "file_str = f\"{period}_over_{relative_to}\" # saves to file with suffix file_str\n",
    "# Generate figure\n",
    "print(target_dates)\n",
    "fh = partial(get_per_period_metrics_df, all_metrics, period, relative_to) \n",
    "plot_toolkit_vs_learner_quadruple(get_metrics_fh=fh, \n",
    "                    gt_id_list=task_ids, \n",
    "                    horizon_list=task_horizons, \n",
    "                    metric=metric, \n",
    "                    target_dates=target_dates, \n",
    "                    model_names=figure_models,\n",
    "                    file_str=file_str)\n",
    "\n",
    "# Skill improvement by season \n",
    "# Subfigure experiment parameters\n",
    "period = \"yearly\" \n",
    "metric = 'skill' \n",
    "relative_to = None # compute value relative to baseline value: 1 - metric(model)/metric(baseline)\n",
    "file_str = f\"{period}_over_{relative_to}\" # saves to file with suffix file_str\n",
    "# Generate figure\n",
    "print(target_dates)\n",
    "fh = partial(get_per_period_metrics_df, all_metrics, period, relative_to) \n",
    "plot_toolkit_vs_learner_quadruple(get_metrics_fh=fh, \n",
    "                    gt_id_list=task_ids, \n",
    "                    horizon_list=task_horizons, \n",
    "                    metric=metric, \n",
    "                    target_dates=target_dates, \n",
    "                    model_names=figure_models,\n",
    "                    file_str=file_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure C1:  \n",
    "### Quarterly % improvement: TOOLKIT vs. BASELINES\n",
    "This code produces plots to analyze model performance over differnt periods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure experiment parameters\n",
    "target_dates = \"std_paper\" \n",
    "period = \"quarterly\" \n",
    "metric = \"rmse\" \n",
    "task_ids = us_gt_ids \n",
    "region = 'us'\n",
    "\n",
    "# RMSE improvement by season \n",
    "# Subfigure experiment parameters\n",
    "figure_models = [\n",
    "    'climatology', \n",
    "    'tuned_climpp',\n",
    "]\n",
    "relative_to = figure_models[0] \n",
    "file_str = f\"{region}_{period}_over_{relative_to}\" # saves to file with suffix file_str\n",
    "#Generate subfigure\n",
    "fh = partial(get_per_period_metrics_df, all_metrics, period, relative_to) \n",
    "plot_models_and_metrics_plus(get_metrics_fh=fh, \n",
    "                            gt_id_list=task_ids, \n",
    "                            horizon_list=horizons, \n",
    "                            metric=metric, \n",
    "                            target_dates=target_dates, \n",
    "                            model_names=figure_models, \n",
    "                            file_str=file_str)\n",
    "\n",
    "# RMSE improvement by season \n",
    "# Subfigure experiment parameters\n",
    "figure_models = [\n",
    "    'deb_cfsv2', \n",
    "    'tuned_cfsv2pp',\n",
    "]\n",
    "relative_to = figure_models[0] \n",
    "file_str = f\"{region}_{period}_over_{relative_to}\" # saves to file with suffix file_str\n",
    "#Generate subfigure\n",
    "fh = partial(get_per_period_metrics_df, all_metrics, period, relative_to) \n",
    "plot_models_and_metrics_plus(get_metrics_fh=fh, \n",
    "                            gt_id_list=task_ids, \n",
    "                            horizon_list=horizons, \n",
    "                            metric=metric, \n",
    "                            target_dates=target_dates, \n",
    "                            model_names=figure_models, \n",
    "                            file_str=file_str)\n",
    "\n",
    "# RMSE improvement by season \n",
    "# Subfigure experiment parameters\n",
    "figure_models = [\n",
    "    'persistence', \n",
    "    'perpp',\n",
    "]\n",
    "relative_to = figure_models[0] \n",
    "file_str = f\"{region}_{period}_over_{relative_to}\" # saves to file with suffix file_str\n",
    "#Generate subfigure\n",
    "fh = partial(get_per_period_metrics_df, all_metrics, period, relative_to) \n",
    "plot_models_and_metrics_plus(get_metrics_fh=fh, \n",
    "                            gt_id_list=task_ids, \n",
    "                            horizon_list=horizons, \n",
    "                            metric=metric, \n",
    "                            target_dates=target_dates, \n",
    "                            model_names=figure_models, \n",
    "                            file_str=file_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure C2:  \n",
    "### Yearly % improvement: TOOLKIT vs. BASELINES\n",
    "This code produces plots to analyze model performance over differnt periods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure experiment parameters\n",
    "target_dates = \"std_paper\" \n",
    "period = \"yearly\" \n",
    "metric = \"rmse\" \n",
    "task_ids = us_gt_ids \n",
    "region = 'us'\n",
    "\n",
    "# RMSE improvement by season \n",
    "# Subfigure experiment parameters\n",
    "figure_models = [\n",
    "    'climatology', \n",
    "    'tuned_climpp',\n",
    "]\n",
    "relative_to = figure_models[0] \n",
    "file_str = f\"{region}_{period}_over_{relative_to}\" # saves to file with suffix file_str\n",
    "#Generate subfigure\n",
    "fh = partial(get_per_period_metrics_df, all_metrics, period, relative_to) \n",
    "plot_models_and_metrics_plus(get_metrics_fh=fh, \n",
    "                            gt_id_list=task_ids, \n",
    "                            horizon_list=horizons, \n",
    "                            metric=metric, \n",
    "                            target_dates=target_dates, \n",
    "                            model_names=figure_models, \n",
    "                            file_str=file_str)\n",
    "\n",
    "# RMSE improvement by season \n",
    "# Subfigure experiment parameters\n",
    "figure_models = [\n",
    "    'deb_cfsv2', \n",
    "    'tuned_cfsv2pp',\n",
    "]\n",
    "relative_to = figure_models[0] \n",
    "file_str = f\"{region}_{period}_over_{relative_to}\" # saves to file with suffix file_str\n",
    "#Generate subfigure\n",
    "fh = partial(get_per_period_metrics_df, all_metrics, period, relative_to) \n",
    "plot_models_and_metrics_plus(get_metrics_fh=fh, \n",
    "                            gt_id_list=task_ids, \n",
    "                            horizon_list=horizons, \n",
    "                            metric=metric, \n",
    "                            target_dates=target_dates, \n",
    "                            model_names=figure_models, \n",
    "                            file_str=file_str)\n",
    "\n",
    "# RMSE improvement by season \n",
    "# Subfigure experiment parameters\n",
    "figure_models = [\n",
    "    'persistence', \n",
    "    'perpp',\n",
    "]\n",
    "relative_to = figure_models[0] \n",
    "file_str = f\"{region}_{period}_over_{relative_to}\" # saves to file with suffix file_str\n",
    "#Generate subfigure\n",
    "fh = partial(get_per_period_metrics_df, all_metrics, period, relative_to) \n",
    "plot_models_and_metrics_plus(get_metrics_fh=fh, \n",
    "                            gt_id_list=task_ids, \n",
    "                            horizon_list=horizons, \n",
    "                            metric=metric, \n",
    "                            target_dates=target_dates, \n",
    "                            model_names=figure_models, \n",
    "                            file_str=file_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICTIONS, ANOMALIES and ERRORS\n",
    "### Generate all preds, anoms and errors for all tasks and all models\n",
    "Reads predictions, generates a summary of missing data, and produces the dictionary to be used in further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate a dictionary with anomalies values for all models and every combination of gt_id, \n",
    "horizon, and target dates\n",
    "\"\"\"\n",
    "all_preds = {}\n",
    "all_anoms = {}\n",
    "all_errors = {}\n",
    "\n",
    "# Populate dictionaries for each gt_id, horizon, and target_dates for main experiment and salient experiment\n",
    "for gt_id, horizon, target_dates in \\\n",
    "            [x for x in product(us_gt_ids, horizons, ['std_paper'])] \\\n",
    "            +[x for x in product(['contest_precip'], ['34w'], ['std_paper'])]:\n",
    "            \n",
    "    #Set model names   \n",
    "    if 'us' in gt_id:\n",
    "        model_names = ecmwf_experiment_models if '1.5x1.5' in gt_id else main_experiment_models\n",
    "        model_names_str = 'ecmwf_experiment_models' if '1.5x1.5' in gt_id else 'main_experiment_models'\n",
    "    elif 'contest' in gt_id:\n",
    "        model_names = rodeo_experiment_models if 'contest' in target_dates else salient_experiment_models\n",
    "        model_names_str = 'rodeo_experiment_models' if 'contest' in target_dates else 'salient_experiment_models'\n",
    "    else:\n",
    "        model_names = all_models\n",
    "        model_names_str = 'all_models'   \n",
    "        \n",
    "    # Get task\n",
    "    task = f\"{gt_id}_{horizon}\"\n",
    "    \n",
    "    display(Markdown(f\"### {model_names_str}: {task}, {target_dates}\"))\n",
    "    \n",
    "    # Get all anoms\n",
    "    print(f\"Creating dataframes for models:\\n {model_names}\\n\")    \n",
    "    df_preds, df_anoms, df_errors = get_trio_df(gt_id=gt_id, horizon=horizon, target_dates=target_dates,\n",
    "                                              model_names=model_names)\n",
    "    print(f\"DONE!\\n\")\n",
    "    # No models exist for this task    \n",
    "    if df_preds is None: \n",
    "        continue\n",
    "    \n",
    "    # Add yearly and quarterly columns to the dataframe\n",
    "    print(f\"\\nAdding group-by columns to dataframes...\")\n",
    "    tic()\n",
    "    df_preds = add_groupby_cols(df_preds, horizon=horizon)\n",
    "    df_anoms = add_groupby_cols(df_anoms, horizon=horizon)\n",
    "    df_errors = add_groupby_cols(df_errors, horizon=horizon)\n",
    "    toc()\n",
    "    print(f\"DONE!\\n\")\n",
    "    \n",
    "    all_preds[(task, target_dates)] = copy.copy(df_preds.reset_index('start_date')) \n",
    "    all_anoms[(task, target_dates)] = copy.copy(df_anoms.reset_index('start_date')) \n",
    "    all_errors[(task, target_dates)] = copy.copy(df_errors.reset_index('start_date')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2: \n",
    "### % improvement RMSE map\n",
    "This code produces maps to analyze RMSE percentage improvment over different periods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_models = [\n",
    "    #Baselines\n",
    "    'deb_cfsv2',\n",
    "    #Toolkit\n",
    "    'tuned_climpp',\n",
    "    'tuned_cfsv2pp',\n",
    "    'perpp',\n",
    "    #Learning\n",
    "    'prophet',\n",
    "    'tuned_salient2',\n",
    "    #Ensembles  \n",
    "    'online_learning'\n",
    "]\n",
    "\n",
    "# Figure parameter values\n",
    "target_dates = 'std_paper'\n",
    "period=\"overall\"\n",
    "dropna=True\n",
    "relative_to='deb_cfsv2'\n",
    "show=True\n",
    "\n",
    "\n",
    "for gt_id, horizon in product(us_gt_ids, horizons):\n",
    "    mean_metric_df, _ = get_per_period_metrics_df(\n",
    "                                all_metrics, period=period, gt_id=gt_id, horizon=horizon,\n",
    "                                metric='rmse', target_dates=target_dates, \n",
    "                                relative_to=relative_to,\n",
    "                                model_names=figure_models, include_overall=False, dropna=dropna)\n",
    "\n",
    "    # Plot anoms for each gt_id, horizon, and target_dates\n",
    "    plot_errormaps(all_errors, \n",
    "                    period=period,\n",
    "                    gt_id = gt_id,\n",
    "                    horizon=horizon, \n",
    "                    target_dates = target_dates,\n",
    "                    model_names = figure_models,\n",
    "                    mean_metric_df = mean_metric_df,\n",
    "                    dropna=dropna,\n",
    "                    relative_to=relative_to,\n",
    "                    show=show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure C3, C4: \n",
    "### % improvement RMSE map\n",
    "This code produces maps to analyze RMSE percentage improvment over different periods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subseasonal_toolkit.utils.viz_util import *\n",
    "figure_models = [\n",
    "    #Baselines\n",
    "    'deb_cfsv2',\n",
    "    #Row 1\n",
    "    'tuned_cfsv2pp',\n",
    "    'tuned_climpp',\n",
    "    'perpp',\n",
    "    'online_learning',\n",
    "    #Row 2\n",
    "    'autoknn',\n",
    "    'climatology',\n",
    "    'persistence',\n",
    "    'linear_ensemble',  \n",
    "    #Row 3\n",
    "    'informer',\n",
    "    'tuned_localboosting',\n",
    "    'multillr',\n",
    "    'nbeats',\n",
    "    #Row 4\n",
    "    'prophet',\n",
    "    'tuned_salient2',\n",
    "]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Plot overall errors maps  for all models for std_contest\n",
    "\"\"\"\n",
    "target_dates = 'std_paper'\n",
    "period=\"overall\"\n",
    "dropna=True\n",
    "relative_to='deb_cfsv2'\n",
    "show=True\n",
    "\n",
    "\n",
    "for gt_id, horizon in product(us_gt_ids, horizons):\n",
    "    mean_metric_df, _ = get_per_period_metrics_df(\n",
    "                                all_metrics, period=period, gt_id=gt_id, horizon=horizon,\n",
    "                                metric='rmse', target_dates=target_dates, \n",
    "                                relative_to=relative_to,\n",
    "                                model_names=figure_models, include_overall=False, dropna=dropna)\n",
    "    # Plot anoms for each gt_id, horizon, and target_dates\n",
    "    plot_errormaps(all_errors, \n",
    "                    period=period,\n",
    "                    gt_id = gt_id,\n",
    "                    horizon=horizon, \n",
    "                    target_dates = target_dates,\n",
    "                    model_names = figure_models,\n",
    "                    mean_metric_df = mean_metric_df,\n",
    "                    dropna=dropna,\n",
    "                    relative_to=relative_to,\n",
    "                    show=show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure C5, C6: \n",
    "### Model bias maps\n",
    "This code produces maps to analyze model bias over different periods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_models = [\n",
    "    #Baselines\n",
    "    #Row 1\n",
    "    'tuned_cfsv2pp',\n",
    "    'tuned_climpp',\n",
    "    'perpp',\n",
    "    'online_learning',\n",
    "    #Row 2\n",
    "    'deb_cfsv2',\n",
    "    'climatology',\n",
    "    'persistence',\n",
    "    'linear_ensemble',  \n",
    "    #Row 3\n",
    "    'autoknn',\n",
    "    'informer',\n",
    "    'tuned_localboosting',\n",
    "    'multillr',\n",
    "    #Row 4\n",
    "    'nbeats',\n",
    "    'prophet',\n",
    "    'tuned_salient2',\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "Plot overall mean model bias maps  for all models for std_contest\n",
    "\"\"\"\n",
    "target_dates = \"std_paper\"\n",
    "period=\"overall\"\n",
    "include_overall=False\n",
    "dropna=True\n",
    "show=True\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "End experiment parameters \n",
    "\"\"\"\n",
    "for gt_id, horizon in product(us_gt_ids, horizons):\n",
    "    # Plot anoms for each gt_id, horizon, and target_dates\n",
    "    plot_biasmaps(all_anoms, \n",
    "                    period=period,\n",
    "                    gt_id = gt_id,\n",
    "                    horizon=horizon, \n",
    "                    target_dates = target_dates,\n",
    "                    model_names = figure_models,\n",
    "                    include_overall=include_overall,\n",
    "                    dropna=dropna,\n",
    "                    show=show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure C7:  \n",
    "### SALIENT 2.0 vs PRECIP \n",
    "This code produces plots to analyze model performance over different periods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_models = [\n",
    "    'deb_cfsv2',\n",
    "    'tuned_cfsv2pp',\n",
    "    'gt',\n",
    "    'tuned_salient2'\n",
    "]\n",
    "\n",
    "\n",
    "target_dates = \"std_paper\" \n",
    "period = \"yearly\" \n",
    "metric = \"rmse\" \n",
    "relative_to = 'deb_cfsv2' \n",
    "task_ids = ['contest_precip'] \n",
    "task_horizons = ['34w']\n",
    "file_str = f\"contest_{period}_over_{relative_to}\" \n",
    "\n",
    "\"\"\"\n",
    "End experiment parameters \n",
    "\"\"\"\n",
    "print(target_dates)\n",
    "# Plot subfigure\n",
    "plot_models_metrics_preds_line(all_metrics,\n",
    "                              all_preds,\n",
    "                              gt_id_list=task_ids, \n",
    "                              horizon_list=task_horizons, \n",
    "                              target_dates=target_dates, \n",
    "                              model_names=figure_models,\n",
    "                              period=period,\n",
    "                              relative_to=relative_to,\n",
    "                              metric=metric,\n",
    "                              file_str=file_str)\n",
    "# Plot subfigure\n",
    "plot_models_metrics_preds_scatter(all_metrics,\n",
    "                              all_preds,\n",
    "                              gt_id_list=task_ids, \n",
    "                              horizon_list=task_horizons, \n",
    "                              target_dates=target_dates, \n",
    "                              model_names=figure_models,\n",
    "                              period=period,\n",
    "                              relative_to=relative_to,\n",
    "                              metric=metric,\n",
    "                              file_str=file_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure B1, B2, B7, B8:  \n",
    "### Tuning plots\n",
    "This code produces plots to analyze submodels selected by the tuner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualize.vis_util_grl import *\n",
    "figure_models = ['tuned_climpp']\n",
    "\n",
    "# Figure parameters\n",
    "target_dates = \"std_paper\"\n",
    "task_ids = us_gt_ids\n",
    "task_horizons = horizons\n",
    "\n",
    "# Generate subfigures\n",
    "plot_tuning(gt_ids = task_ids,\n",
    "            horizons = task_horizons,\n",
    "            target_dates = target_dates,\n",
    "            model_names = figure_models)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_models = ['tuned_cfsv2pp']\n",
    "\n",
    "# Figure parameters\n",
    "target_dates = \"std_paper\"\n",
    "task_ids = us_gt_ids\n",
    "task_horizons = horizons\n",
    "\n",
    "# Generate subfigures\n",
    "plot_tuning(gt_ids = task_ids,\n",
    "            horizons = task_horizons,\n",
    "            target_dates = target_dates,\n",
    "            model_names = figure_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_models = ['tuned_localboosting']\n",
    "\n",
    "# Figure parameters\n",
    "target_dates = \"std_paper\"\n",
    "task_ids = us_gt_ids\n",
    "task_horizons = horizons\n",
    "\n",
    "# Generate subfigures\n",
    "plot_tuning(gt_ids = task_ids,\n",
    "            horizons = task_horizons,\n",
    "            target_dates = target_dates,\n",
    "            model_names = figure_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_models = ['tuned_salient2']\n",
    "\n",
    "# Figure parameters\n",
    "target_dates = \"std_paper\"\n",
    "task_ids = us_gt_ids\n",
    "task_horizons = horizons\n",
    "\n",
    "# Generate subfigures\n",
    "plot_tuning(gt_ids = task_ids,\n",
    "            horizons = task_horizons,\n",
    "            target_dates = target_dates,\n",
    "            model_names = figure_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
