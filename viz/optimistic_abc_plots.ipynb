{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohort Shapley Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure notebook is being run from base repository directory\n",
    "try:\n",
    "    %cd \"~/forecast_rodeo_ii\"\n",
    "except Exception as err:\n",
    "    print(f\"Warning: unable to change directory; {repr(err)}\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import itertools\n",
    "import importlib\n",
    "import subprocess\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import copy\n",
    "import pdb\n",
    "import calendar \n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "from subseasonal_toolkit.utils.eval_util import get_metric_filename, get_target_dates, score_to_mean_rmse, contest_quarter_start_dates, contest_quarter\n",
    "from subseasonal_toolkit.models.multillr.stepwise_util import default_stepwise_candidate_predictors\n",
    "from subseasonal_toolkit.utils.models_util import get_selected_submodel_name\n",
    "from subseasonal_data.utils import get_measurement_variable\n",
    "from subseasonal_data import data_loaders\n",
    "from subseasonal_toolkit.utils.experiments_util import clim_merge, pandas2hdf\n",
    "from subseasonal_toolkit.utils.general_util import printf, tic, toc, make_directories, set_file_permissions\n",
    "\n",
    "from cohortshapley import cohortshapley as cs\n",
    "from cohortshapley import similarity\n",
    "from cohortshapley import figure\n",
    "from cohortshapley import varianceshapley as vs\n",
    "\n",
    "# from viz_util_abc import *\n",
    "from viz_util_optimistic_abc_plots import *\n",
    "# (get_plot_params_vertical, color_dic, LinearSegmentedColormap, cmap_name, get_feature_name)\n",
    "\n",
    "import statsmodels.stats.proportion as proportion\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "%matplotlib inline\n",
    "# Inline figure display: SVG for interactive and PDF for \n",
    "# eventual outputting as PDF through nbconvert\n",
    "%config InlineBackend.figure_formats = ['pdf','svg']\n",
    "#%matplotlib notebook # interactive plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify task and models to explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Read input arguments from environment variable or specify interactively\n",
    "#\n",
    "gt_id = os.environ.get(\"COMPARE_MODELS_gt_id\", \"us_precip_1.5x1.5\")\n",
    "horizon = os.environ.get(\"COMPARE_MODELS_horizon\", \"34w\")\n",
    "target_dates = os.environ.get(\"COMPARE_MODELS_target_dates\", \"std_paper_forecast\")\n",
    "metric = os.environ.get(\"COMPARE_MODELS_metric\", \"skill\")\n",
    "task = f\"{gt_id}_{horizon}\"\n",
    "task_long = task.replace('us_precip_1.5x1.5_','precipitation').replace('us_tmp2m_1.5x1.5_','temperature').replace('34w', ', weeks 3-4').replace('56w','weeks 5-6')\n",
    "\n",
    "\n",
    "# This notebook will explain the metrics of model if model2 is None\n",
    "# and will explain the difference between model and model2 metrics\n",
    "# if model2 is not None\n",
    "model = os.environ.get(\"COMPARE_MODELS_model\", \"abc_ecmwf\")\n",
    "model2 = os.environ.get(\"COMPARE_MODELS_model2\", \"deb_ecmwf\")#\"None\")\n",
    "if model2 == \"None\":\n",
    "    model2 = None \n",
    "if model2 is None:\n",
    "    model_str = model\n",
    "else:\n",
    "    model_str = f\"{model}-vs-{model2}\"\n",
    "# Prepare figure output directories\n",
    "bin_fig_dir = os.path.join(\"subseasonal_toolkit\", \"viz\", \"bin_figs\")\n",
    "make_directories(bin_fig_dir)\n",
    "date_fig_dir = os.path.join(\"subseasonal_toolkit\", \"viz\", \"date_figs\")\n",
    "make_directories(date_fig_dir)\n",
    "\n",
    "# Identify measurement variable name\n",
    "measurement_variable = get_measurement_variable(gt_id) # 'tmp2m' or 'precip'\n",
    "gt_col = measurement_variable\n",
    "# column name for climatology\n",
    "clim_col = measurement_variable+\"_clim\"\n",
    "\n",
    "#Save figures source data here                                    \n",
    "fig_dir = os.path.join(\"subseasonal_toolkit\",\"viz\",\"abc_figures_source_data\")\n",
    "if os.path.isdir(fig_dir) is False:\n",
    "    make_directories(fig_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load outcome to be explained: model metric or model metric difference per target date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.read_hdf(get_metric_filename(model=model, gt_id=gt_id, horizon=horizon, target_dates=target_dates, metric=metric))\n",
    "metrics = metrics.set_index('start_date')\n",
    "\n",
    "if model2 is not None:\n",
    "    metrics2 = pd.read_hdf(get_metric_filename(model=model2, gt_id=gt_id, horizon=horizon, target_dates=target_dates, metric=metric))\n",
    "    metrics2 = metrics2.set_index('start_date')\n",
    "    outcome = metrics - metrics2\n",
    "else:\n",
    "    outcome = metrics\n",
    "    \n",
    "# Discard NA values\n",
    "outcome = outcome.dropna()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load explanatory features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load continuous features\n",
    "def continuous_feature_names(gt_id, horizon):\n",
    "    \"\"\"Returns a list of continuous feature names for a given gt_id and horizon\"\"\"\n",
    "    #---------------\n",
    "    # temperature, 3-4 weeks\n",
    "    if \"tmp2m\" in gt_id and horizon == \"34w\":\n",
    "        feature_names = ['mei_shift45',\n",
    "                        'sst_anom_2010_1_shift30', 'sst_anom_2010_2_shift30', 'sst_anom_2010_3_shift30',\n",
    "                        'icec_anom_2010_1_shift30', 'icec_anom_2010_2_shift30', 'icec_anom_2010_3_shift30',\n",
    "                        'hgt_10_anom_2010_1_shift30', 'hgt_10_anom_2010_2_shift30',\n",
    "                        'hgt_500_anom_2010_1_shift30', 'hgt_500_anom_2010_2_shift30']\n",
    "    #---------------\n",
    "    # temperature, 5-6 weeks\n",
    "    if \"tmp2m\" in gt_id and horizon == \"56w\":\n",
    "        feature_names = ['mei_shift59', \n",
    "                        'sst_anom_2010_1_shift44', 'sst_anom_2010_2_shift44', 'sst_anom_2010_3_shift44',\n",
    "                        'icec_anom_2010_1_shift44', 'icec_anom_2010_2_shift44', 'icec_anom_2010_3_shift44',\n",
    "                        'hgt_10_anom_2010_1_shift44', 'hgt_10_anom_2010_2_shift44',\n",
    "                        'hgt_500_anom_2010_1_shift44', 'hgt_500_anom_2010_2_shift44']\n",
    "    #---------------\n",
    "    # precipitation, 3-4 weeks\n",
    "    if \"precip\" in gt_id and horizon == \"34w\":\n",
    "        feature_names = ['mei_shift45',\n",
    "                        'sst_anom_2010_1_shift30', 'sst_anom_2010_2_shift30', 'sst_anom_2010_3_shift30',\n",
    "                        'icec_anom_2010_1_shift30', 'icec_anom_2010_2_shift30', 'icec_anom_2010_3_shift30',\n",
    "                        'hgt_10_anom_2010_1_shift30', 'hgt_10_anom_2010_2_shift30',\n",
    "                        'hgt_500_anom_2010_1_shift30', 'hgt_500_anom_2010_2_shift30']\n",
    "    #---------------\n",
    "    # precipitation, 5-6 weeks\n",
    "    if \"precip\" in gt_id and horizon == \"56w\":\n",
    "        feature_names = ['mei_shift59',  \n",
    "                        'sst_anom_2010_1_shift44', 'sst_anom_2010_2_shift44', 'sst_anom_2010_3_shift44',\n",
    "                        'icec_anom_2010_1_shift44', 'icec_anom_2010_2_shift44', 'icec_anom_2010_3_shift44',\n",
    "                        'hgt_10_anom_2010_1_shift44', 'hgt_10_anom_2010_2_shift44',\n",
    "                        'hgt_500_anom_2010_1_shift44', 'hgt_500_anom_2010_2_shift44']\n",
    "    \n",
    "    return feature_names\n",
    "\n",
    "cols_to_load = ['start_date'] + continuous_feature_names(gt_id, horizon)\n",
    "file_id = 'date_anom_data'\n",
    "continuous = data_loaders.load_combined_data(\n",
    "    file_id, gt_id.replace(\"_1.5x1.5\",\"\"), horizon, \n",
    "    columns=cols_to_load).set_index('start_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load discrete features\n",
    "def discrete_feature_names(gt_id, horizon):\n",
    "    \"\"\"Returns a list of discrete feature names for a given gt_id and horizon\"\"\"\n",
    "    if \"tmp2m\" in gt_id and horizon == \"34w\":\n",
    "        feature_names = ['phase_shift17']\n",
    "    #---------------\n",
    "    # temperature, 5-6 weeks\n",
    "    if \"tmp2m\" in gt_id and horizon == \"56w\":\n",
    "        feature_names = ['phase_shift31']\n",
    "    #---------------\n",
    "    # precipitation, 3-4 weeks\n",
    "    if \"precip\" in gt_id and horizon == \"34w\":\n",
    "        feature_names = ['phase_shift17']\n",
    "    #---------------\n",
    "    # precipitation, 5-6 weeks\n",
    "    if \"precip\" in gt_id and horizon == \"56w\":\n",
    "        feature_names = ['phase_shift31']\n",
    "    return feature_names\n",
    "\n",
    "cols_to_load = ['start_date'] + discrete_feature_names(gt_id, horizon)\n",
    "discrete = data_loaders.load_combined_data(\n",
    "    'date_data', gt_id.replace(\"_1.5x1.5\",\"\"), horizon, \n",
    "    columns=cols_to_load).set_index('start_date')\n",
    "# Add month feature\n",
    "discrete[\"month\"] = discrete.index.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge outcome and feature data to ensure common indices\n",
    "data = pd.merge(outcome, continuous, how='left', left_index=True, right_index=True)\n",
    "data = pd.merge(data, discrete, how='left', left_index=True, right_index=True)\n",
    "\n",
    "# Isolate outcome and feature components as y and X\n",
    "y = data[metric]\n",
    "X = data.loc[:, continuous.columns.append(discrete.columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess global variable importance with Shapley effects\n",
    "\n",
    "- Song et al., \"Shapley effects for global sensitivity analysis: Theory and computation\" https://epubs.siam.org/doi/abs/10.1137/15M1048070"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Shapley effects (a.k.a. Variance Shapley)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form quantile bins for continuous features\n",
    "quantiles = 10\n",
    "X_q = X.copy()\n",
    "for col in continuous.columns:\n",
    "    X_q[col] = pd.qcut(X[col],q=quantiles)\n",
    "\n",
    "tic()\n",
    "vs_values = vs.VarianceShapley(y.values, X_q.values)\n",
    "# similarity.bins = 10\n",
    "# vs_values = vs.VarianceShapley(y.values, \n",
    "#                                np.concatenate([similarity.binning(X[continuous.columns].values)[0], \n",
    "#                                                X[discrete.columns].values], axis=1))\n",
    "toc()\n",
    "\n",
    "# Store permutation that places features in decreasing order of their Shapley effect\n",
    "order=np.argsort(vs_values)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure A12.  Variable importance\n",
    "### Visualize Shapley effects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 10,\n",
    "                     'font.weight': 'bold',\n",
    "                     'figure.titlesize' : 12,\n",
    "                     'figure.titleweight': 'bold',\n",
    "                     'lines.markersize'  : 10,\n",
    "                     'xtick.labelsize'  : 10,\n",
    "                     'ytick.labelsize'  : 10})\n",
    "\n",
    "### TODO: improve title\n",
    "title = f\"{gt_id} {horizon}\"\n",
    "if model2 is None:\n",
    "    title += f\", {all_model_names[model]}\"\n",
    "else:\n",
    "    title += f\" ({all_model_names[model]} vs. {all_model_names[model2]})\"\n",
    "ylabel = 'Variable importance'\n",
    "# printf(title)\n",
    "title = title.replace('_','').replace('1.5x1.5','').replace('us','U.S.').replace('precip',' Precipitation').replace('tmp2m',' Temperature').replace('56w', ', weeks 5-6').replace('34w', ', weeks 3-4').replace('12w', ', weeks 1-2').replace(' ,', ',')\n",
    "# printf(title)\n",
    "fig=plt.figure(dpi=300)\n",
    "ax = plt.bar(X.columns[order],vs_values[order])\n",
    "plt.title(title, fontdict={'weight': 'bold'})\n",
    "plt.ylabel(ylabel, fontdict={'weight': 'bold'})\n",
    "l = [get_feature_name(l) for l in  X.columns[order].values]\n",
    "l = [l if l.startswith('month') else l[:-6] for l in l]\n",
    "plt.xticks(ticks=range(len(l)),labels=l)\n",
    "fig.autofmt_xdate(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "out_file = f\"subseasonal_toolkit/viz/shapley_effects_{title.replace(',','').replace(' ','_')}.pdf\"\n",
    "fig.savefig(out_file, bbox_inches='tight')#; fig.savefig(out_file.replace('.pdf','.png'))\n",
    "printf(f'Saving {out_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Figure source data                                      \n",
    "fig_filename = os.path.join(fig_dir, \"fig_a12-variable_importance.xlsx\")    \n",
    "if os.path.isfile(fig_filename):\n",
    "    os.remove(fig_filename)\n",
    "    \n",
    "df_barplot = pd.DataFrame(columns=[\"variable\", \"importance\"])\n",
    "df_barplot[\"variable\"] = X.columns[order]\n",
    "df_barplot[\"importance\"] = vs_values[order]\n",
    "task = f\"{gt_id} {horizon}\"\n",
    "with pd.ExcelWriter(fig_filename) as writer:  \n",
    "    df_barplot.to_excel(writer, sheet_name=task, na_rep=\"NaN\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute or load Cohort Shapley impacts for local explanation\n",
    "\n",
    "- Mase et al., \"Explaining black box decisions by Shapley cohort refinement\" https://arxiv.org/pdf/1911.00467.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose similarity type\n",
    "# If quantiles is not None, replace continuous values with their quantile bins\n",
    "# and declare points similar if they match exactly\n",
    "# If quantiles is None, use similarity.similar_in_distance_cutoff with\n",
    "# specified similarity.ratio to determine similarity\n",
    "quantiles = 10\n",
    "similarity.ratio = 0.1\n",
    "if quantiles is None:\n",
    "    sim_func = similarity.similar_in_distance_cutoff\n",
    "    features = X.values\n",
    "else:\n",
    "    sim_func = similarity.similar_in_unity\n",
    "    X_q = X.copy()\n",
    "    for col in continuous.columns:\n",
    "        X_q[col] = pd.qcut(data[col],q=quantiles)\n",
    "    features = X_q.values\n",
    "\n",
    "# Initialize Cohort Shapley object\n",
    "cs_obj = cs.CohortShapley(\n",
    "    None, sim_func,\n",
    "    np.arange(len(y)), features, y=y.values, parallel=16) ##mc_num=10000) ##, parallel=4)\n",
    "\n",
    "# Prepare results directory\n",
    "if model2 is None:\n",
    "    model_str = model\n",
    "else:\n",
    "    model_str = f\"{model}-vs-{model2}\"\n",
    "results_dir = os.path.join(\"eval\", \"cohort_shapley\", model_str)\n",
    "make_directories(results_dir)\n",
    "\n",
    "# Construct results file name\n",
    "result_file = os.path.join(results_dir,f'{model_str}-{metric}-{gt_id}_{horizon}-{target_dates}')\n",
    "if quantiles is None:\n",
    "    result_file += f'-ratio{similarity.ratio}'\n",
    "else:\n",
    "    result_file += f'-q{quantiles}'\n",
    "print(result_file)    \n",
    "\n",
    "try:\n",
    "    # Load previously saved results if available\n",
    "    cs_obj.load(result_file)\n",
    "    printf(f\"Loaded Cohort Shapley results from {result_file}\")\n",
    "except:\n",
    "    # Otherwise, compute Cohort Shapley from scratch and save to disk\n",
    "    printf(f\"Computing Cohort Shapley results\")\n",
    "    cs_obj.compute_cohort_shapley()\n",
    "    printf(f\"Saving Cohort Shapley results to {result_file}\")\n",
    "    cs_obj.save(result_file)\n",
    "    # Ensure saved files have full read and write permissions\n",
    "    set_file_permissions(result_file+\".cs.npy\", mode=0o666)\n",
    "    set_file_permissions(result_file+\".cs2.npy\", mode=0o666)\n",
    "    \n",
    "# Store results as dataframe\n",
    "df_cs = pd.DataFrame(cs_obj.shapley_values, index=X.index, columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize feature impact by quantile or categorical bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plotting parameters\n",
    "plt.rcParams.update({'font.size': 16,\n",
    "                     'font.weight': 'bold',\n",
    "                     'figure.titlesize' : 16,\n",
    "                     'figure.titleweight': 'bold',\n",
    "                     'lines.markersize'  : 14,\n",
    "                     'xtick.labelsize'  : 14,\n",
    "                     'ytick.labelsize'  : 14})\n",
    "\n",
    "# Define helper functions and dictionary for plotting\n",
    "def get_viz_var(feature):\n",
    "    \"\"\"Returns the identifier of the visualization variable associated with a given feature\"\"\"\n",
    "    if feature.startswith('mei') or feature.startswith('sst'):\n",
    "        viz_var = 'us_sst_anom'\n",
    "    else:\n",
    "        viz_var = str.split(feature,'_2010')[0]\n",
    "        if viz_var.startswith('icec') or viz_var.startswith('sst'):\n",
    "            viz_var = 'us_'+viz_var\n",
    "        elif viz_var.startswith('hgt'):\n",
    "            viz_var = 'north_'+viz_var\n",
    "            \n",
    "    return 'wide_'+viz_var\n",
    "\n",
    "# Provide a description of the vizualization variables\n",
    "mean_viz_var_long = {'wide_us_sst_anom': 'Mean sea surface temperature anomalies',\n",
    "                    'wide_us_icec_anom': 'Mean sea ice concentration anomalies',\n",
    "                    'wide_north_hgt_10_anom': 'Mean 10 hPa geopotential height anomalies',\n",
    "                    'wide_north_hgt_500_anom': 'Mean 500 hPa geopotential height anomalies'\n",
    "                   }\n",
    "\n",
    "def lat_lon_mat(data):\n",
    "    \"\"\"Converts a series or dataframe with indices of the form '(gt_var, lat, lon)_shift###' \n",
    "    into a matrix with rows indexed by lat and columns by lon. \n",
    "    Add in rows corresponding to any missing lat values with NaN values.\n",
    "    \"\"\"\n",
    "    # Parse index to extract lat and lon values\n",
    "    lats = [float(str.split(tup,',')[1]) for tup in data.index]\n",
    "    # Ensure lons are in [-180,180]\n",
    "    lons = [(float(str.split(str.split(tup,',')[2],')')[0]) + 180) % 360 - 180\n",
    "            for tup in data.index]\n",
    "    # Construct lat lon matrix\n",
    "    data = pd.DataFrame({'var' : data.values, 'lat' : lats, 'lon' : lons}).set_index(['lat','lon']).squeeze().unstack('lon')\n",
    "    return data \n",
    "\n",
    "def get_impact_levels_errors(feature, cis, num_bins):\n",
    "    \"\"\"Returns the center and halflengths of the confidence intervals associated\n",
    "    with each bin of a given feature\"\"\"\n",
    "    cis = cis.to_frame() \n",
    "    ci_centers, ci_halflens = [],[]\n",
    "    for bin_num in range(num_bins):\n",
    "        ci = cis.iloc[bin_num][feature]\n",
    "        ci_center, ci_halflen = (ci[0]+ci[1])/2, (ci[1]-ci[0])/2\n",
    "        ci_centers += [round(ci_center,2)]\n",
    "        ci_halflens += [round(ci_halflen,2)]\n",
    "#         printf(f\"Decile {bin_num+1}: Probability of positive impact {ci_center:.2g}\" \n",
    "#                          \" +/- \" f\"{ci_halflen:.2g}\")\n",
    "    return ci_centers, ci_halflens\n",
    "\n",
    "def get_high_impact_bins(feature, cis, num_bins):\n",
    "    \"\"\"Returns the bins with impact probability estimates inside the confidence interval\n",
    "    of the highest impact probability estimate\"\"\"\n",
    "    # From each confidence interval, extract point estimate of probability of \n",
    "    # positive impact per feature quantile or bin\n",
    "    impact_levels, errors = get_impact_levels_errors(feature, cis, num_bins)\n",
    "    \n",
    "    # Identify the highest impact bins (those within confidence interval of bin\n",
    "    # with overall highest impact_level)\n",
    "    impact_max = max(impact_levels)\n",
    "    errors_max = errors[impact_levels.index(impact_max)]\n",
    "    high_impact_bins = cis.index[(impact_max-errors_max <= impact_levels) & \n",
    "                                 (impact_levels <= impact_max + errors_max)]\n",
    "    return high_impact_bins\n",
    "\n",
    "def get_low_impact_bins(feature, cis, num_bins):\n",
    "    \"\"\"Returns the bins with impact probability estimates inside the confidence interval\n",
    "    of the lowest impact probability estimate\"\"\"\n",
    "    # From each confidence interval, extract point estimate of probability of \n",
    "    # positive impact per feature quantile or bin\n",
    "    impact_levels, errors = get_impact_levels_errors(feature, cis, num_bins)\n",
    "    \n",
    "    # Identify the highest impact bins (those within confidence interval of bin\n",
    "    # with overall highest impact_level)\n",
    "    impact_min = min(impact_levels)\n",
    "    errors_min = errors[impact_levels.index(impact_min)]\n",
    "    low_impact_bins = cis.index[(impact_min-errors_min <= impact_levels) & \n",
    "                                 (impact_levels <= impact_min + errors_min)]\n",
    "    return low_impact_bins\n",
    "\n",
    "# def plot_metric_maps_base(metric_dfs, model_names, gt_ids, horizons, metric, target_dates, mean_metric_df=None, show=True, scale_type=\"linear\", CB_colors_customized=None, CB_minmax=[], zoom=False):\n",
    "### TODO: add function comment block\n",
    "def plot_lat_lon_mat_all(viz_df, feature, cis, num_bins, viz_var):\n",
    "    subplots_num = viz_df.shape[0]\n",
    "    params =  get_plot_params_vertical(subplots_num=subplots_num)\n",
    "    nrows, ncols = params['nrows'], params['ncols']\n",
    "\n",
    "    fig = plt.figure(figsize=(nrows*params['figsize_x'], ncols*params['figsize_y']))\n",
    "    gs = GridSpec(nrows=nrows-1, ncols=ncols, width_ratios=params['width_ratios']) #, wspace=0.15, hspace=0.15)#, bottom=0.5)\n",
    "    \n",
    "    impact_levels, errors = get_impact_levels_errors(feature, cis, num_bins)\n",
    "    impact_min, impact_max = min(impact_levels), max(impact_levels)\n",
    "    errors_min, errors_max = errors[impact_levels.index(impact_min)], errors[impact_levels.index(impact_max)]\n",
    "    cis = cis.to_frame()    \n",
    "    \n",
    "    \n",
    "    for bin_num, xy in enumerate(product(range(nrows), range(ncols))):\n",
    "        if bin_num >= subplots_num:\n",
    "            break\n",
    "        \n",
    "        \n",
    "        i = bin_num\n",
    "        x, y = xy[0], xy[1]\n",
    "        task = f'{gt_id}_{horizon}'\n",
    "        \n",
    "        data_matrix = lat_lon_mat(viz_df.iloc[bin_num])\n",
    "        if feature.startswith('icec'):\n",
    "            # Add in rows corresponding to any missing lat values with NaN values\n",
    "            data_matrix = data_matrix.reindex(\n",
    "                np.arange(data_matrix.index.min(), data_matrix.index.max()+1), fill_value = np.nan)\n",
    "            # For icec, NaN and 0 values should be treated identically\n",
    "            data_matrix[data_matrix.isna()] = 0\n",
    "        \n",
    "        # Subsample lats and lons to reduce figure size\n",
    "        if 'hgt' in viz_var:\n",
    "            subsample_factor = 1\n",
    "        elif 'sst' in viz_var:\n",
    "            subsample_factor = 4\n",
    "        else:\n",
    "            subsample_factor = 2\n",
    "        data_matrix = data_matrix.iloc[::subsample_factor, ::subsample_factor]\n",
    "        \n",
    "        ci = cis.iloc[bin_num][feature]\n",
    "        num_bins = quantiles\n",
    "        viz_var = viz_var\n",
    "        \n",
    "        # Set lats and lons\n",
    "        lats = data_matrix.index.values\n",
    "        lons = data_matrix.columns.values\n",
    "        if 'hgt' in viz_var:\n",
    "            edge_len = 2.5 * subsample_factor\n",
    "        elif 'global' in viz_var:\n",
    "            edge_len = 1.5 * subsample_factor\n",
    "        else:\n",
    "            edge_len = 1 * subsample_factor\n",
    "        lats_edges = np.asarray(list(np.arange(lats[0], lats[-1]+edge_len*2, edge_len))) - edge_len/2\n",
    "        lons_edges = np.asarray(list(np.arange(lons[0], lons[-1]+edge_len*2, edge_len))) - edge_len/2\n",
    "        lat_grid, lon_grid = np.meshgrid(lats_edges,lons_edges)\n",
    "\n",
    "#         if feature.startswith(\"sst\"):\n",
    "        if 'sst' in viz_var:\n",
    "            ax = fig.add_subplot(gs[x,y], projection=ccrs.PlateCarree(), aspect=\"auto\")\n",
    "        else:\n",
    "            ax = fig.add_subplot(gs[x,y], aspect=\"auto\")\n",
    "        \n",
    "        ax.set_facecolor('w')\n",
    "        ax.axis('off')\n",
    "\n",
    "#         ax.coastlines(linewidth=0.9, color='gray') \n",
    "        if 'sst' in viz_var:\n",
    "            ax.coastlines(linewidth=0.9, color='gray') \n",
    "            land_110m = cfeature.NaturalEarthFeature('physical', 'land', '110m',\n",
    "                                            edgecolor='face',\n",
    "                                            facecolor='white')\n",
    "\n",
    "            ax.add_feature(land_110m, edgecolor='gray')\n",
    "\n",
    "        gt_var = \"tmp2m\" if \"tmp2m\" in gt_id else \"precip\"\n",
    "\n",
    "        metric = 'skill'\n",
    "        scale_type='linear'\n",
    "        CB_colors_customized=(\n",
    "            ['white','peachpuff','green','lightskyblue','dodgerblue','blue'] if 'icec' in viz_var\n",
    "            else ['purple','blue','lightblue','white','pink','yellow','red'])\n",
    "        if 'icec' in viz_var:\n",
    "            #CB_minmax = (0, 1) # raw icec\n",
    "            #cb_skip = 1 #color_dic[(metric, gt_var, horizon)]['cb_skip']\n",
    "            max_val = 1/4\n",
    "            CB_minmax = (-max_val, max_val)\n",
    "            cb_skip = max_val\n",
    "            CB_colors_customized=['blue','dodgerblue','lightskyblue','white','pink', 'red', 'darkred']\n",
    "        elif 'sst' in viz_var:\n",
    "            CB_minmax = (-2, 2)\n",
    "            cb_skip = 1 #color_dic[(metric, gt_var, horizon)]['cb_skip']   \n",
    "            CB_colors_customized=['blue','dodgerblue','lightskyblue','white','pink', 'red', 'darkred']\n",
    "        elif 'global' in viz_var:\n",
    "            CB_minmax = (-5, 5)#30)\n",
    "            cb_skip = 1#color_dic[(metric, gt_var, horizon)]['cb_skip']   \n",
    "            CB_colors_customized=['tan','violet','yellow','green','lightskyblue','dodgerblue','blue']\n",
    "        elif 'hgt' in viz_var:\n",
    "            #CB_minmax = (29500, 31250) # raw hgt\n",
    "            #cb_skip = 500\n",
    "            max_val = max(np.abs(viz_df.min().min()), viz_df.max().max())/1.25\n",
    "            CB_minmax = (-max_val, max_val)\n",
    "            cb_skip = max_val #color_dic[(metric, gt_var, horizon)]['cb_skip']   \n",
    "            CB_colors_customized=['blue','dodgerblue','lightskyblue','white','pink', 'red', 'darkred']\n",
    "        else:\n",
    "            CB_minmax = []\n",
    "\n",
    "        if CB_minmax == []:\n",
    "            colorbar_min_value = color_dic[(metric, gt_var, horizon)]['colorbar_min_value'] \n",
    "            colorbar_max_value = color_dic[(metric, gt_var, horizon)]['colorbar_max_value'] \n",
    "        else:\n",
    "            colorbar_min_value = CB_minmax[0]\n",
    "            colorbar_max_value = CB_minmax[1]\n",
    "# \n",
    "        color_map_str = color_dic[(metric, gt_var, horizon)]['color_map_str'] \n",
    "\n",
    "\n",
    "        if CB_colors_customized is not None:\n",
    "            if CB_colors_customized == []:\n",
    "                cmap = LinearSegmentedColormap.from_list(cmap_name, color_dic[(metric, gt_var, horizon)]['CB_colors'] , N=100)\n",
    "            else:\n",
    "                #customized cmap\n",
    "                cmap = LinearSegmentedColormap.from_list(cmap_name, CB_colors_customized, N=100)\n",
    "            color_map = matplotlib.cm.get_cmap(cmap)\n",
    "            if \"sst\" in viz_var:\n",
    "                plot = ax.pcolormesh(lon_grid,lat_grid, np.transpose(data_matrix),\n",
    "                         vmin=colorbar_min_value, vmax=colorbar_max_value,\n",
    "                         cmap=color_map, rasterized=True)\n",
    "            elif \"icec\" in viz_var:\n",
    "                m = Basemap(projection='npstere',boundinglat=45,lon_0=270,resolution='c', round=True)\n",
    "                m.drawcoastlines()\n",
    "                plot = m.pcolor(lon_grid,lat_grid, np.transpose(data_matrix), vmin=colorbar_min_value, \n",
    "                                vmax=colorbar_max_value, cmap=color_map, latlon=True, rasterized=True)\n",
    "            else:\n",
    "                m = Basemap(projection='npstere',boundinglat=15,lon_0=270,resolution='c', round=True)\n",
    "                m.drawcoastlines()\n",
    "                plot = m.pcolor(lon_grid,lat_grid, np.transpose(data_matrix), vmin=colorbar_min_value, \n",
    "                                vmax=colorbar_max_value, cmap=color_map, latlon=True, rasterized=True)\n",
    "        else:\n",
    "            color_map = matplotlib.cm.get_cmap(color_map_str)      \n",
    "            if \"linear\" in scale_type:\n",
    "                plot = ax.pcolormesh(lon_grid,lat_grid, np.transpose(data_matrix),\n",
    "                                 vmin=colorbar_min_value, vmax=colorbar_max_value,\n",
    "                                 cmap=color_map)\n",
    "            elif \"symlognorm\" in scale_type:\n",
    "                plot = ax.pcolormesh(lon_grid,lat_grid, np.transpose(data_matrix),\n",
    "                                 cmap=color_map, \n",
    "                                 norm=colors.SymLogNorm(vmin=colorbar_min_value, vmax=colorbar_max_value, linthresh=0.03, base=10))\n",
    "\n",
    "        ax.tick_params(axis='both', labelsize=params['fontsize_ticks'])\n",
    "\n",
    "\n",
    "        ci_center = round((ci[0]+ci[1])/2,2)\n",
    "        ci_halflen = round((ci[1]-ci[0])/2,2)\n",
    "        if  impact_min-errors_min <= impact_levels[i] <= impact_min+errors_min:\n",
    "            ax.set_title(r\"$\\bf{Decile\\ \" + str(bin_num+1)  + \":}$\" + \n",
    "                        f\"{str(ci_center).replace('0.','.')}\" \n",
    "                        \" ${\\pm}$ \" f\"{str(ci_halflen).replace('0.','.')}\", fontsize = params['fontsize_title'], color='red')\n",
    "        elif impact_max-errors_max <= impact_levels[i] <= impact_max + errors_max:\n",
    "            ax.set_title(r\"$\\bf{Decile\\ \" + str(bin_num+1)  + \":}$\" + \n",
    "                        f\"{str(ci_center).replace('0.','.')}\" \n",
    "                        \" ${\\pm}$ \" f\"{str(ci_halflen).replace('0.','.')}\", fontsize = params['fontsize_title'], color='blue')\n",
    "        else:\n",
    "            ax.set_title(r\"$\\bf{Decile\\ \" + str(bin_num+1)  + \":}$\" + \n",
    "                        f\"{str(ci_center).replace('0.','.')}\" \n",
    "                        \" ${\\pm}$ \" f\"{str(ci_halflen).replace('0.','.')}\", fontsize = params['fontsize_title'])\n",
    "    \n",
    "    \n",
    "        \n",
    "        if x == 0:\n",
    "            fig_title = f\"Impact of {get_feature_name(feature)[:-6]} on ABC-ECMWF skill for {task_long}\"\n",
    "            fig_title = fig_title.replace('skill for','skill improvement for') if model2 is not None else fig_title\n",
    "            fig.suptitle(fig_title,\n",
    "                         fontsize = params['fontsize_suptitle'],fontweight=\"bold\",\n",
    "                         y=1.04, x=.55)\n",
    "            fig.subplots_adjust(wspace=0.025, hspace=0.25)\n",
    "        \n",
    "        #Add colorbar\n",
    "        if CB_minmax != []:\n",
    "            if  (i == ncols):#subplots_num-1):\n",
    "                #Add colorbar for weeks 3-4 and 5-6\n",
    "                cb_ax_loc = [0.92, 0.1, 0.01, 0.8] if subplots_num == 10 else [0.2, 0.08, 0.6, 0.02]\n",
    "                cb_ax = fig.add_axes(cb_ax_loc) \n",
    "                if CB_colors_customized is not None:\n",
    "                    cb = fig.colorbar(plot, cax=cb_ax, cmap=cmap, orientation='vertical')\n",
    "                else:\n",
    "                    cb = fig.colorbar(plot, cax=cb_ax, orientation='vertical')\n",
    "                cb.outline.set_edgecolor('black')\n",
    "                cb.ax.tick_params(labelsize=params['fontsize_ticks']) \n",
    "                cbar_title = mean_viz_var_long[viz_var] #+ f' per decile' #'Skill (%)' if 'skill' in metric else metric\n",
    "                if metric == 'lat_lon_error':\n",
    "                    cbar_title = 'model bias (mm)' if 'precip' in gt_id else 'model bias ($^\\degree$C)'\n",
    "\n",
    "                cb.ax.set_ylabel(cbar_title, fontsize=params['fontsize_title'], weight='bold', rotation=270, labelpad=25)\n",
    "                     \n",
    "                if \"linear\" in scale_type:  \n",
    "                    cb_ticklabels = [f'{tick}' if 'icec' in viz_var else f'{tick:.0f}' \n",
    "                                     for tick in np.arange(colorbar_min_value, colorbar_max_value+cb_skip, cb_skip)]\n",
    "                    cb.set_ticks(np.arange(colorbar_min_value, colorbar_max_value+cb_skip, cb_skip))\n",
    "                    cb.ax.set_yticklabels(cb_ticklabels, fontsize=params['fontsize_title'], \n",
    "                                          weight='bold')\n",
    "    \n",
    "    \n",
    "    #Save figure\n",
    "    out_file = os.path.join(bin_fig_dir, \n",
    "        f'{model_str}-{metric}-{gt_id}_{horizon}-{target_dates}-{feature}-perdecile.pdf')\n",
    "    plt.savefig(out_file, orientation = 'landscape', bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    # Ensure saved files have full read and write permissions\n",
    "    set_file_permissions(out_file, mode=0o666)\n",
    "    print(f\"\\nFigure saved: {out_file}\\n\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 6. Opportunistic ABC\n",
    "### Generate forecast of opportunity table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for third model: either raw or debiased\n",
    "if model2.startswith(\"raw_\"):\n",
    "    raw_model = model2\n",
    "    model3 = model2.replace(\"raw_\", \"deb_\")\n",
    "    deb_model = model3\n",
    "else:\n",
    "    deb_model = model2\n",
    "    model3 = model2.replace(\"deb_\", \"raw_\")\n",
    "    raw_model = model3\n",
    "metrics3 = pd.read_hdf(get_metric_filename(model=model3, gt_id=gt_id, horizon=horizon, target_dates=target_dates, metric=metric))\n",
    "metrics3 = metrics3.set_index('start_date')\n",
    "task = f\"{gt_id}_{horizon}\"\n",
    "\n",
    "# Merge outcome data with individual model performances\n",
    "all_metrics = pd.merge(y, metrics, how=\"left\", left_index=True, right_index=True, \n",
    "                       suffixes=('','_'+model))\n",
    "if model2 is not None:\n",
    "    all_metrics = pd.merge(all_metrics, metrics2, how=\"left\", left_index=True, right_index=True, \n",
    "                           suffixes=('','_'+model2))\n",
    "if model3 is not None:\n",
    "    all_metrics = pd.merge(all_metrics, metrics3, how=\"left\", left_index=True, right_index=True, \n",
    "                           suffixes=('','_'+model3))    \n",
    "    \n",
    "# For each forecast, identify how many explanatory features are in high-impact bins\n",
    "features = X.columns[order[:]]\n",
    "num_high_impact = pd.Series(int(0), index=all_metrics.index, dtype=int)\n",
    "\n",
    "for feature in features:\n",
    "    # Estimate 95% Wilson confidence intervals for probability of positive impact \n",
    "    # for each feature quantile / bin\n",
    "    cis = (df_cs[feature] > 0).groupby(X_q[feature]).apply(\n",
    "        lambda x: proportion.proportion_confint(x.sum(), x.size))\n",
    "    # Identify the highest impact bins\n",
    "    num_bins = len(cis)\n",
    "    high_impact_bins = get_high_impact_bins(feature, cis, num_bins)\n",
    "    # Identify which forecasts have this feature in a high-impact bin\n",
    "    num_high_impact += X_q[feature].isin(high_impact_bins)\n",
    "\n",
    "# Construct a table summarizing the forecast of opportunity benefits of\n",
    "# selectively using ABC in when num_high_impact >= k\n",
    "opportunity_high = pd.DataFrame(index=np.sort(num_high_impact.unique()))\n",
    "\n",
    "num_name = \"# High-impact features\"\n",
    "perc_name = \"% Forecasts using ABC\"\n",
    "op_name = f\"Opportunistic ABC overall {metric}\"\n",
    "abc_high_name = f\"ABC high-impact {metric}\"\n",
    "deb_high_name = f\"Deb. ECMWF high-impact {metric}\"\n",
    "#abc_low_name = f\"ABC low-impact {metric}\"\n",
    "#deb_low_name = f\"Deb. ECMWF low-impact {metric}\"\n",
    "#deb_overall_name = f\"Deb. ECMWF overall {metric}\"\n",
    "for k in opportunity_high.index:\n",
    "    opportunity_high.loc[k,num_name] = k\n",
    "    # Store percentage of dates flagged as high impact\n",
    "    which_rows = (num_high_impact) >= k\n",
    "    opportunity_high.loc[k,perc_name] = sum(which_rows)/len(num_high_impact)    \n",
    "    # Store mean model performances \n",
    "    opportunity_high.loc[k,abc_high_name] = all_metrics.loc[which_rows,metric+'_'+model].mean()\n",
    "    opportunity_high.loc[k,deb_high_name] = all_metrics.loc[which_rows,metric+'_'+deb_model].mean()\n",
    "    opportunity_high.loc[k,op_name] = (all_metrics.loc[which_rows,metric+'_'+model].sum()+\n",
    "                                                    all_metrics.loc[~which_rows,metric+'_'+deb_model].sum())/all_metrics.shape[0]\n",
    "    #opportunity_high.loc[k,deb_low_name] = all_metrics.loc[~which_rows,metric+'_'+deb_model].mean()\n",
    "    #opportunity_high.loc[k,abc_low_name] = all_metrics.loc[~which_rows,metric+'_'+model].mean()\n",
    "    #opportunity_high.loc[k,deb_overall_name] = all_metrics.loc[:,metric+'_'+deb_model].mean()\n",
    "\n",
    "print('\\033[1m'+f\"\\nForecasts of opportunity:\"+'\\033[0m'+\n",
    "      (f\" Mean {metric} of ABC and debiased ECMWF on high-impact target dates\"))\n",
    "#      (f\" Mean {metric} of opportunistic ABC versus standard ECMWF bias correction\"))\n",
    "# display(opportunity_high)\n",
    "opportunity_high_table = opportunity_high.drop(columns=op_name).style.hide_index().set_properties(**{'text-align': 'center'}).format(\n",
    "    '{:,.2%}'.format, subset=[abc_high_name,deb_high_name]).format(\n",
    "    '{:,.0f} or more'.format, subset=num_name).format(\n",
    "    '{:,.0%}'.format, subset=[perc_name])\n",
    "display(opportunity_high_table)\n",
    "\n",
    "#Save Figure source data                                      \n",
    "fig_filename = os.path.join(fig_dir, \"fig_6-windows_opportunity_abc.xlsx\")    \n",
    "if os.path.isfile(fig_filename):\n",
    "    os.remove(fig_filename)\n",
    "with pd.ExcelWriter(fig_filename) as writer:  \n",
    "    opportunity_high_table.to_excel(writer, sheet_name=f\"table_{task}\", na_rep=\"NaN\") \n",
    "# # save dataframe in latex table format\n",
    "# table_to_tex(opportunity_high.mul(100).round(2).astype(str).add(' %'), out_dir, f\"table_opportunity\", precision=2)\n",
    "\n",
    "\n",
    "# Form accompanying plot of high-impact skill and overall skill of opportunistic ABC\n",
    "plt.plot(\n",
    "    opportunity_high[num_name],\n",
    "    opportunity_high[abc_high_name],\n",
    "    label=\"ABC-ECMWF on high-impact dates\",\n",
    "    color='tab:blue', linestyle='dashed')\n",
    "plt.plot(\n",
    "    opportunity_high[num_name],\n",
    "    opportunity_high[deb_high_name],\n",
    "    label=\"Deb. ECMWF on high-impact dates\",\n",
    "    color='tab:red', linestyle='dashdot')\n",
    "plt.plot(\n",
    "    opportunity_high[num_name],\n",
    "    opportunity_high[op_name],\n",
    "    label=\"Opportunistic ABC on all dates\",\n",
    "    color='tab:green', linewidth=2)\n",
    "plt.ylabel(\"Skill\", fontsize=12, weight='bold')\n",
    "plt.xlabel(\"Minimum number of high-impact features\", fontsize=12, weight='bold')\n",
    "plt.legend(prop={\"size\":12})\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "out_file = f\"subseasonal_toolkit/viz/opportunity.pdf\"\n",
    "plt.savefig(out_file)#; fig.savefig(out_file.replace('.pdf','.png'))\n",
    "printf(f'Saving {out_file}')\n",
    "\n",
    "#save figure source data\n",
    "with pd.ExcelWriter(fig_filename, mode='a') as writer:  \n",
    "    opportunity_high.to_excel(writer, sheet_name=f\"lineplot_{task}\", na_rep=\"NaN\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify continuous explanatory features to visualize\n",
    "features = [feature for feature in X.columns[order[:]] if feature in continuous.columns]\n",
    "# print(features)\n",
    "\n",
    "for feature in [\"hgt_500_anom_2010_1_shift30\"]: #features:\n",
    "    printf(f\"Visualizing {feature}\")\n",
    "    # Estimate 95% Wilson confidence intervals for probability of positive impact \n",
    "    # for each feature quantile / bin\n",
    "    cis = (df_cs[feature] > 0).groupby(X_q[feature]).apply(\n",
    "        lambda x: proportion.proportion_confint(x.sum(), x.size))\n",
    "\n",
    "    # Identify associated visualization variable\n",
    "    viz_var = get_viz_var(feature)\n",
    "    shift = int(str.split(str.split(feature,'_')[-1],'shift')[1])\n",
    "\n",
    "    # Load visualization variable\n",
    "    tic()\n",
    "    viz_df = data_loaders.get_ground_truth(\n",
    "        viz_var, shift=shift).set_index('start_date')\n",
    "    toc()\n",
    "\n",
    "    # Restrict to relevant dates\n",
    "    viz_df = viz_df.loc[X_q.index]\n",
    "\n",
    "    # Average visualization variable by bin / quantile\n",
    "    viz_df = viz_df.groupby(X_q[feature]).mean()\n",
    "#     display(viz_df)\n",
    "    plot_lat_lon_mat_all(viz_df, feature, cis, quantiles, viz_var)\n",
    "    \n",
    "    if feature == \"hgt_500_anom_2010_1_shift30\":\n",
    "        fig_filename = os.path.join(fig_dir, \"fig_4-impact_hgt_500_pc1.xlsx\")    \n",
    "        if os.path.isfile(fig_filename):\n",
    "            os.remove(fig_filename)\n",
    "        with pd.ExcelWriter(fig_filename) as writer:  \n",
    "            viz_df.T.to_excel(writer, sheet_name=f\"binfig_{task}\", na_rep=\"NaN\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize MJO impact by phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16,\n",
    "                     'font.weight': 'bold',\n",
    "                     'figure.titlesize' : 16,\n",
    "                     'figure.titleweight': 'bold',\n",
    "                     'lines.markersize'  : 14,\n",
    "                     'xtick.labelsize'  : 14,\n",
    "                     'ytick.labelsize'  : 14})\n",
    "\n",
    "# Specify explanatory feature to visualize\n",
    "feature = 'phase_shift17'\n",
    "\n",
    "# Estimate 95% Wilson confidence intervals for probability of positive impact \n",
    "# for each feature quantile / bin\n",
    "cis_mjo = (df_cs[feature] > 0).groupby(X_q[feature]).apply(\n",
    "    lambda x: proportion.proportion_confint(x.sum(), x.size))\n",
    "num_bins = len(cis_mjo)\n",
    "\n",
    "impact_levels, errors = get_impact_levels_errors(feature, cis_mjo, num_bins)\n",
    "impact_min, impact_max = min(impact_levels), max(impact_levels)\n",
    "errors_min, errors_max = errors[impact_levels.index(impact_min)], errors[impact_levels.index(impact_max)]\n",
    "\n",
    "colors = impact_levels\n",
    "probabilities = [str(impact_level) + u\"\\u00B1\" + str(error) for impact_level, error in zip(impact_levels, errors)]\n",
    "\n",
    "triangles = {\n",
    "    \"P1\": {\"x\": (0, -2, -2, 0), \"y\": (0, -2, 0, 0), \"prob_position\": (-1.75, -0.7), \"text_position\": (-1.95, -1.65)},\n",
    "    \"P2\": {\"x\": (0, 0, -2, 0), \"y\": (0, -2, -2, 0), \"prob_position\": (-1.1, -1.5), \"text_position\": (-1.65, -1.9)},\n",
    "    \"P3\": {\"x\": (0, 0, 2, 0), \"y\": (0, -2, -2, 0), \"prob_position\": (0.25, -1.5), \"text_position\": (1.5, -1.9)},\n",
    "    \"P4\": {\"x\": (0, 2, 2, 0), \"y\": (0, -2, 0, 0), \"prob_position\": (1, -0.7), \"text_position\": (1.75, -1.65)},\n",
    "    \"P5\": {\"x\": (0, 2, 2, 0), \"y\": (0, 2, 0, 0), \"prob_position\": (1, 0.5), \"text_position\": (1.75, 1.5)},\n",
    "    \"P6\": {\"x\": (0, 0, 2, 0), \"y\": (0, 2, 2, 0), \"prob_position\": (0.25, 1.35), \"text_position\": (1.5, 1.75)},\n",
    "    \"P7\": {\"x\": (0, -2, 0, 0), \"y\": (0, 2, 2, 0), \"prob_position\": (-1.1, 1.35), \"text_position\": (-1.6, 1.75)},\n",
    "    \"P8\": {\"x\": (0, -2, -2, 0), \"y\": (0, 2, 0, 0), \"prob_position\": (-1.75, 0.5), \"text_position\": (-1.95, 1.5)},\n",
    "}\n",
    "\n",
    "fig = plt.figure('Triangles')\n",
    "fig.set_size_inches(6, 6)\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "ax = fig.add_subplot()\n",
    "plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)\n",
    "\n",
    "plt.plot([-2, 2], [2, -2], 'black', lw=2, alpha=0.4)\n",
    "plt.plot([-2, 2], [-2, 2], 'black', lw=2, alpha=0.4)\n",
    "plt.plot([-2, 2], [0, 0], 'black', lw=2, alpha=0.4)\n",
    "plt.plot([0, 0], [-2, 2], 'black', lw=2, alpha=0.4)\n",
    "# plt.plot([0, -2], [0, 2], 'black', lw=2)\n",
    "\n",
    "for i, id in enumerate(triangles):\n",
    "    if  impact_min-errors_min <= impact_levels[i] <= impact_min+errors_min:\n",
    "        plt.fill(triangles[id][\"x\"], triangles[id][\"y\"], 'darkorange', alpha=impact_levels[i])\n",
    "        ax.text(*triangles[id][\"prob_position\"], probabilities[i], fontsize=12, color=\"red\")\n",
    "        ax.text(*triangles[id][\"text_position\"], id, fontsize=12)\n",
    "    elif impact_max-errors_max <= impact_levels[i] <= impact_max + errors_max:\n",
    "        plt.fill(triangles[id][\"x\"], triangles[id][\"y\"], 'darkorange', alpha=impact_levels[i])\n",
    "        ax.text(*triangles[id][\"prob_position\"], probabilities[i], fontsize=12, color=\"blue\")\n",
    "        ax.text(*triangles[id][\"text_position\"], id, fontsize=12)\n",
    "    else:\n",
    "        plt.fill(triangles[id][\"x\"], triangles[id][\"y\"], 'darkorange', alpha=impact_levels[i])#'#1102b3', alpha=impact_levels[i])\n",
    "        ax.text(*triangles[id][\"prob_position\"], probabilities[i], fontsize=12)\n",
    "        ax.text(*triangles[id][\"text_position\"], id, fontsize=12)\n",
    "        \n",
    "ax.text(-0.72, 2.05, \"Western Pacific\")\n",
    "ax.text(2.0, -1, \" Maritime Continent\", rotation=-90)\n",
    "ax.text(-0.62, -2.16, \"Indian Ocean\")\n",
    "ax.text(-2.17, -1.1, \" West. Hem. & Africa\", rotation=90)\n",
    "\n",
    "fig_title = f\"Impact of {get_feature_name(feature)[:-6]} on ABC-ECMWF skill for {task_long}\"\n",
    "fig_title = fig_title.replace('skill for','skill improvement for') if model2 is not None else fig_title       \n",
    "plt.title(f'{fig_title}\\n', weight='bold', fontsize=14)\n",
    "\n",
    "plt.xticks([-2, -1, 0, 1, 2])\n",
    "plt.yticks([-2, -1, 0, 1, 2])\n",
    "\n",
    "ax.set_xlabel('RMM1', weight='bold')\n",
    "ax.set_ylabel('RMM2', weight='bold')\n",
    "\n",
    "#Save figure\n",
    "out_file = os.path.join(bin_fig_dir, \n",
    "    f'{model_str}-mjo-{gt_id}_{horizon}-{target_dates}-{feature}.pdf')\n",
    "plt.savefig(out_file, orientation = 'landscape', bbox_inches='tight')\n",
    "# plt.close(fig)\n",
    "# Ensure saved files have full read and write permissions\n",
    "set_file_permissions(out_file, mode=0o666)\n",
    "print(f\"\\nFigure saved: {out_file}\\n\")  \n",
    "\n",
    "#Save Figure source data                                      \n",
    "fig_filename = os.path.join(fig_dir, \"fig_5-impact_mjo_phase.xlsx\")    \n",
    "if os.path.isfile(fig_filename):\n",
    "    os.remove(fig_filename)\n",
    "with pd.ExcelWriter(fig_filename) as writer:  \n",
    "    df_cs[feature].to_excel(writer, sheet_name=f\"cs_{task}\", na_rep=\"NaN\") \n",
    "with pd.ExcelWriter(fig_filename, mode='a') as writer:  \n",
    "    cis_mjo.to_excel(writer, sheet_name=f\"ci_{task}\", na_rep=\"NaN\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize individual forecasts on which each feature had the greatest impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify dates with largest impacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X.columns[order][:]\n",
    "dates_largest_impact, dates_largest_metric = {}, {}\n",
    "\n",
    "for feature in features:\n",
    "    display(Markdown(f\"#### {feature}:\"))\n",
    "    \n",
    "    # Produce confidence interval for the probability of positive impact\n",
    "    # within each feature quantile or bin\n",
    "    cis_feature = (df_cs[feature] > 0).groupby(X_q[feature]).apply(\n",
    "        lambda x: proportion.proportion_confint(x.sum(), x.size))\n",
    "    num_bins = len(cis_feature)\n",
    "    \n",
    "    # Identify the highest impact bins (those within confidence interval of bin\n",
    "    # with overall highest impact_level)\n",
    "    high_impact_bins = get_high_impact_bins(feature, cis_feature, num_bins)\n",
    "    \n",
    "    # Identify the largest impact amongst all high impact bin forecasts\n",
    "    largest_impact = -np.inf\n",
    "    for feature_edges in high_impact_bins:        \n",
    "        # Identify dates in the highest positive impact probability bin\n",
    "        if feature in continuous:\n",
    "            df = data[feature].to_frame()\n",
    "            dmin, dmax = feature_edges.left, feature_edges.right\n",
    "            dsel = df[(df[feature]>dmin)&(df[feature]<dmax)]\n",
    "            label_str = f\"between {dmin} and {dmax}\"\n",
    "        else:\n",
    "            df = data[feature].to_frame()\n",
    "            dm= feature_edges\n",
    "            dsel = df[(df[feature]==dm)]\n",
    "            label_str = f\"{dm}\"\n",
    "\n",
    "        # Identify impacts of dates in the highest positive impact probability bin\n",
    "        dsel_cs = df_cs[feature][df_cs.index.isin(dsel.index)]\n",
    "        # Find the highest impact date in the highest positive impact probability bin\n",
    "        dsel_cs = dsel_cs.sort_values(ascending=False).iloc[:10]\n",
    "        if dsel_cs[0] > largest_impact:\n",
    "            largest_impact = dsel_cs[0]\n",
    "            dsel_date = datetime.strftime(dsel_cs.index[0],'%Y%m%d')\n",
    "            dates_largest_impact[feature] = dsel_date\n",
    "            printf(f\"Date with largest impact ({dsel_cs[0]:.2}), where {feature} is {label_str}: {dsel_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate metrics for dates with largest impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    target_ranges = sorted(list(set([d for d in dates_largest_impact.values()] + [d for d in dates_largest_metric.values()])))\n",
    "    figure_metrics = ['lat_lon_anom']#, 'lat_lon_skill'] \n",
    "    model_names='gt '\n",
    "    for m in ['ecmwf']:#,'cfsv2']:\n",
    "        if model2 is None:\n",
    "            model_names += f'abc_{m} '\n",
    "        elif model2.startswith(\"raw_\"):\n",
    "            model_names += f'raw_{m} abc_{m} '\n",
    "        elif model2.startswith(\"deb_\"):\n",
    "            model_names += f'deb_{m} abc_{m} '\n",
    "    cmd = 'rm -f jobs_metrics.out'\n",
    "    subprocess.call(cmd, shell=True)\n",
    "    for figure_metric in figure_metrics:\n",
    "        print(figure_metric)\n",
    "        for target_range in target_ranges:\n",
    "            print(target_range)\n",
    "            cmd = f\"python src/eval/bulk_batch_metrics.py -mn {model_names} -t {target_range} -m {figure_metric} >> jobs_metrics.out\"\n",
    "            print(cmd)\n",
    "            subprocess.call(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize anomalies for dates with largest impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions and dictionary for plotting\n",
    "\n",
    "# Provide a description of the vizualization variable\n",
    "viz_var_long = {'wide_us_sst_anom': 'Lagged SST anomalies',\n",
    "                'wide_us_icec_anom': 'Lagged sea ice concentration anomalies',\n",
    "                'wide_north_hgt_10_anom': 'Lagged 10 hPa HGT anomalies',# 'geopotential height anomalies',\n",
    "                'wide_north_hgt_500_anom': 'Lagged 500 hPa HGT anomalies' #\\n' 'geopotential height anomalies'\n",
    "               }\n",
    "\n",
    "\n",
    "###TODO: add comment block\n",
    "def plot_metric_maps_trio(metric_dfs, model_names, gt_ids, horizons,\n",
    "                          data_matrix, viz_df, viz_var, \n",
    "                          metric, target_dates, mean_metric_df=None, show=True, \n",
    "                          scale_type=\"linear\", CB_colors_customized=None, CB_minmax=[], zoom=False,\n",
    "                          feature='mei_shift45', bin_str=\"decile 1\"):\n",
    "    \n",
    "    # Save original settings\n",
    "    CB_colors_customized_or = CB_colors_customized\n",
    "    CB_minmax_or = CB_minmax\n",
    "    metric_or = metric\n",
    "    \n",
    "    # Format target date\n",
    "    target_dates_objs = get_target_dates(target_dates)\n",
    "    target_dates_str = datetime.strftime(target_dates_objs[0], '%Y-%m-%d')\n",
    "    \n",
    "    #Make figure with compared models plots\n",
    "    tasks = [f\"{t[0]}_{t[1]}\" for t in product(gt_ids, horizons)]\n",
    "    subplots_num = 1 + (len(model_names) * len(tasks))\n",
    "#     params =  get_plot_params_vertical(subplots_num=subplots_num)\n",
    "    params =  get_plot_params_horizontal(subplots_num=subplots_num)\n",
    "    params['fontsize_title'] += 4\n",
    "    params['fontsize_ticks'] += 4\n",
    "    params['y_sup_fontsize'] += 4\n",
    "    nrows, ncols = params['nrows'], params['ncols']\n",
    "    \n",
    "\n",
    "    #Set properties common to all subplots\n",
    "    fig = plt.figure(figsize=(nrows*params['figsize_x'], ncols*params['figsize_y']))\n",
    "    gs = GridSpec(nrows=nrows-1, ncols=ncols, width_ratios=params['width_ratios']) #, wspace=0.15, hspace=0.15)#, bottom=0.5)\n",
    "\n",
    "    \n",
    "    \n",
    "# SUBPLOT 1 *******************************************************************************************\n",
    "        \n",
    "    # Subsample lats and lons to reduce figure size\n",
    "    if 'hgt' in viz_var:\n",
    "        subsample_factor = 1\n",
    "    elif 'sst' in viz_var:\n",
    "        subsample_factor = 4\n",
    "    else:\n",
    "        subsample_factor = 2\n",
    "    data_matrix = data_matrix.iloc[::subsample_factor, ::subsample_factor]\n",
    "\n",
    "    # Set lats and lons\n",
    "    lats = data_matrix.index.values\n",
    "    lons = data_matrix.columns.values\n",
    "    if 'hgt' in viz_var:\n",
    "        edge_len = 2.5 * subsample_factor\n",
    "    elif 'global' in viz_var:\n",
    "        edge_len = 1.5 * subsample_factor\n",
    "    else:\n",
    "        edge_len = 1 * subsample_factor\n",
    "    lats_edges = np.asarray(list(np.arange(lats[0], lats[-1]+edge_len*2, edge_len))) - edge_len/2\n",
    "    lons_edges = np.asarray(list(np.arange(lons[0], lons[-1]+edge_len*2, edge_len))) - edge_len/2\n",
    "    lat_grid, lon_grid = np.meshgrid(lats_edges,lons_edges)\n",
    "    i=0\n",
    "    gt_id, horizon = gt_ids[i], horizons[i]\n",
    "    task = f'{gt_id}_{horizon}'\n",
    "    x, y = 0, 0\n",
    "\n",
    "    if 'sst' in viz_var:\n",
    "        ax = fig.add_subplot(gs[x,y], projection=ccrs.PlateCarree(), aspect=\"auto\")\n",
    "    else:\n",
    "        ax = fig.add_subplot(gs[x,y], aspect=\"auto\")\n",
    "#     ax = fig.add_subplot(gs[x,y], aspect=\"auto\")\n",
    "    ax.set_facecolor('w')\n",
    "    ax.axis('off')\n",
    "\n",
    "    if 'sst' in viz_var:\n",
    "        ax.coastlines(linewidth=0.9, color='gray') \n",
    "        ax.add_feature(cfeature.STATES.with_scale('110m'), edgecolor='gray', linewidth=0.9, linestyle=':')\n",
    "        land_110m = cfeature.NaturalEarthFeature('physical', 'land', '110m',\n",
    "                                        edgecolor='face',\n",
    "                                        facecolor='white')#cfeature.COLORS['land'])\n",
    "\n",
    "        ax.add_feature(land_110m, edgecolor='gray')\n",
    "\n",
    "    gt_var = \"tmp2m\" if \"tmp2m\" in gt_id else \"precip\"\n",
    "\n",
    "    metric = 'skill'\n",
    "    scale_type='linear'\n",
    "    CB_colors_customized=(\n",
    "        ['white','peachpuff','green','lightskyblue','dodgerblue','blue'] if 'icec' in viz_var\n",
    "        else ['purple','blue','lightblue','white','pink','yellow','red'])\n",
    "    if 'icec' in viz_var:\n",
    "        #CB_minmax = (0, 1) # raw icec\n",
    "        #cb_skip = 1 #color_dic[(metric, gt_var, horizon)]['cb_skip']\n",
    "        max_val = 1/4\n",
    "        CB_minmax = (-max_val, max_val)\n",
    "        cb_skip = max_val\n",
    "        CB_colors_customized=['blue','dodgerblue','lightskyblue','white','pink', 'red', 'darkred']\n",
    "    elif 'sst' in viz_var:\n",
    "        CB_minmax = (-2, 2)\n",
    "        cb_skip = 1 #color_dic[(metric, gt_var, horizon)]['cb_skip']   \n",
    "        CB_colors_customized=['blue','dodgerblue','lightskyblue','white','pink', 'red', 'darkred']\n",
    "    elif 'global' in viz_var:\n",
    "        CB_minmax = (-5, 5)#30)\n",
    "        cb_skip = 1#color_dic[(metric, gt_var, horizon)]['cb_skip']   \n",
    "        CB_colors_customized=['tan','violet','yellow','green','lightskyblue','dodgerblue','blue']\n",
    "    elif 'hgt' in viz_var:\n",
    "        #CB_minmax = (29500, 31250) # raw hgt\n",
    "        #cb_skip = 500\n",
    "        max_val = max(np.abs(viz_df.min().min()), viz_df.max().max())/1.25\n",
    "        CB_minmax = (-max_val, max_val)\n",
    "        cb_skip = max_val #color_dic[(metric, gt_var, horizon)]['cb_skip']   \n",
    "        CB_colors_customized=['blue','dodgerblue','lightskyblue','white','pink', 'red', 'darkred']\n",
    "    else:\n",
    "        CB_minmax = []\n",
    "\n",
    "    if CB_minmax == []:\n",
    "        colorbar_min_value = color_dic[(metric, gt_var, horizon)]['colorbar_min_value'] \n",
    "        colorbar_max_value = color_dic[(metric, gt_var, horizon)]['colorbar_max_value'] \n",
    "    else:\n",
    "        colorbar_min_value = CB_minmax[0]\n",
    "        colorbar_max_value = CB_minmax[1]\n",
    "\n",
    "    color_map_str = color_dic[(metric, gt_var, horizon)]['color_map_str'] \n",
    "\n",
    "\n",
    "    if CB_colors_customized is not None:\n",
    "        \n",
    "        if CB_colors_customized == []:\n",
    "            cmap = LinearSegmentedColormap.from_list(cmap_name, color_dic[(metric, gt_var, horizon)]['CB_colors'] , N=100)\n",
    "        else:\n",
    "            #customized cmap\n",
    "            cmap = LinearSegmentedColormap.from_list(cmap_name, CB_colors_customized, N=100)\n",
    "        color_map = matplotlib.cm.get_cmap(cmap) \n",
    "#         plot = ax.pcolormesh(lon_grid,lat_grid, np.transpose(data_matrix),\n",
    "#                      vmin=colorbar_min_value, vmax=colorbar_max_value,\n",
    "#                      cmap=color_map)\n",
    "        if 'sst' in viz_var: \n",
    "            plot = ax.pcolormesh(lon_grid,lat_grid, np.transpose(data_matrix), \n",
    "                                 vmin=colorbar_min_value, vmax=colorbar_max_value,\n",
    "                                 cmap=color_map, rasterized=True)\n",
    "        elif 'icec' in viz_var:\n",
    "            m = Basemap(projection='npstere',boundinglat=45,lon_0=270,resolution='c', round=True)\n",
    "            m.drawcoastlines()\n",
    "            plot = m.pcolor(lon_grid,lat_grid, np.transpose(data_matrix),\n",
    "                             vmin=colorbar_min_value, vmax=colorbar_max_value,\n",
    "                             cmap=color_map, latlon=True, rasterized=True)\n",
    "        else:\n",
    "            m = Basemap(projection='npstere',boundinglat=15,lon_0=270,resolution='c', round=True)\n",
    "            m.drawcoastlines()\n",
    "            plot = m.pcolor(lon_grid,lat_grid, np.transpose(data_matrix),\n",
    "                             vmin=colorbar_min_value, vmax=colorbar_max_value,\n",
    "                             cmap=color_map, latlon=True, rasterized=True)\n",
    "    else:\n",
    "        color_map = matplotlib.cm.get_cmap(color_map_str)      \n",
    "        if \"linear\" in scale_type:\n",
    "            plot = ax.pcolormesh(lon_grid,lat_grid, np.transpose(data_matrix),\n",
    "                             vmin=colorbar_min_value, vmax=colorbar_max_value,\n",
    "                             cmap=color_map)\n",
    "        elif \"symlognorm\" in scale_type:\n",
    "            plot = ax.pcolormesh(lon_grid,lat_grid, np.transpose(data_matrix),\n",
    "                             cmap=color_map, \n",
    "                             norm=colors.SymLogNorm(vmin=colorbar_min_value, vmax=colorbar_max_value, linthresh=0.03, base=10))\n",
    "\n",
    "    ax.tick_params(axis='both', labelsize=params['fontsize_ticks'])\n",
    "#     feature_name = get_feature_name(feature)\n",
    "#     if 'mei' in feature:\n",
    "#         shift = int(str.split(str.split(feature,'_')[-1],'shift')[1])\n",
    "#         feature_df = data_loaders.get_ground_truth(f'mei', shift=shift).set_index('start_date')\n",
    "#         feature_mean = feature_df.loc[target_dates_str][feature]\n",
    "#     else:\n",
    "#         ###TODO: this shouldn't be the mean of the data matrix but rather the scalar value of the feature \n",
    "#         ###(e.g., the single eof value on this date)\n",
    "#         feature_mean = str(round(data_matrix.mean().mean(),2))[:5]\n",
    "#     ylabel = f\"{feature_name} = {feature_mean}\"\n",
    "#     ax.text(-0.05, 0.50, ylabel, va='bottom', ha='center',\n",
    "#                     rotation='vertical', rotation_mode='anchor',\n",
    "#                     transform=ax.transAxes, fontsize = params['fontsize_title'], fontweight=\"bold\")\n",
    "    # Plot title below figure\n",
    "    ax.set_title(viz_var_long[viz_var], fontsize = params['fontsize_title'],fontweight=\"bold\",\n",
    "                 y=-0.2, \n",
    "                 x=.45 if 'icec' in viz_var else .5)\n",
    "\n",
    "    #Add colorbar\n",
    "    cb_shift = 0.02 if ncols == 4 else 0\n",
    "    if CB_minmax != []:\n",
    "        if  (i == 0):#subplots_num-1):\n",
    "            #Add colorbar for weeks 3-4 and 5-6\n",
    "            # first coordinate moves colorbar right as it increases\n",
    "            # second coordinate moves colorbar up as it increases\n",
    "            # third coordinate determines colorbar width\n",
    "            cb_ax_loc = [0.105, 0.16, 0.007, 0.7]\n",
    "            cb_ax = fig.add_axes(cb_ax_loc) \n",
    "            if CB_colors_customized is not None:\n",
    "                cb = fig.colorbar(plot, cax=cb_ax, cmap=cmap, orientation='vertical')\n",
    "            else:\n",
    "                cb = fig.colorbar(plot, cax=cb_ax, orientation='vertical')\n",
    "#             cb_ax = fig.add_axes([0.14-cb_shift, 0.06, 0.18, 0.04]) #fig.add_axes([0.2, 0.08, 0.6, 0.02])\n",
    "#             if CB_colors_customized is not None:\n",
    "#                 cb = fig.colorbar(plot, cax=cb_ax, cmap=cmap, orientation='horizontal')\n",
    "#             else:\n",
    "#                 cb = fig.colorbar(plot, cax=cb_ax, orientation='horizontal')\n",
    "            cb.outline.set_edgecolor('black')\n",
    "            cb.ax.tick_params(labelsize=params['fontsize_ticks']) \n",
    "            cbar_title = viz_var_long[viz_var] #+ f' per decile' #'Skill (%)' if 'skill' in metric else metric\n",
    "            if metric == 'lat_lon_error':\n",
    "                cbar_title = 'model bias (mm)' if 'precip' in gt_id else 'model bias ($^\\degree$C)'\n",
    "\n",
    "            ###cb.ax.set_xlabel(cbar_title, fontsize=params['fontsize_title'], weight='bold')\n",
    "            if \"linear\" in scale_type:\n",
    "                #cb_skip = 1#color_dic[(metric, gt_var, horizon)]['cb_skip']\n",
    "                cb_ticklabels = [f'{tick}' if 'icec' in viz_var else f'{tick:.0f}' \n",
    "                                 for tick in np.arange(colorbar_min_value, colorbar_max_value+cb_skip, cb_skip)]\n",
    "                cb.set_ticks(np.arange(colorbar_min_value, colorbar_max_value+cb_skip, cb_skip))\n",
    "                cb.ax.set_yticklabels(cb_ticklabels, fontsize=params['fontsize_title'], weight='bold')\n",
    "                cb.ax.yaxis.set_ticks_position('left')\n",
    "\n",
    "# SUBPLOT 2 and 3 *******************************************************************************************\n",
    "    # Get original settings\n",
    "    CB_colors_customized = CB_colors_customized_or\n",
    "    CB_minmax = CB_minmax_or\n",
    "    metric = metric_or\n",
    "\n",
    "    # Create latitude, longitude list, model data is not yet used\n",
    "    df_models = metric_dfs[tasks[0]][metric]\n",
    "    df_models, model_names = format_df_models(df_models, model_names)\n",
    "    data_matrix = pivot_model_output(df_models, model_name=model_names[0])\n",
    "\n",
    "\n",
    "    # Get grid edges for each latitude, longitude coordinate\n",
    "    if '1.5' in tasks[0]:\n",
    "        lats = np.linspace(25.5, 48, data_matrix.shape[0])\n",
    "        lons = np.linspace(-123, -67.5, data_matrix.shape[1])\n",
    "    elif 'us' in tasks[0]:\n",
    "        lats = np.linspace(27, 49, data_matrix.shape[0])\n",
    "        lons = np.linspace(-124, -68, data_matrix.shape[1])\n",
    "    elif 'contest' in tasks[0]:\n",
    "        lats = np.linspace(27, 49, data_matrix.shape[0])\n",
    "        lons = np.linspace(-124, -94, data_matrix.shape[1])\n",
    "\n",
    "    if '1.5' in tasks[0]:\n",
    "        lats_edges = np.asarray(list(np.arange(lats[0], lats[-1]+1.5, 1.5))) - 0.75\n",
    "        lons_edges = np.asarray(list(np.arange(lons[0], lons[-1]+1.5, 1.5))) - 0.75\n",
    "        lat_grid, lon_grid = np.meshgrid(lats_edges,lons_edges)\n",
    "    else:\n",
    "        lats_edges = np.asarray(list(range(int(lats[0]), (int(lats[-1])+1)+1))) - 0.5\n",
    "        lons_edges = np.asarray(list(range(int(lons[0]), (int(lons[-1])+1)+1))) - 0.5\n",
    "        lat_grid, lon_grid = np.meshgrid(lats_edges,lons_edges)\n",
    "    \n",
    "\n",
    "    for i, xy in enumerate(product(model_names, tasks)):\n",
    "        if i >= subplots_num:\n",
    "            break\n",
    "\n",
    "        model_name, task = xy[0], xy[1]\n",
    "        x, y = tasks.index(task), model_names.index(model_name)+1\n",
    "        \n",
    "        ax = fig.add_subplot(gs[x,y], projection=ccrs.PlateCarree(), aspect=\"auto\")\n",
    "        ax.set_facecolor('w')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        df_models = metric_dfs[task][metric]\n",
    "        if 'skill' in metric:\n",
    "            df_models =df_models.apply(lambda x: x*100)      \n",
    "        df_models, model_names = format_df_models(df_models, model_names)  \n",
    "\n",
    "\n",
    "        data_matrix = pivot_model_output(df_models, model_name=model_name)\n",
    "        ax.coastlines(linewidth=0.9, color='gray') \n",
    "        ax.add_feature(cfeature.STATES.with_scale('110m'), edgecolor='gray', linewidth=0.9, linestyle=':')      \n",
    "\n",
    "        # Set color parameters\n",
    "        gt_id, horizon = task[:-4], task[-3:]\n",
    "        gt_var = \"tmp2m\" if \"tmp2m\" in gt_id else \"precip\" #gt_id.split(\"_\")[-1]\n",
    "        if CB_minmax == []:\n",
    "            colorbar_min_value = color_dic[(metric, gt_var, horizon)]['colorbar_min_value'] \n",
    "            colorbar_max_value = color_dic[(metric, gt_var, horizon)]['colorbar_max_value'] \n",
    "        else:\n",
    "            colorbar_min_value = CB_minmax[0]\n",
    "            colorbar_max_value = CB_minmax[1]\n",
    "        \n",
    "        color_map_str = color_dic[(metric, gt_var, horizon)]['color_map_str'] \n",
    "        \n",
    "        \n",
    "        if CB_colors_customized is not None:\n",
    "            if CB_colors_customized == []:\n",
    "                cmap = LinearSegmentedColormap.from_list(cmap_name, color_dic[(metric, gt_var, horizon)]['CB_colors'] , N=100)\n",
    "            else:\n",
    "                #customized cmap\n",
    "                cmap = LinearSegmentedColormap.from_list(cmap_name, CB_colors_customized, N=100)\n",
    "            color_map = matplotlib.cm.get_cmap(cmap) \n",
    "            plot = ax.pcolormesh(lon_grid,lat_grid, np.transpose(data_matrix),\n",
    "                         vmin=colorbar_min_value, vmax=colorbar_max_value,\n",
    "                         cmap=color_map, rasterized=True)\n",
    "        else:\n",
    "            color_map = matplotlib.cm.get_cmap(color_map_str)      \n",
    "            if \"linear\" in scale_type:\n",
    "                plot = ax.pcolormesh(lon_grid,lat_grid, np.transpose(data_matrix),\n",
    "                                 vmin=colorbar_min_value, vmax=colorbar_max_value,\n",
    "                                 cmap=color_map, rasterized=True)\n",
    "            elif \"symlognorm\" in scale_type:\n",
    "                plot = ax.pcolormesh(lon_grid,lat_grid, np.transpose(data_matrix),\n",
    "                                 cmap=color_map, \n",
    "                                 norm=colors.SymLogNorm(vmin=colorbar_min_value, \n",
    "                                                        vmax=colorbar_max_value, linthresh=0.03, base=10),\n",
    "                                 rasterized=True)\n",
    "    \n",
    "        \n",
    "        ax.tick_params(axis='both', labelsize=params['fontsize_ticks'])\n",
    "   \n",
    "        if mean_metric_df is not None:\n",
    "            df_mean_metric = mean_metric_df\n",
    "            mean_metric = '' if model_name =='gt' else int(df_mean_metric[model_name].mean())\n",
    "        elif metric == 'lat_lon_anom' and 'lat_lon_skill' in metric_dfs[task].keys():\n",
    "            df_mean_metric = metric_dfs[task]['lat_lon_skill'].apply(lambda x: x*100)\n",
    "            df_mean_metric, model_names = format_df_models(df_mean_metric, model_names)\n",
    "            mean_metric = int(df_mean_metric[model_name].mean())\n",
    "        else:\n",
    "            df_mean_metric = df_models\n",
    "            mean_metric = int(df_mean_metric[model_name].mean())\n",
    "            \n",
    "            \n",
    "        mean_metric_title = f\"{mean_metric}%\" if 'skill' in metric else str(mean_metric)\n",
    "        if x == 0 and y==0:\n",
    "#             ax.set_title(f\"{mean_metric_title}\", fontsize = params['fontsize_title'],fontweight=\"bold\")\n",
    "            #ax.set_ylabel(all_model_names[model_name], fontsize = params['fontsize_title'],fontweight=\"bold\")\n",
    "            ax.text(0.005, 0.55, all_model_names[model_name], va='bottom', ha='center',\n",
    "                    rotation='vertical', rotation_mode='anchor',\n",
    "                    transform=ax.transAxes, fontsize = params['fontsize_title'], fontweight=\"bold\")\n",
    "        elif x == 0 and y==1:\n",
    "#             ax.set_title(f\"Skill: {mean_metric_title}%\", fontsize = params['fontsize_title'],fontweight=\"bold\")\n",
    "            #ax.set_ylabel(all_model_names[model_name], fontsize = params['fontsize_title'],fontweight=\"bold\")\n",
    "            ax.text(0.005, 0.55, all_model_names[model_name], va='bottom', ha='center',\n",
    "                    rotation='vertical', rotation_mode='anchor',\n",
    "                    transform=ax.transAxes, fontsize = params['fontsize_title'], fontweight=\"bold\")\n",
    "        elif x == 0 and y>1:\n",
    "            ax.set_title(f\"Skill: {mean_metric_title}%\", fontsize = params['fontsize_title'],fontweight=\"bold\")\n",
    "            #ax.set_ylabel(all_model_names[model_name], fontsize = params['fontsize_title'],fontweight=\"bold\")\n",
    "            ax.text(0.005, 0.55, all_model_names[model_name], va='bottom', ha='center',\n",
    "                    rotation='vertical', rotation_mode='anchor',\n",
    "                    transform=ax.transAxes, fontsize = params['fontsize_title'], fontweight=\"bold\")\n",
    "        elif y>=1:\n",
    "            ax.set_title(f\"{mean_metric_title}\", fontsize = params['fontsize_title'],fontweight=\"bold\")\n",
    "       \n",
    "\n",
    "        #'''\n",
    "        #Add colorbar\n",
    "        \n",
    "        if CB_minmax != []:\n",
    "            if  i == 0:#subplots_num-1:                \n",
    "                #Add colorbar for weeks 3-4 and 5-6\n",
    "                cb_ax = fig.add_axes([0.45-cb_shift, 0.06, 0.4, 0.04]) #fig.add_axes([0.2, 0.08, 0.6, 0.02])\n",
    "                if CB_colors_customized is not None:\n",
    "                    cb = fig.colorbar(plot, cax=cb_ax, cmap=cmap, orientation='horizontal')\n",
    "                else:\n",
    "                    cb = fig.colorbar(plot, cax=cb_ax, orientation='horizontal')\n",
    "                cb.outline.set_edgecolor('black')\n",
    "                cb.ax.tick_params(labelsize=params['fontsize_ticks']) \n",
    "                if metric == 'lat_lon_error':\n",
    "                    cbar_title = 'model bias (mm)' if 'precip' in gt_id else 'model bias ($^\\degree$C)'\n",
    "                elif metric == 'lat_lon_anom':\n",
    "                    cbar_title = f\"{gt_var.replace('precip','Precipitation').replace('tmp2m','Temperature')} anomalies\"\n",
    "                elif 'skill' in metric:\n",
    "                    cbar_title = 'Skill (%)'\n",
    "                else:\n",
    "                    cbar_title = metric\n",
    "                cb.ax.set_xlabel(cbar_title, fontsize=params['fontsize_title'], weight='bold')\n",
    "                if \"linear\" in scale_type:\n",
    "                    cb_skip = color_dic[(metric, gt_var, horizon)]['cb_skip']   \n",
    "                    cb_ticklabels = [f'{tick}' for tick in range(colorbar_min_value, colorbar_max_value+cb_skip, cb_skip)]\n",
    "                    cb.set_ticks(range(colorbar_min_value, colorbar_max_value+cb_skip, cb_skip))\n",
    "                    cb.ax.set_xticklabels(cb_ticklabels, fontsize=params['fontsize_title'], weight='bold')  \n",
    "                    \n",
    "    fig_title = f\"Forecast with largest {get_feature_name(feature)[:-6]} impact in {bin_str}: {target_dates_str}\"\n",
    "    #set figure superior title\n",
    "    fig.suptitle(fig_title, fontsize=params['y_sup_fontsize'], y=params['y_sup_title'])\n",
    "        \n",
    "    #Save figure\n",
    "    model_names_str = '-'.join(model_names)\n",
    "    out_file = os.path.join(date_fig_dir, f\"{metric}_{target_dates}_{gt_id}_n{subplots_num}_{model_names_str}_zoom{zoom}_{feature}.pdf\") \n",
    "    plt.savefig(out_file, orientation = 'landscape', bbox_inches='tight')\n",
    "#     plt.savefig(out_file.replace('.pdf','.png'), orientation = 'landscape', bbox_inches='tight')\n",
    "    subprocess.call(\"chmod a+w \"+out_file, shell=True)\n",
    "    subprocess.call(\"chown $USER:sched_mit_hill \"+out_file, shell=True)\n",
    "    print(f\"\\nFigure saved: {out_file}\\n\")\n",
    "    if not show:\n",
    "        fig.clear()\n",
    "        plt.close(fig)  \n",
    "        \n",
    "    return fig\n",
    "\n",
    "\n",
    "# Figure parameter values\n",
    "figure_gt_ids = [gt_id]#us_1_5_gt_ids\n",
    "figure_horizons = [horizon]#horizons\n",
    "figure_metrics = ['lat_lon_anom']#, 'lat_lon_skill']\n",
    "figure_mean_metric_df = pd.DataFrame()#None\n",
    "figure_show = False\n",
    "figure_models = [\"gt\", model] if model2 is None else ['gt', model2, model]\n",
    "figure_zoom = False\n",
    "\n",
    "\n",
    "features = [feature for feature in X.columns[order[:]] if feature in continuous.columns]\n",
    "\n",
    "for feature in [\"hgt_500_anom_2010_1_shift30\"]:#features[1:6]:\n",
    "    # Identify the largest impact forecast date\n",
    "    figure_target_dates = dates_largest_impact[feature]\n",
    "    # Skip over discrete features\n",
    "    if feature.startswith('phase_shift') or feature.startswith('month'):\n",
    "        display(Markdown(f\"### {feature}, {figure_target_dates}: SKIPPING.\"))\n",
    "        continue\n",
    "    display(Markdown(f\"### {feature}, {figure_target_dates}:\"))\n",
    "    \n",
    "    # Compute impact level (i.e., the probability of positive impact in \n",
    "    # the feature bin associated with this forecast date) and the associated\n",
    "    # decile\n",
    "    cis_feature = (df_cs[feature] > 0).groupby(X_q[feature]).apply(\n",
    "        lambda x: proportion.proportion_confint(x.sum(), x.size))\n",
    "    num_bins = len(cis_feature)\n",
    "    \n",
    "    # Prepare string summary of high impact bins\n",
    "    high_impact_bins = get_high_impact_bins(feature, cis_feature, num_bins)\n",
    "    if len(high_impact_bins) == 1:\n",
    "        bin_str = f\"decile {high_impact_bins.categories.get_loc(high_impact_bins[0])+1}\"\n",
    "    else:\n",
    "        bin_str = f\"deciles \" + \", \".join(\n",
    "            [str(high_impact_bins.categories.get_loc(b)+1) for b in high_impact_bins])\n",
    "\n",
    "    \n",
    "    # Store metric for each model\n",
    "    figure_mean_metric_df = pd.DataFrame()\n",
    "    figure_mean_metric_df[model] = metrics.loc[metrics.index == datetime.strptime(figure_target_dates, '%Y%m%d'), metric].values\n",
    "    if model2 is not None:\n",
    "        figure_mean_metric_df[model2] = metrics2.loc[metrics2.index == datetime.strptime(figure_target_dates, '%Y%m%d'), metric].values\n",
    "    if metric == 'skill':\n",
    "        # Convert to a percentage\n",
    "        figure_mean_metric_df = figure_mean_metric_df.apply(lambda x: x*100)\n",
    "    #RDA: models for which Raw, Debiased and Abc versions are available\n",
    "    metric_dfs_rda = {}\n",
    "    for fig_gt_id, fig_horizon in product(figure_gt_ids, figure_horizons):\n",
    "        fig_task = f\"{fig_gt_id}_{fig_horizon}\"\n",
    "        display(Markdown(f\"#### Getting metrics for {fig_gt_id} {fig_horizon}\"))\n",
    "        metric_dfs_rda[fig_task] = get_models_metric_lat_lon(gt_id=fig_gt_id, horizon=fig_horizon, \n",
    "                                                         target_dates=figure_target_dates, \n",
    "                                                         metrics = figure_metrics, model_names=figure_models)\n",
    "    \n",
    "    \n",
    "    figure_target_date = dates_largest_impact[feature]\n",
    "\n",
    "\n",
    "    # Identify associated visualization variable\n",
    "    viz_var = get_viz_var(feature)\n",
    "    shift = int(str.split(str.split(feature,'_')[-1],'shift')[1])\n",
    "\n",
    "    # Load visualization variable\n",
    "    tic()\n",
    "    viz_df = data_loaders.get_ground_truth(\n",
    "        viz_var, shift=shift).set_index('start_date')\n",
    "    toc()\n",
    "\n",
    "\n",
    "    # Restrict to relevant dates\n",
    "    target_date_obj = get_target_dates(figure_target_date,'%Y%m%d')[0]\n",
    "    target_date_ind = datetime.strftime(target_date_obj,'%Y-%m-%d')#'2019-04-09'\n",
    "    data_matrix = viz_df.loc[target_date_ind].to_frame().T\n",
    "    data_matrix.index.names = [feature]\n",
    "\n",
    "    #plot single lat lon mat\n",
    "    bin_num = 0\n",
    "    data_matrix = lat_lon_mat(data_matrix.iloc[bin_num])\n",
    "    if feature.startswith('icec'):\n",
    "        # Add in rows corresponding to any missing lat values with NaN values\n",
    "        data_matrix = data_matrix.reindex(\n",
    "            np.arange(data_matrix.index.min(), data_matrix.index.max()+1), fill_value = np.nan)\n",
    "        # For icec, NaN and 0 values should be treated identically\n",
    "        data_matrix[data_matrix.isna()] = 0\n",
    "\n",
    "    # Also provide access to mean anomalies per bin / quantile to set colorbar range\n",
    "    viz_df = viz_df.loc[X_q.index]\n",
    "    viz_df = viz_df.groupby(X_q[feature]).mean()    \n",
    "    \n",
    "    plot_metric_maps_trio(metric_dfs_rda, \n",
    "                          model_names=figure_models,\n",
    "                          gt_ids=figure_gt_ids,\n",
    "                          horizons=figure_horizons,\n",
    "                          data_matrix=data_matrix,\n",
    "                          viz_df=viz_df,\n",
    "                          viz_var=viz_var,\n",
    "                          metric='lat_lon_anom',\n",
    "                          target_dates=figure_target_dates,\n",
    "                          mean_metric_df=figure_mean_metric_df,\n",
    "                          show=figure_show,\n",
    "                          scale_type=\"linear\", \n",
    "                          CB_colors_customized=['orangered','darkorange',\"white\",'forestgreen','darkgreen'], #['saddlebrown','peru',\"white\",'yellowgreen','green'], \n",
    "                          CB_minmax=(-20, 20), \n",
    "                          zoom=figure_zoom,\n",
    "                          feature=feature, \n",
    "                          bin_str=bin_str)\n",
    "    if feature == \"hgt_500_anom_2010_1_shift30\":\n",
    "        # Save figure source data\n",
    "        fig_filename = os.path.join(fig_dir, \"fig_4-impact_hgt_500_pc1.xlsx\")    \n",
    "\n",
    "        if os.path.isfile(fig_filename):\n",
    "            with pd.ExcelWriter(fig_filename, mode='a') as writer:  \n",
    "                data_matrix.to_excel(writer, sheet_name=f\"high_impact_bin_{task}\", na_rep=\"NaN\") \n",
    "                metric_dfs_rda[task]['lat_lon_anom'].to_excel(writer, sheet_name=f\"anom_{task}\", na_rep=\"NaN\") \n",
    "        else:\n",
    "            with pd.ExcelWriter(fig_filename) as writer:  \n",
    "                data_matrix.to_excel(writer, sheet_name=f\"high_impact_bin_{task}\", na_rep=\"NaN\") \n",
    "                metric_dfs_rda[task]['lat_lon_anom'].to_excel(writer, sheet_name=f\"anom_{task}\", na_rep=\"NaN\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MJO - Visualize anomalies for dates with largest impact \n",
    "# Figure parameter values\n",
    "figure_gt_ids = [gt_id]#us_1_5_gt_ids\n",
    "figure_horizons = [horizon]#horizons\n",
    "figure_metrics = ['lat_lon_anom']#, 'lat_lon_skill']\n",
    "figure_mean_metric_df = pd.DataFrame()#None\n",
    "figure_show = False\n",
    "figure_models = [\"gt\", model] if model2 is None else ['gt', model2, model]\n",
    "figure_zoom = False\n",
    "\n",
    "\n",
    "features = [feature for feature in X.columns[order[:]]]\n",
    "\n",
    "for feature in [f for f in features if f.startswith('phase')][:1]:\n",
    "    figure_target_dates = dates_largest_impact[feature]\n",
    "    printf(f'{feature} - {figure_target_dates}:')\n",
    "    \n",
    "    cis_feature = (df_cs[feature] > 0).groupby(X_q[feature]).apply(\n",
    "    lambda x: proportion.proportion_confint(x.sum(), x.size))\n",
    "    num_bins = len(cis_feature)\n",
    "    \n",
    "    # Prepare string summary of high impact bins\n",
    "    high_impact_bins = get_high_impact_bins(feature, cis_feature, num_bins)\n",
    "    if len(high_impact_bins) == 1:\n",
    "        bin_str = f\"phase {int(high_impact_bins[0])}\"\n",
    "    else:\n",
    "        bin_str = f\"phases \" + \", \".join([str(int(b)) for b in high_impact_bins])\n",
    "    \n",
    "    figure_mean_metric_df = pd.DataFrame()\n",
    "    if model2 is not None:\n",
    "        figure_mean_metric_df[model2] = metrics2[metrics2.index == datetime.strptime(figure_target_dates, '%Y%m%d')].skill.values\n",
    "        figure_mean_metric_df[model] = metrics[metrics.index == datetime.strptime(figure_target_dates, '%Y%m%d')].skill.values\n",
    "    else:\n",
    "        figure_mean_metric_df[model] = outcome[outcome.index == datetime.strptime(figure_target_dates, '%Y%m%d')].skill.values\n",
    "    figure_mean_metric_df = figure_mean_metric_df.apply(lambda x: x*100)\n",
    "    #RDA: models for which Raw, Debiased and Abc versions are available\n",
    "    metric_dfs_rda = {}\n",
    "    for gt_id, horizon in product(figure_gt_ids, figure_horizons):\n",
    "        task = f\"{gt_id}_{horizon}\"\n",
    "        display(Markdown(f\"#### Getting metrics for {gt_id} {horizon}\"))\n",
    "        metric_dfs_rda[task] = get_models_metric_lat_lon(gt_id=gt_id, horizon=horizon, \n",
    "                                                         target_dates=figure_target_dates, \n",
    "                                                         metrics = figure_metrics, model_names=figure_models)\n",
    "#     display(metric_dfs_rda)\n",
    "    plot_metric_maps(metric_dfs_rda, \n",
    "                         model_names=figure_models,\n",
    "                         gt_ids=figure_gt_ids,\n",
    "                         horizons=figure_horizons,\n",
    "                         metric='lat_lon_anom',\n",
    "                         target_dates=figure_target_dates,\n",
    "                         mean_metric_df=figure_mean_metric_df,\n",
    "                         show=figure_show, \n",
    "                         scale_type='linear',\n",
    "                         CB_colors_customized=['orangered','darkorange',\"white\",'forestgreen','darkgreen'], #['saddlebrown','peru',\"white\",'yellowgreen','green'],# \"#dede00\", \"#ff7f00\", \"blueviolet\", \"indigo\", \"yellowgreen\", \"lightgreen\", \"darkgreen\"],\n",
    "                         CB_minmax = (-20, 20),\n",
    "                         zoom = figure_zoom,\n",
    "                        feature = feature,\n",
    "                        bin_str = bin_str)\n",
    "    \n",
    "    # Save figure source data\n",
    "    fig_filename = os.path.join(fig_dir, \"fig_5-impact_mjo_phase.xlsx\")    \n",
    "    if os.path.isfile(fig_filename):\n",
    "        with pd.ExcelWriter(fig_filename, mode='a') as writer:  \n",
    "            metric_dfs_rda[task]['lat_lon_anom'].to_excel(writer, sheet_name=f\"anom_{task}\", na_rep=\"NaN\") \n",
    "    else:\n",
    "        with pd.ExcelWriter(fig_filename) as writer:  \n",
    "            metric_dfs_rda[task]['lat_lon_anom'].to_excel(writer, sheet_name=f\"anom_{task}\", na_rep=\"NaN\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old / obsolete code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize relationship between variables and their impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Choose title and y-axis label\n",
    "    title = f\"{gt_id} {horizon}\"\n",
    "    if model2 is None:\n",
    "        title += f\", {model}\"\n",
    "        ylabel = f'Variable impact on {metric}'\n",
    "    else:\n",
    "        title += f\" ({model} vs. {model2})\"\n",
    "        ylabel = f'Variable impact on {metric} difference'\n",
    "\n",
    "    from scipy import stats\n",
    "\n",
    "    for col in X.columns[order][:5]:\n",
    "        if False:\n",
    "            # Scatterplots with kernel density shading\n",
    "            fig = plt.figure(dpi=300)\n",
    "            if col in continuous:\n",
    "                ###sns.violinplot(x=pd.qcut(data[col],q=10), y=df_cs[col], scale='width')\n",
    "                #sns.scatterplot(x=data[col], y=df_cs[col])\n",
    "                #sns.kdeplot(x=data[col], y=df_cs[col], fill=True, alpha=0.6, cut=2, cmap=\"viridis\")\n",
    "                values = np.vstack([data[col], df_cs[col]])\n",
    "                kernel = stats.gaussian_kde(values)(values)\n",
    "                sns.scatterplot(\n",
    "                    x=data[col], y=df_cs[col],\n",
    "                    c=kernel,\n",
    "                    cmap=\"viridis\",\n",
    "                )\n",
    "            else:\n",
    "                ###sns.violinplot(x=data[col], y=df_cs[col], scale='width')\n",
    "                #sns.kdeplot(x=data[col], y=df_cs[col], fill=True, alpha=0.6, cut=2, cmap=\"viridis\")\n",
    "                values = np.vstack([data[col], df_cs[col]])\n",
    "                kernel = stats.gaussian_kde(values)(values)\n",
    "                sns.scatterplot(\n",
    "                    x=data[col], y=df_cs[col],\n",
    "                    c=kernel,\n",
    "                    cmap=\"viridis\",\n",
    "                )\n",
    "            fig.autofmt_xdate(rotation=45)\n",
    "            plt.title(title)\n",
    "            plt.ylabel(ylabel)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        if False:\n",
    "            # For continuous features, plot Cohort Shapley distribution conditional\n",
    "            # on quantile bins\n",
    "            # For discrete features, plot Cohort Shapley distribution conditional\n",
    "            # on feature value\n",
    "            fig = plt.figure(dpi=300)\n",
    "            if col in continuous:\n",
    "                sns.violinplot(x=pd.qcut(data[col],q=10), y=df_cs[col], scale='width')\n",
    "            else:\n",
    "                sns.violinplot(x=data[col], y=df_cs[col], scale='width')\n",
    "            fig.autofmt_xdate(rotation=45)\n",
    "\n",
    "\n",
    "        fig = plt.figure(dpi=300)\n",
    "        if col in continuous:\n",
    "\n",
    "            # Display estimated positive impact probability for each variable's quantiles\n",
    "            # with 95% bootstrap confidence intervals \n",
    "            impact_name = \"Positive impact probability\"\n",
    "            ax = sns.catplot(kind=\"bar\",data = pd.DataFrame({col: pd.qcut(data[col],q=10), impact_name:df_cs[col]>0}), \n",
    "                        x = col, y = impact_name, estimator = np.mean, alpha=.7, \n",
    "                        edgecolor=\".2\",\n",
    "                        #edgecolor=sns.color_palette()[1], \n",
    "                        #errcolor=sns.color_palette()[1],\n",
    "                        color=sns.color_palette()[1])\n",
    "            xlabels = [f'{int(1+i)}' for i in range(10)]\n",
    "            ax.set(xticks=range(10),xticklabels=xlabels)\n",
    "            plt.ylim(0,1)\n",
    "            plt.xlabel(f\"{col} decile\", fontdict={'weight': 'bold'})\n",
    "            plt.ylabel(impact_name, fontdict={'weight': 'bold'})\n",
    "            plt.xticks(ticks=range(10), labels=xlabels, weight= 'bold')\n",
    "    #         plt.xticks(rotation=90)\n",
    "            out_file = f\"subseasonal_toolkit/viz/variable_impact_catplot_{title.replace(',','').replace(' ','_')}_{col}.pdf\"\n",
    "            ax.fig.savefig(out_file, dpi=300, bbox_inches = \"tight\")#; ax.fig.savefig(out_file.replace('.pdf','.png'), dpi=300, bbox_inches = \"tight\")\n",
    "            printf(f'Saving figure {out_file}')\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Display estimated positive impact probability for each variable\n",
    "            # with 95% bootstrap confidence intervals \n",
    "            impact_name = \"Positive impact probability\"\n",
    "            ax = sns.catplot(kind=\"bar\",data = pd.DataFrame({col: data[col], impact_name:df_cs[col]>0}), \n",
    "                        x = col, y = impact_name, estimator = np.mean, alpha=.7, \n",
    "                        edgecolor=\".2\",\n",
    "                        #edgecolor=sns.color_palette()[1], \n",
    "                        #errcolor=sns.color_palette()[1],\n",
    "                        color=sns.color_palette()[1])\n",
    "    #         labels = [f'Decile {1+i}' for i in range(12)]\n",
    "    #         ax.set(xticks=range(12),xticklabels=labels)\n",
    "            ax.set(xticks=range(8),xticklabels=[f'{int(1+i)}' for i in range(8)])\n",
    "            plt.ylim(0,1)\n",
    "            plt.xlabel(f\"{col} decile\", fontdict={'weight': 'bold'})\n",
    "            plt.ylabel(impact_name, fontdict={'weight': 'bold'})\n",
    "            out_file = f\"subseasonal_toolkit/viz/variable_impact_catplot_{title.replace(',','').replace(' ','_')}_{col}.pdf\"\n",
    "            ax.fig.savefig(out_file, dpi=300, bbox_inches = \"tight\")#; ax.fig.savefig(out_file.replace('.pdf','.png'), dpi=300, bbox_inches = \"tight\")\n",
    "            printf(f'Saving figure {out_file}')\n",
    "    #         plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
